{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "import random\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from scipy import interp\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from matplotlib import pyplot\n",
    "\n",
    "from scipy.optimize import differential_evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "dataset = pd.read_csv(os.path.join(os.getcwd(), 'data/pima_indian_data.csv'))\n",
    "\n",
    "# creating input features and target variables\n",
    "X = np.asarray(dataset.iloc[:,0:8], dtype=np.float32)\n",
    "y = np.asarray(dataset.iloc[:,8], dtype=np.float32)\n",
    "\n",
    "#standardizing the input feature\n",
    "sc = StandardScaler()\n",
    "X = sc.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "num_features = X_train.shape[1]\n",
    "\n",
    "# Train with RF\n",
    "rf = RandomForestClassifier(n_estimators=25, random_state=3)\n",
    "rf.fit(X_train, y_train)\n",
    "rf_probs = rf.predict_proba(X_test)\n",
    "# Train with LR\n",
    "lr = LogisticRegression(solver='lbfgs')\n",
    "lr.fit(X_train, y_train)\n",
    "# predict probabilities\n",
    "lr_probs = lr.predict_proba(X_test)\n",
    "\n",
    "# keep probabilities for the positive outcome only\n",
    "rf_probs = rf_probs[:, 1]\n",
    "lr_probs = lr_probs[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF: ROC AUC=0.821\n",
      "LS: ROC AUC=0.802\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x12f85bcc0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# calculate scores\n",
    "rf_auc = roc_auc_score(y_test, rf_probs)\n",
    "lr_auc = roc_auc_score(y_test, lr_probs)\n",
    "\n",
    "# summarize scores\n",
    "print('RF: ROC AUC=%.3f' % (rf_auc))\n",
    "print('LS: ROC AUC=%.3f' % (lr_auc))\n",
    "\n",
    "# calculate roc curves\n",
    "rf_fpr, rf_tpr, _ = roc_curve(y_test, rf_probs)\n",
    "lr_fpr, lr_tpr, _ = roc_curve(y_test, lr_probs)\n",
    "\n",
    "# plot the roc curve for the model\n",
    "pyplot.plot(rf_fpr, rf_tpr, linestyle='--', label='RF')\n",
    "pyplot.plot(lr_fpr, lr_tpr, marker='.', label='LR')\n",
    "\n",
    "# axis labels\n",
    "pyplot.xlabel('False Positive Rate')\n",
    "pyplot.ylabel('True Positive Rate')\n",
    "\n",
    "# show the legend\n",
    "pyplot.legend()\n",
    "# show the plot\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ConeOpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ER (train) = 0.3582089552238806\n",
      "ER (test) = 0.3246753246753247\n",
      "================================================================================\n",
      "Y (curr) has prob =  [0.86330464]\n",
      "X (curr) =  [[-0.25002164  2.0696416   0.04641078  0.09297736  1.0424324   0.5593035\n",
      "  -0.19220549  0.23678604]]\n",
      "================================================================================\n",
      "Y (ref) has prob =  [0.08120282]\n",
      "X (ref) =  [[-0.546874   -1.0595031  -0.5735673  -0.28334787 -0.24220139  0.08999533\n",
      "  -0.9227028  -0.69981205]]\n"
     ]
    }
   ],
   "source": [
    "print(\"ER (train) = {}\".format(sum(y_train)/len(y_train)))\n",
    "print(\"ER (test) = {}\".format(sum(y_test)/len(y_test)))\n",
    "#np.select(y_test == 1, y_test)\n",
    "\n",
    "# get current point and reference point \n",
    "idx_y_test_pos = np.argwhere(y_test == 1).flatten()\n",
    "idx_y_test_neg = np.argwhere(y_test == 0).flatten()\n",
    "\n",
    "idx_curr = idx_y_test_pos[5]\n",
    "idx_ref = idx_y_test_neg[4]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "X_curr = X_test[idx_curr:idx_curr+1, :]\n",
    "print(\"Y (curr) has prob = \", lr.predict_proba(X_curr)[:, 1])\n",
    "print(\"X (curr) = \", X_curr)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "X_ref = X_test[idx_ref:idx_ref+1, :]\n",
    "print(\"Y (ref) has prob = \", lr.predict_proba(X_ref)[:, 1])\n",
    "print(\"X (ref) = \", X_ref)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.differential_evolution.html#r108fc14fa019-1\n",
    "\n",
    "def run_coneopt(X_curr, X_ref, max_step = 0.3, fixed_features = []):\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    X_cone = X_ref - X_curr\n",
    "    print(\"Cone = \",  X_cone)\n",
    "\n",
    "    bounds = list(zip(X_curr.flatten(), (X_curr + X_cone * max_step).flatten()))\n",
    "    for b in range(len(bounds)):\n",
    "        bound = bounds[b]\n",
    "        if bound[0] > bound[1]:\n",
    "            bounds[b] = bound[1], bound[0]\n",
    "\n",
    "    for idx_feat in fixed_features:\n",
    "        bounds[idx_feat] = (X_curr[0][idx_feat], X_curr[0][idx_feat])\n",
    "    print(\"Bounds = \", bounds)\n",
    "\n",
    "    #print(X_curr, X_curr + X_cone * max_step)\n",
    "\n",
    "    def my_predict_proba(x, method):\n",
    "        return method.predict_proba(x.reshape(1, len(x)))[:, 1]\n",
    "\n",
    "    result = differential_evolution(\n",
    "        func=my_predict_proba, \n",
    "        bounds=bounds, \n",
    "        args=[lr],\n",
    "        disp=True,\n",
    "        seed=0)\n",
    "    X_opt = result.x.reshape(1, len(result.x))\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(\"CURR\")\n",
    "    print(\"Y (curr) has prob = \", lr.predict_proba(X_curr)[:, 1])\n",
    "    print(\"X (curr) = \", X_curr)\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(\"OPT\")\n",
    "    print(\"Y (opt) has prob = \", lr.predict_proba(X_opt)[:, 1])\n",
    "    print(\"X (opt) = \", X_opt)\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(\"REF\")\n",
    "    print(\"Y (ref) has prob = \", lr.predict_proba(X_ref)[:, 1])\n",
    "    print(\"X (ref) = \", X_ref)\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    return X_opt\n",
    "    \n",
    "    \n",
    "#https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.differential_evolution.html#r108fc14fa019-1\n",
    "\n",
    "def run_coneopt2(X_curr, X_ref, max_step = 0.3, fixed_features = []):\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    X_cone = X_ref - X_curr\n",
    "    print(\"Cone = \",  X_cone)\n",
    "\n",
    "    bounds = list(zip(X_curr.flatten(), (X_curr + X_cone * max_step).flatten()))\n",
    "    for b in range(len(bounds)):\n",
    "        bound = bounds[b]\n",
    "        if bound[0] > bound[1]:\n",
    "            bounds[b] = bound[1], bound[0]\n",
    "\n",
    "    bounds2 = []\n",
    "    fixed_x = []\n",
    "    non_fixed_features = []\n",
    "    for b in range(len(bounds)):\n",
    "        if b not in set(fixed_features):\n",
    "            bounds2.append(bounds[b])\n",
    "            non_fixed_features.append(b)\n",
    "        else:\n",
    "            fixed_x.append(X_curr[0][b]) \n",
    "    num_features = len(bounds)\n",
    "    bounds = bounds2\n",
    "    num_features_active = len(bounds)\n",
    "    print(\"Bounds = \", bounds)\n",
    "    print(\"fixed_features = \", fixed_features)\n",
    "    print(\"fixed_x = \", fixed_x)\n",
    "    print(\"non_fixed_features = \", non_fixed_features)\n",
    "\n",
    "    #print(X_curr, X_curr + X_cone * max_step)\n",
    "\n",
    "    def get_full_x(non_fixed_x, fixed_features, non_fixed_features, fixed_x):\n",
    "        full_x = [b for b in range(len(fixed_features) + len(non_fixed_features))]\n",
    "        for b in range(len(non_fixed_features)):\n",
    "            full_x[non_fixed_features[b]] = non_fixed_x[b]\n",
    "        for b in range(len(fixed_features)):\n",
    "            full_x[fixed_features[b]] = fixed_x[b]\n",
    "        return full_x\n",
    "\n",
    "    def my_predict_proba(non_fixed_x, method, fixed_features, non_fixed_features, fixed_x):\n",
    "        if non_fixed_features == []:\n",
    "            return method.predict_proba(x.reshape(1, len(x)))[:, 1]\n",
    "        else:\n",
    "            full_x = get_full_x(non_fixed_x, fixed_features, non_fixed_features, fixed_x)\n",
    "            #print(\"non_fixed_features\", non_fixed_features)\n",
    "            #print(\"fixed_features\", fixed_features)\n",
    "            return method.predict_proba(np.array(full_x).reshape(1, len(full_x)))[:, 1]\n",
    "\n",
    "    result = differential_evolution(\n",
    "        func=my_predict_proba, \n",
    "        bounds=bounds, \n",
    "        args=[lr, fixed_features, non_fixed_features, fixed_x],\n",
    "        disp=True,\n",
    "        seed=0)\n",
    "    \n",
    "    full_x = get_full_x(result.x, fixed_features, non_fixed_features, fixed_x)\n",
    "    X_opt = np.array(full_x).reshape(1, len(full_x))\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(\"CURR\")\n",
    "    print(\"Y (curr) has prob = \", lr.predict_proba(X_curr)[:, 1])\n",
    "    print(\"X (curr) = \", X_curr)\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(\"OPT\")\n",
    "    print(\"Y (opt) has prob = \", lr.predict_proba(X_opt)[:, 1])\n",
    "    print(\"X (opt) = \", X_opt)\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(\"REF\")\n",
    "    print(\"Y (ref) has prob = \", lr.predict_proba(X_ref)[:, 1])\n",
    "    print(\"X (ref) = \", X_ref)\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    return X_opt\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_fixed_features(\n",
    "    X_curr,\n",
    "    X_opt,\n",
    "    influential_features_percentage = 0.5, \n",
    "    delta_feature_eps = 0.0001):\n",
    "\n",
    "    # Identify the most influential features -- 50% of the most important features.\n",
    "    #influential_features_percentage = 0.5 \n",
    "    #delta_feature_eps = 0.0001\n",
    "    \n",
    "    num_features = X_curr.shape[1]\n",
    "    diff = list(map(abs, X_opt.flatten() - X_curr.flatten()))\n",
    "    for i in range(len(diff)):\n",
    "        if diff[i] == 0:\n",
    "            diff[i] += random.randrange(100)*delta_feature_eps\n",
    "    num_features_changed = sum(np.array(diff) > delta_feature_eps)\n",
    "\n",
    "    num_target_features = int(max(1, influential_features_percentage * num_features_changed))\n",
    "    print(\"Will use [{}] feautres for the analysis\".format(num_target_features))\n",
    "\n",
    "    #print(\"diff\", diff)\n",
    "    #print(\"list(map(abs, X_curr))\", list(map(abs, X_curr)))\n",
    "    delta_changes = np.divide(diff, list(map(abs, X_curr)))[0]\n",
    "    print(\"delta_changes = \", delta_changes)\n",
    "    cutoff_feature_value = sorted(delta_changes, reverse = True)[num_target_features - 1]\n",
    "    print(\"Cutoff feature values (only feature with values >= cutoff will be included) = {}\".format(cutoff_feature_value))\n",
    "    flag_required_feature = delta_changes >= cutoff_feature_value\n",
    "    \n",
    "    #print(idx_required_feature)\n",
    "    assert(sum(flag_required_feature) == num_target_features)\n",
    "    \n",
    "    return [i for i in range(num_features) if flag_required_feature[i]==False]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Cone =  [[-0.29685235 -3.1291447  -0.6199781  -0.37632525 -1.2846339  -0.4693082\n",
      "  -0.7304973  -0.93659806]]\n",
      "Bounds =  [(-0.35391995, -0.25002164), (0.97444093, 2.0696416), (-0.17058153, 0.046410784), (-0.038736477, 0.09297736), (0.5928106, 1.0424324), (0.39504564, 0.5593035), (-0.44787955, -0.19220549), (-0.09102327, 0.23678604)]\n",
      "differential_evolution step 1: f(x)= 0.59389\n",
      "differential_evolution step 2: f(x)= 0.573098\n",
      "differential_evolution step 3: f(x)= 0.570045\n",
      "differential_evolution step 4: f(x)= 0.570045\n",
      "differential_evolution step 5: f(x)= 0.570045\n",
      "differential_evolution step 6: f(x)= 0.570045\n",
      "differential_evolution step 7: f(x)= 0.570045\n",
      "differential_evolution step 8: f(x)= 0.568798\n",
      "differential_evolution step 9: f(x)= 0.566948\n",
      "differential_evolution step 10: f(x)= 0.566948\n",
      "differential_evolution step 11: f(x)= 0.565765\n",
      "differential_evolution step 12: f(x)= 0.565306\n",
      "================================================================================\n",
      "CURR\n",
      "Y (curr) has prob =  [0.86330464]\n",
      "X (curr) =  [[-0.25002164  2.0696416   0.04641078  0.09297736  1.0424324   0.5593035\n",
      "  -0.19220549  0.23678604]]\n",
      "================================================================================\n",
      "OPT\n",
      "Y (opt) has prob =  [0.55434108]\n",
      "X (opt) =  [[-0.35391995  0.97444093  0.04641078  0.09297736  1.04243243  0.39504564\n",
      "  -0.44787955 -0.09102327]]\n",
      "================================================================================\n",
      "REF\n",
      "Y (ref) has prob =  [0.08120282]\n",
      "X (ref) =  [[-0.546874   -1.0595031  -0.5735673  -0.28334787 -0.24220139  0.08999533\n",
      "  -0.9227028  -0.69981205]]\n",
      "================================================================================\n",
      "Will use [4] feautres for the analysis\n",
      "delta_changes =  [0.4155573  0.52917407 0.19607512 0.00215106 0.00527612 0.29368291\n",
      " 1.33021209 1.38441146]\n",
      "Cutoff feature values (only feature with values >= cutoff will be included) = 0.41555730168807203\n",
      "[2, 3, 4, 5]\n",
      "================================================================================\n",
      "Cone =  [[-0.29685235 -3.1291447  -0.6199781  -0.37632525 -1.2846339  -0.4693082\n",
      "  -0.7304973  -0.93659806]]\n",
      "Bounds =  [(-0.35391995, -0.25002164), (0.97444093, 2.0696416), (-0.44787955, -0.19220549), (-0.09102327, 0.23678604)]\n",
      "fixed_features =  [2, 3, 4, 5]\n",
      "fixed_x =  [0.046410784, 0.09297736, 1.0424324, 0.5593035]\n",
      "non_fixed_features =  [0, 1, 6, 7]\n",
      "differential_evolution step 1: f(x)= 0.599306\n",
      "differential_evolution step 2: f(x)= 0.597033\n",
      "differential_evolution step 3: f(x)= 0.597033\n",
      "differential_evolution step 4: f(x)= 0.597033\n",
      "differential_evolution step 5: f(x)= 0.597033\n",
      "differential_evolution step 6: f(x)= 0.592135\n",
      "differential_evolution step 7: f(x)= 0.592135\n",
      "differential_evolution step 8: f(x)= 0.592135\n",
      "differential_evolution step 9: f(x)= 0.592135\n",
      "differential_evolution step 10: f(x)= 0.591677\n",
      "================================================================================\n",
      "CURR\n",
      "Y (curr) has prob =  [0.86330464]\n",
      "X (curr) =  [[-0.25002164  2.0696416   0.04641078  0.09297736  1.0424324   0.5593035\n",
      "  -0.19220549  0.23678604]]\n",
      "================================================================================\n",
      "OPT\n",
      "Y (opt) has prob =  [0.58915392]\n",
      "X (opt) =  [[-0.35391995  0.97444093  0.04641078  0.09297736  1.04243243  0.55930352\n",
      "  -0.44787955 -0.09102327]]\n",
      "================================================================================\n",
      "REF\n",
      "Y (ref) has prob =  [0.08120282]\n",
      "X (ref) =  [[-0.546874   -1.0595031  -0.5735673  -0.28334787 -0.24220139  0.08999533\n",
      "  -0.9227028  -0.69981205]]\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "max_step = 0.35\n",
    "\n",
    "X_opt_init = run_coneopt(X_curr, X_ref, max_step=max_step, fixed_features=[])\n",
    "fixed_features = identify_fixed_features(X_curr, X_opt_init)\n",
    "print(fixed_features)\n",
    "\n",
    "#X_opt = run_coneopt(X_curr, X_ref, max_step=max_step, fixed_features=fixed_features)\n",
    "X_opt = run_coneopt2(X_curr, X_ref, max_step=max_step, fixed_features=fixed_features)\n",
    "#X_opt = run_coneopt(X_curr, X_ref, max_step=max_step, fixed_features=fixed_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NO NEED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y (curr) has prob =  [0.90827086]\n",
      "X (curr) =  [[2.1247969  1.0683153  0.3563998  0.4693026  0.60843456 0.16609915\n",
      "  2.6603563  1.5139652 ]]\n",
      "Y (opt) has prob =  [0.53369112]\n",
      "X (opt) =  [[ 0.49428485  0.33325735  0.32808709  0.29517624  0.54907977 -0.21322886\n",
      "   1.8112237   0.72292829]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Y (curr) has prob = \", lr.predict_proba(X_curr)[:, 1])\n",
    "print(\"X (curr) = \", X_curr)\n",
    "\n",
    "X_opt = result.x.reshape(1, len(result.x))\n",
    "print(\"Y (opt) has prob = \", lr.predict_proba(X_opt)[:, 1])\n",
    "print(\"X (opt) = \", X_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import feature_column\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import backend as k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConEOpt: Contractual Explanation with Optimization\n",
      "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
      "0   63    1   1       145   233    1        2      150      0      2.3      3   \n",
      "1   67    1   4       160   286    0        2      108      1      1.5      2   \n",
      "2   67    1   4       120   229    0        2      129      1      2.6      2   \n",
      "3   37    1   3       130   250    0        0      187      0      3.5      3   \n",
      "4   41    0   2       130   204    0        2      172      0      1.4      1   \n",
      "\n",
      "   ca        thal  target  \n",
      "0   0       fixed       0  \n",
      "1   3      normal       1  \n",
      "2   2  reversible       0  \n",
      "3   0      normal       0  \n",
      "4   0      normal       0  \n",
      "193 train examples\n",
      "49 validation examples\n",
      "61 test examples\n",
      "Every feature: ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal']\n",
      "A batch of ages: tf.Tensor([59 54 43 67 47], shape=(5,), dtype=int32)\n",
      "A batch of targets: tf.Tensor([0 0 0 0 0], shape=(5,), dtype=int32)\n",
      "NumericColumn(key='age', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None)\n",
      "WARNING:tensorflow:From /Users/AF45008/Research/ColinML/coneopt/conda_env36/lib/python3.6/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4267: IndicatorColumn._variable_shape (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "WARNING:tensorflow:From /Users/AF45008/Research/ColinML/coneopt/conda_env36/lib/python3.6/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4322: CrossedColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "WARNING:tensorflow:From /Users/AF45008/Research/ColinML/coneopt/conda_env36/lib/python3.6/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4322: VocabularyListCategoricalColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "Train for 7 steps, validate for 2 steps\n",
      "Epoch 1/5\n",
      "7/7 [==============================] - 1s 186ms/step - loss: 1.4338 - accuracy: 0.6580 - val_loss: 0.5768 - val_accuracy: 0.7347\n",
      "Epoch 2/5\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8596 - accuracy: 0.5492 - val_loss: 1.2900 - val_accuracy: 0.6735\n",
      "Epoch 3/5\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.9963 - accuracy: 0.7513 - val_loss: 0.7582 - val_accuracy: 0.6735\n",
      "Epoch 4/5\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.5179 - accuracy: 0.7409 - val_loss: 0.5028 - val_accuracy: 0.7347\n",
      "Epoch 5/5\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.4584 - accuracy: 0.7668 - val_loss: 0.6129 - val_accuracy: 0.6735\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.5560 - accuracy: 0.7377\n",
      "Accuracy 0.73770493\n"
     ]
    }
   ],
   "source": [
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.max_rows', None)\n",
    "\n",
    "    # https://www.tensorflow.org/tutorials/structured_data/feature_columns\n",
    "    print(\"ConEOpt: Contractual Explanation with Optimization\")\n",
    "\n",
    "    # Use Pandas to create a dataframe\n",
    "    filepath = '/Users/AF45008/Research/ColinML/coneopt/data/heart.csv'\n",
    "    dataframe = pd.read_csv(filepath)\n",
    "    print(dataframe.head())\n",
    "\n",
    "    # Split the dataframe into train, validation, and test\n",
    "    train, test = train_test_split(dataframe, test_size=0.2)\n",
    "    train, val = train_test_split(train, test_size=0.2)\n",
    "    print(len(train), 'train examples')\n",
    "    print(len(val), 'validation examples')\n",
    "    print(len(test), 'test examples')\n",
    "\n",
    "    # Create an input pipeline using tf.data\n",
    "    # A utility method to create a tf.data dataset from a Pandas Dataframe\n",
    "    def df_to_dataset(dataframe, shuffle=True, batch_size=32):\n",
    "        dataframe = dataframe.copy()\n",
    "        labels = dataframe.pop('target')\n",
    "        ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
    "        if shuffle:\n",
    "            ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "        ds = ds.batch(batch_size)\n",
    "        return ds\n",
    "\n",
    "    batch_size = 5  # A small batch sized is used for demonstration purposes\n",
    "    train_ds = df_to_dataset(train, batch_size=batch_size)\n",
    "    val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\n",
    "    test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)\n",
    "\n",
    "    # Understand the input pipeline\n",
    "    for feature_batch, label_batch in train_ds.take(1):\n",
    "        print('Every feature:', list(feature_batch.keys()))\n",
    "        print('A batch of ages:', feature_batch['age'])\n",
    "        print('A batch of targets:', label_batch)\n",
    "\n",
    "    # Numeric columns\n",
    "    age = feature_column.numeric_column(\"age\")\n",
    "    print(age)\n",
    "\n",
    "    feature_columns = []\n",
    "\n",
    "    # numeric cols\n",
    "    for header in ['age', 'trestbps', 'chol', 'thalach', 'oldpeak', 'slope', 'ca']:\n",
    "      feature_columns.append(feature_column.numeric_column(header))\n",
    "\n",
    "    # bucketized cols\n",
    "    age_buckets = feature_column.bucketized_column(age, boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65])\n",
    "    feature_columns.append(age_buckets)\n",
    "\n",
    "    # indicator cols\n",
    "    thal = feature_column.categorical_column_with_vocabulary_list(\n",
    "          'thal', ['fixed', 'normal', 'reversible'])\n",
    "    thal_one_hot = feature_column.indicator_column(thal)\n",
    "    feature_columns.append(thal_one_hot)\n",
    "\n",
    "    # embedding cols\n",
    "    thal_embedding = feature_column.embedding_column(thal, dimension=8)\n",
    "    feature_columns.append(thal_embedding)\n",
    "\n",
    "    # crossed cols\n",
    "    crossed_feature = feature_column.crossed_column([age_buckets, thal], hash_bucket_size=1000)\n",
    "    crossed_feature = feature_column.indicator_column(crossed_feature)\n",
    "    feature_columns.append(crossed_feature)\n",
    "\n",
    "    feature_layer = tf.keras.layers.DenseFeatures(feature_columns)\n",
    "    batch_size = 32\n",
    "    train_ds = df_to_dataset(train, batch_size=batch_size)\n",
    "    val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\n",
    "    test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)\n",
    "\n",
    "    # Create, compile, and train the model\n",
    "    model = tf.keras.Sequential([\n",
    "        feature_layer,\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model.fit(train_ds,\n",
    "              validation_data=val_ds,\n",
    "              epochs=5)\n",
    "\n",
    "    loss, accuracy = model.evaluate(test_ds)\n",
    "    print(\"Accuracy\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 3ms/step - loss: 0.5560 - accuracy: 0.7377\n",
      "     age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
      "96    41    1   3       112   250    0        0      179      0      0.0   \n",
      "283   60    0   4       150   258    0        2      157      0      2.6   \n",
      "186   42    1   2       120   295    0        0      162      0      0.0   \n",
      "31    60    1   4       117   230    1        0      160      1      1.4   \n",
      "112   58    0   4       100   248    0        2      122      0      1.0   \n",
      "266   60    1   4       130   253    0        0      144      1      1.4   \n",
      "295   39    1   4       118   219    0        0      140      0      1.2   \n",
      "134   42    1   3       120   240    1        0      194      0      0.8   \n",
      "173   47    1   4       112   204    0        0      143      0      0.1   \n",
      "248   57    0   0       140   241    0        1      123      1      0.2   \n",
      "52    53    0   4       130   264    0        2      143      0      0.4   \n",
      "297   56    1   4       125   249    1        2      144      1      1.2   \n",
      "179   49    1   3       118   149    0        2      126      0      0.8   \n",
      "79    54    1   3       120   258    0        2      147      0      0.4   \n",
      "292   59    1   4       140   177    0        0      162      1      0.0   \n",
      "55    52    1   2       120   325    0        0      172      0      0.2   \n",
      "103   64    1   4       120   246    0        2       96      1      2.2   \n",
      "66    62    0   3       130   263    0        0       97      0      1.2   \n",
      "33    59    1   4       135   234    0        0      161      0      0.5   \n",
      "122   64    1   4       145   212    0        2      132      0      2.0   \n",
      "73    65    1   1       138   282    1        2      174      0      1.4   \n",
      "2     67    1   4       120   229    0        2      129      1      2.6   \n",
      "74    45    0   2       130   234    0        2      175      0      0.6   \n",
      "77    44    1   2       120   220    0        0      170      0      0.0   \n",
      "277   60    1   4       145   282    0        2      142      1      2.8   \n",
      "90    59    1   1       170   288    0        2      159      0      0.2   \n",
      "252   57    0   1       130   236    0        0      174      0      0.0   \n",
      "299   43    0   4       132   341    1        2      136      1      3.0   \n",
      "224   57    1   2       154   232    0        2      164      0      0.0   \n",
      "276   54    1   4       120   188    0        0      113      0      1.4   \n",
      "70    63    1   4       130   330    1        2      132      1      1.8   \n",
      "290   49    1   3       120   188    0        0      139      0      2.0   \n",
      "240   41    1   2       120   157    0        0      182      0      0.0   \n",
      "126   43    1   3       130   315    0        0      162      0      1.9   \n",
      "172   34    0   2       118   210    0        0      192      0      0.7   \n",
      "265   44    1   2       130   219    0        2      188      0      0.0   \n",
      "301   48    1   4       130   256    1        2      150      1      0.0   \n",
      "159   41    1   3       130   214    0        2      168      0      2.0   \n",
      "71    51    1   3       100   222    0        0      143      1      1.2   \n",
      "208   58    0   2       136   319    1        2      152      0      0.0   \n",
      "41    65    0   3       155   269    0        0      148      0      0.8   \n",
      "220   59    1   1       134   204    0        0      162      0      0.8   \n",
      "227   47    1   3       130   253    0        0      179      0      0.0   \n",
      "148   64    0   4       180   325    0        0      154      1      0.0   \n",
      "16    48    1   2       110   229    0        0      168      0      1.0   \n",
      "223   39    0   3       138   220    0        0      152      0      0.0   \n",
      "231   58    1   4       114   318    0        1      140      0      4.4   \n",
      "136   54    1   2       192   283    0        2      195      0      0.0   \n",
      "85    70    1   4       145   174    0        0      125      1      2.6   \n",
      "206   57    1   2       124   261    0        0      141      0      0.3   \n",
      "302   63    0   4       150   407    0        2      154      0      4.0   \n",
      "256   61    0   4       130   330    0        2      169      0      0.0   \n",
      "267   54    1   4       124   266    0        2      109      1      2.2   \n",
      "100   42    0   4       102   265    0        2      122      0      0.6   \n",
      "69    35    0   4       138   183    0        0      182      0      1.4   \n",
      "157   37    0   3       120   215    0        0      170      0      0.0   \n",
      "174   67    0   3       152   277    0        0      172      0      0.0   \n",
      "50    58    1   4       150   270    0        2      111      1      0.8   \n",
      "175   54    1   4       110   206    0        2      108      1      0.0   \n",
      "181   54    0   3       160   201    0        0      163      0      0.0   \n",
      "219   71    0   4       112   149    0        0      125      0      1.6   \n",
      "\n",
      "     slope  ca        thal  target  \n",
      "96       1   0      normal       0  \n",
      "283      2   2  reversible       1  \n",
      "186      1   0      normal       0  \n",
      "31       1   2  reversible       1  \n",
      "112      2   0      normal       0  \n",
      "266      1   1  reversible       0  \n",
      "295      2   0  reversible       1  \n",
      "134      3   0  reversible       0  \n",
      "173      1   0      normal       0  \n",
      "248      1   0      normal       0  \n",
      "52       2   0      normal       0  \n",
      "297      2   1      normal       0  \n",
      "179      1   3      normal       0  \n",
      "79       2   0  reversible       0  \n",
      "292      1   1  reversible       1  \n",
      "55       1   0      normal       0  \n",
      "103      3   1      normal       1  \n",
      "66       2   1  reversible       1  \n",
      "33       2   0  reversible       0  \n",
      "122      2   2       fixed       1  \n",
      "73       2   1      normal       0  \n",
      "2        2   2  reversible       0  \n",
      "74       2   0      normal       0  \n",
      "77       1   0      normal       0  \n",
      "277      2   2  reversible       1  \n",
      "90       2   0  reversible       0  \n",
      "252      1   1           2       0  \n",
      "299      2   0  reversible       1  \n",
      "224      1   1      normal       0  \n",
      "276      2   1  reversible       1  \n",
      "70       1   3  reversible       1  \n",
      "290      2   3  reversible       1  \n",
      "240      1   0      normal       0  \n",
      "126      1   1      normal       0  \n",
      "172      1   0      normal       0  \n",
      "265      1   0      normal       0  \n",
      "301      1   2  reversible       1  \n",
      "159      2   0      normal       0  \n",
      "71       2   0      normal       0  \n",
      "208      1   2      normal       1  \n",
      "41       1   0      normal       0  \n",
      "220      1   2      normal       0  \n",
      "227      1   0      normal       0  \n",
      "148      1   0      normal       0  \n",
      "16       3   0  reversible       0  \n",
      "223      2   0      normal       0  \n",
      "231      3   3       fixed       1  \n",
      "136      1   1  reversible       0  \n",
      "85       3   0  reversible       1  \n",
      "206      1   0  reversible       0  \n",
      "302      2   3  reversible       1  \n",
      "256      1   0      normal       0  \n",
      "267      2   1  reversible       0  \n",
      "100      2   0      normal       0  \n",
      "69       1   0      normal       0  \n",
      "157      1   0      normal       0  \n",
      "174      1   1      normal       0  \n",
      "50       1   0  reversible       1  \n",
      "175      2   1      normal       1  \n",
      "181      1   1      normal       0  \n",
      "219      2   0      normal       0  \n"
     ]
    }
   ],
   "source": [
    "model.evaluate(test_ds)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BatchDataset' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-e5cb4b876385>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'BatchDataset' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "train_ds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.keras.layers.core.Dense object at 0x138fffcc0>\n"
     ]
    }
   ],
   "source": [
    "print(model.layers[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('/Users/AF45008/Research/ColinML/coneopt/data/pima_indian_data.csv')\n",
    "\n",
    "# creating input features and target variables\n",
    "X = np.asarray(dataset.iloc[:,0:8], dtype=np.float32)\n",
    "y = np.asarray(dataset.iloc[:,8], dtype=np.float32)\n",
    "\n",
    "#standardizing the input feature\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X = sc.fit_transform(X)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "#Build the model\n",
    "class model:\n",
    "    \n",
    "    def __init__(self):\n",
    "        xavier=tf.keras.initializers.GlorotUniform()\n",
    "        self.l1=tf.keras.layers.Dense(\n",
    "            4, \n",
    "            kernel_initializer=xavier, \n",
    "            #activation=tf.nn.linear, \n",
    "            input_shape=[1])\n",
    "        #self.l2=tf.keras.layers.Dense(\n",
    "        #    2, \n",
    "        #    kernel_initializer=xavier, \n",
    "        #    #activation=tf.nn.linear\n",
    "        #)\n",
    "        self.out=tf.keras.layers.Dense(\n",
    "            1, \n",
    "            kernel_initializer=xavier)\n",
    "        self.train_op = tf.keras.optimizers.Adagrad(learning_rate=0.1)\n",
    "        print(self.out)\n",
    "        \n",
    "    # Running the model\n",
    "    def run(self,X):\n",
    "        boom=self.l1(X)\n",
    "        #boom1=self.l2(boom)\n",
    "        boom1=boom\n",
    "        boom2=self.out(boom1)\n",
    "        return boom2\n",
    "    \n",
    "    #Custom loss fucntion\n",
    "    def get_loss(self,X,Y):\n",
    "        boom=self.l1(X)\n",
    "        #boom1=self.l2(boom)\n",
    "        boom1=boom\n",
    "        boom2=self.out(boom1)\n",
    "        return tf.math.square(boom2-Y)\n",
    "    \n",
    "    # get gradients\n",
    "    def get_grad(self,X,Y):\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(self.l1.variables)\n",
    "            #tape.watch(self.l2.variables)\n",
    "            tape.watch(self.out.variables)\n",
    "            L = self.get_loss(X,Y)\n",
    "            g = tape.gradient(L, [self.l1.variables[0],\n",
    "                                  self.l1.variables[1],\n",
    "                                  #self.l2.variables[0],\n",
    "                                  #self.l2.variables[1],\n",
    "                                  self.out.variables[0],\n",
    "                                  self.out.variables[1]])\n",
    "        return g\n",
    "    \n",
    "    # get gradients\n",
    "    def get_grad2(self, X):\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(self.l1.variables)\n",
    "            #tape.watch(self.l2.variables)\n",
    "            tape.watch(self.out.variables)\n",
    "            g = tape.gradient(self.run(X), \n",
    "                              [self.l1.variables[0],\n",
    "                               self.l1.variables[1],\n",
    "                                #self.l2.variables[0],\n",
    "                                #self.l2.variables[1],\n",
    "                                self.out.variables[0],\n",
    "                                self.out.variables[1]])\n",
    "        return g\n",
    "    \n",
    "    # perform gradient descent\n",
    "    def network_learn(self,X,Y):\n",
    "        g = self.get_grad(X,Y)\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"g={g}\")\n",
    "        # print(self.var)\n",
    "        self.train_op.apply_gradients(\n",
    "            zip(g, [self.l1.variables[0],\n",
    "                    self.l1.variables[1],\n",
    "                    #self.l2.variables[0],\n",
    "                    #self.l2.variables[1],\n",
    "                    self.out.variables[0],\n",
    "                    self.out.variables[1]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.keras.layers.core.Dense object at 0x13a511da0>\n",
      "================================================================================\n",
      "g=[<tf.Tensor: shape=(8, 4), dtype=float32, numpy=\n",
      "array([[ 839952.7  ,  350051.06 , -155563.7  , -748251.56 ],\n",
      "       [ 206270.97 ,   85963.61 ,  -38202.477, -183751.52 ],\n",
      "       [  51173.133,   21326.447,   -9477.538,  -45586.355],\n",
      "       [-389902.53 , -162492.23 ,   72212.02 ,  347335.25 ],\n",
      "       [-152099.31 ,  -63387.53 ,   28169.602,  135494.   ],\n",
      "       [ 315516.84 ,  131491.92 ,  -58435.4  , -281070.56 ],\n",
      "       [-176917.38 ,  -73730.47 ,   32766.035,  157602.56 ],\n",
      "       [1006093.5  ,  419290.38 , -186333.9  , -896254.1  ]],\n",
      "      dtype=float32)>, <tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 249008.38 ,  103774.484,  -46117.688, -221823.02 ], dtype=float32)>, <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\n",
      "array([[ -708786.06],\n",
      "       [-1098114.4 ],\n",
      "       [  100387.67],\n",
      "       [  816072.3 ]], dtype=float32)>, <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-228848.4], dtype=float32)>]\n",
      "================================================================================\n",
      "g=[<tf.Tensor: shape=(8, 4), dtype=float32, numpy=\n",
      "array([[ 394074.2   ,  140969.5   ,  -40488.773 , -346697.38  ],\n",
      "       [  16450.414 ,    5884.6895,   -1690.1808,  -14472.6875],\n",
      "       [-111259.414 ,  -39800.074 ,   11431.239 ,   97883.46  ],\n",
      "       [-107077.11  ,  -38303.973 ,   11001.532 ,   94203.97  ],\n",
      "       [  59705.906 ,   21358.19  ,   -6134.425 ,  -52527.875 ],\n",
      "       [ 199767.03  ,   71461.3   ,  -20524.867 , -175750.42  ],\n",
      "       [  34489.21  ,   12337.588 ,   -3543.559 ,  -30342.805 ],\n",
      "       [ 489362.22  ,  175056.22  ,  -50279.03  , -430529.5   ]],\n",
      "      dtype=float32)>, <tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 26449.084 ,   9461.441 ,  -2717.4875, -23269.275 ], dtype=float32)>, <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\n",
      "array([[-493309.28],\n",
      "       [-384075.34],\n",
      "       [ -35134.67],\n",
      "       [ 157047.64]], dtype=float32)>, <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-26767.795], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "#Custom training\n",
    "#x=[1,2,3,4,5,6,7,8,9,10]\n",
    "#x=np.asarray(x,dtype=np.float32).reshape((10,1))\n",
    "#y=[1,4,9,16,25,36,49,64,81,100]\n",
    "#y=np.asarray(y,dtype=np.float32).reshape((10,1))\n",
    "\n",
    "m=model()\n",
    "\n",
    "for i in range(2):\n",
    "    m.network_learn(X_train, y_train)\n",
    "\n",
    "    \n",
    "# Test Case\n",
    "#x=[11]\n",
    "#x=np.asarray(x,dtype=np.float32).reshape((1,1))\n",
    "#print(model.run(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(8, 4), dtype=float32, numpy=\n",
       " array([[ 273064.06  ,   93991.55  ,  -39466.55  , -249433.67  ],\n",
       "        [ -58423.203 ,  -20109.889 ,    8444.035 ,   53367.38  ],\n",
       "        [ -26712.32  ,   -9194.664 ,    3860.791 ,   24400.691 ],\n",
       "        [ -97421.58  ,  -33533.54  ,   14080.555 ,   88990.92  ],\n",
       "        [   5197.2783,    1788.9595,    -751.1743,   -4747.508 ],\n",
       "        [ 124758.64  ,   42943.246 ,  -18031.639 , -113962.266 ],\n",
       "        [ -14599.819 ,   -5025.41  ,    2110.1448,   13336.376 ],\n",
       "        [ 367553.97  ,  126515.984 ,  -53123.39  , -335746.62  ]],\n",
       "       dtype=float32)>,\n",
       " <tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 7214.6   ,  2483.3396, -1042.7422, -6590.259 ], dtype=float32)>,\n",
       " <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\n",
       " array([[-236536.97 ],\n",
       "        [-210715.67 ],\n",
       "        [   9073.932],\n",
       "        [ 198444.3  ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-7749.5654], dtype=float32)>]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = m.get_grad(X_train, y_train)\n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: shape=(8, 4), dtype=float32, numpy=\n",
      "array([[-0.04359787, -0.01500685,  0.0063013 ,  0.039825  ],\n",
      "       [ 0.84070694,  0.28938025, -0.12150922, -0.7679539 ],\n",
      "       [-0.5241901 , -0.18043183,  0.07576235,  0.4788278 ],\n",
      "       [ 1.1980461 ,  0.41238022, -0.17315625, -1.0943696 ],\n",
      "       [ 0.64568156,  0.22225046, -0.09332178, -0.5898056 ],\n",
      "       [-1.2055811 , -0.41497383,  0.17424528,  1.1012526 ],\n",
      "       [ 0.65948224,  0.2270008 , -0.09531642, -0.60241205],\n",
      "       [ 0.33443257,  0.11511525, -0.04833627, -0.30549145]],\n",
      "      dtype=float32)>, <tf.Tensor: shape=(4,), dtype=float32, numpy=array([-0.93096817, -0.32044914,  0.13455488,  0.8504041 ], dtype=float32)>, <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\n",
      "array([[-0.8798704],\n",
      "       [-1.5463147],\n",
      "       [-0.1866839],\n",
      "       [-2.1644511]], dtype=float32)>, <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>]\n",
      "================================================================================\n",
      "[<tf.Tensor: shape=(8, 4), dtype=float32, numpy=\n",
      "array([[-0.87267804, -0.30038506,  0.12613007,  0.7971583 ],\n",
      "       [-1.1110929 , -0.38244998,  0.1605887 ,  1.0149412 ],\n",
      "       [ 0.24538295,  0.08446342, -0.03546574, -0.224148  ],\n",
      "       [ 1.1980461 ,  0.41238022, -0.17315625, -1.0943696 ],\n",
      "       [ 0.64568156,  0.22225046, -0.09332178, -0.5898056 ],\n",
      "       [ 0.5420627 ,  0.18658374, -0.07834551, -0.4951537 ],\n",
      "       [ 0.4993006 ,  0.17186457, -0.07216501, -0.45609215],\n",
      "       [-0.53751045, -0.18501683,  0.07768757,  0.4909954 ]],\n",
      "      dtype=float32)>, <tf.Tensor: shape=(4,), dtype=float32, numpy=array([-0.93096817, -0.32044914,  0.13455488,  0.8504041 ], dtype=float32)>, <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\n",
      "array([[-0.7263459 ],\n",
      "       [ 1.8670055 ],\n",
      "       [ 0.3546153 ],\n",
      "       [ 0.12431362]], dtype=float32)>, <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>]\n",
      "[<tf.Tensor: shape=(8, 4), dtype=float32, numpy=\n",
      "array([[-0.9162759 , -0.3153919 ,  0.13243137,  0.8369833 ],\n",
      "       [-0.27038598, -0.09306973,  0.03907947,  0.24698734],\n",
      "       [-0.27880716, -0.09596841,  0.04029662,  0.2546798 ],\n",
      "       [ 2.3960922 ,  0.82476044, -0.3463125 , -2.1887393 ],\n",
      "       [ 1.2913631 ,  0.44450092, -0.18664356, -1.1796112 ],\n",
      "       [-0.66351837, -0.22839008,  0.09589977,  0.6060989 ],\n",
      "       [ 1.1587828 ,  0.39886537, -0.16748144, -1.0585042 ],\n",
      "       [-0.20307788, -0.06990158,  0.02935129,  0.18550396]],\n",
      "      dtype=float32)>, <tf.Tensor: shape=(4,), dtype=float32, numpy=array([-1.8619363 , -0.6408983 ,  0.26910976,  1.7008082 ], dtype=float32)>, <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\n",
      "array([[-1.6062164 ],\n",
      "       [ 0.320691  ],\n",
      "       [ 0.16793141],\n",
      "       [-2.0401378 ]], dtype=float32)>, <tf.Tensor: shape=(1,), dtype=float32, numpy=array([2.], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "print(m.get_grad2(X_train[0:1,:]))\n",
    "print(\"=\" * 80)\n",
    "print(m.get_grad2(X_train[1:2,:]))\n",
    "print(m.get_grad2(X_train[0:2,:]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(8, 4), dtype=float32, numpy=\n",
       " array([[-3.573741  ,  4.4614577 , -2.0754642 ,  1.1735111 ],\n",
       "        [-3.9608998 ,  4.9447885 , -2.3003097 ,  1.3006427 ],\n",
       "        [-0.6210106 ,  0.77526915, -0.36065477,  0.20392124],\n",
       "        [ 3.7513583 , -4.6831937 ,  2.1786158 , -1.2318348 ],\n",
       "        [-0.7751977 ,  0.9677548 , -0.4501989 ,  0.25455114],\n",
       "        [ 3.4337068 , -4.286639  ,  1.9941385 , -1.1275274 ],\n",
       "        [-2.8637142 ,  3.5750604 , -1.6631137 ,  0.9403584 ],\n",
       "        [-4.6870346 ,  5.851292  , -2.7220147 ,  1.5390848 ]],\n",
       "       dtype=float32)>,\n",
       " <tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 65.345055, -81.576866,  37.949524, -21.457415], dtype=float32)>,\n",
       " <tf.Tensor: shape=(4, 2), dtype=float32, numpy=\n",
       " array([[ 15.225409 ,   7.9595876],\n",
       "        [-18.307314 ,  -9.570757 ],\n",
       "        [ 14.491468 ,   7.5758953],\n",
       "        [-16.125036 ,  -8.429895 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(2,), dtype=float32, numpy=array([138.7106 ,  72.51534], dtype=float32)>,\n",
       " <tf.Tensor: shape=(2, 1), dtype=float32, numpy=\n",
       " array([[74.71012 ],\n",
       "        [12.841896]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1,), dtype=float32, numpy=array([231.], dtype=float32)>]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g2_test = m.get_grad2(X_test)\n",
    "g2_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.24188049]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(m.run(X_test[0:1,]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.ones((2, 2))\n",
    "\n",
    "with tf.GradientTape() as t:\n",
    "    t.watch(x)\n",
    "    y = tf.reduce_sum(x)\n",
    "    z = tf.multiply(y, y)\n",
    "\n",
    "# Derivative of z with respect to the original input tensor x\n",
    "dz_dx = t.gradient(z, x)\n",
    "for i in [0, 1]:\n",
    "    for j in [0, 1]:\n",
    "        assert dz_dx[i][j].numpy() == 8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[1., 1.],\n",
       "       [1., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10 samples\n",
      "Epoch 1/1000\n",
      "10/10 [==============================] - 0s 34ms/sample - loss: 2660.8171\n",
      "Epoch 2/1000\n",
      "10/10 [==============================] - 0s 124us/sample - loss: 2635.6772\n",
      "Epoch 3/1000\n",
      "10/10 [==============================] - 0s 198us/sample - loss: 2618.0007\n",
      "Epoch 4/1000\n",
      "10/10 [==============================] - 0s 145us/sample - loss: 2603.4373\n",
      "Epoch 5/1000\n",
      "10/10 [==============================] - 0s 184us/sample - loss: 2590.6538\n",
      "Epoch 6/1000\n",
      "10/10 [==============================] - 0s 191us/sample - loss: 2579.0430\n",
      "Epoch 7/1000\n",
      "10/10 [==============================] - 0s 206us/sample - loss: 2567.2781\n",
      "Epoch 8/1000\n",
      "10/10 [==============================] - 0s 234us/sample - loss: 2553.9441\n",
      "Epoch 9/1000\n",
      "10/10 [==============================] - 0s 222us/sample - loss: 2541.9351\n",
      "Epoch 10/1000\n",
      "10/10 [==============================] - 0s 260us/sample - loss: 2530.7222\n",
      "Epoch 11/1000\n",
      "10/10 [==============================] - 0s 259us/sample - loss: 2520.2920\n",
      "Epoch 12/1000\n",
      "10/10 [==============================] - 0s 207us/sample - loss: 2510.5376\n",
      "Epoch 13/1000\n",
      "10/10 [==============================] - 0s 254us/sample - loss: 2501.8210\n",
      "Epoch 14/1000\n",
      "10/10 [==============================] - 0s 222us/sample - loss: 2493.4617\n",
      "Epoch 15/1000\n",
      "10/10 [==============================] - 0s 292us/sample - loss: 2485.4863\n",
      "Epoch 16/1000\n",
      "10/10 [==============================] - 0s 223us/sample - loss: 2478.0571\n",
      "Epoch 17/1000\n",
      "10/10 [==============================] - 0s 420us/sample - loss: 2470.5425\n",
      "Epoch 18/1000\n",
      "10/10 [==============================] - 0s 341us/sample - loss: 2463.2515\n",
      "Epoch 19/1000\n",
      "10/10 [==============================] - 0s 409us/sample - loss: 2455.9800\n",
      "Epoch 20/1000\n",
      "10/10 [==============================] - 0s 339us/sample - loss: 2448.8992\n",
      "Epoch 21/1000\n",
      "10/10 [==============================] - 0s 241us/sample - loss: 2441.5793\n",
      "Epoch 22/1000\n",
      "10/10 [==============================] - 0s 337us/sample - loss: 2434.7478\n",
      "Epoch 23/1000\n",
      "10/10 [==============================] - 0s 291us/sample - loss: 2427.5203\n",
      "Epoch 24/1000\n",
      "10/10 [==============================] - 0s 344us/sample - loss: 2420.4634\n",
      "Epoch 25/1000\n",
      "10/10 [==============================] - 0s 371us/sample - loss: 2413.5378\n",
      "Epoch 26/1000\n",
      "10/10 [==============================] - 0s 250us/sample - loss: 2406.3921\n",
      "Epoch 27/1000\n",
      "10/10 [==============================] - 0s 265us/sample - loss: 2399.4116\n",
      "Epoch 28/1000\n",
      "10/10 [==============================] - 0s 294us/sample - loss: 2392.4961\n",
      "Epoch 29/1000\n",
      "10/10 [==============================] - 0s 425us/sample - loss: 2385.3708\n",
      "Epoch 30/1000\n",
      "10/10 [==============================] - 0s 449us/sample - loss: 2378.2812\n",
      "Epoch 31/1000\n",
      "10/10 [==============================] - 0s 335us/sample - loss: 2371.2844\n",
      "Epoch 32/1000\n",
      "10/10 [==============================] - 0s 281us/sample - loss: 2364.2437\n",
      "Epoch 33/1000\n",
      "10/10 [==============================] - 0s 309us/sample - loss: 2357.3350\n",
      "Epoch 34/1000\n",
      "10/10 [==============================] - 0s 328us/sample - loss: 2350.1899\n",
      "Epoch 35/1000\n",
      "10/10 [==============================] - 0s 427us/sample - loss: 2343.0503\n",
      "Epoch 36/1000\n",
      "10/10 [==============================] - 0s 457us/sample - loss: 2335.9272\n",
      "Epoch 37/1000\n",
      "10/10 [==============================] - 0s 397us/sample - loss: 2328.8159\n",
      "Epoch 38/1000\n",
      "10/10 [==============================] - 0s 287us/sample - loss: 2321.7014\n",
      "Epoch 39/1000\n",
      "10/10 [==============================] - 0s 323us/sample - loss: 2314.6699\n",
      "Epoch 40/1000\n",
      "10/10 [==============================] - 0s 254us/sample - loss: 2307.4292\n",
      "Epoch 41/1000\n",
      "10/10 [==============================] - 0s 555us/sample - loss: 2300.1731\n",
      "Epoch 42/1000\n",
      "10/10 [==============================] - 0s 328us/sample - loss: 2292.9043\n",
      "Epoch 43/1000\n",
      "10/10 [==============================] - 0s 266us/sample - loss: 2285.6282\n",
      "Epoch 44/1000\n",
      "10/10 [==============================] - 0s 247us/sample - loss: 2278.3411\n",
      "Epoch 45/1000\n",
      "10/10 [==============================] - 0s 373us/sample - loss: 2271.0396\n",
      "Epoch 46/1000\n",
      "10/10 [==============================] - 0s 305us/sample - loss: 2263.7690\n",
      "Epoch 47/1000\n",
      "10/10 [==============================] - 0s 382us/sample - loss: 2256.5911\n",
      "Epoch 48/1000\n",
      "10/10 [==============================] - 0s 376us/sample - loss: 2249.5881\n",
      "Epoch 49/1000\n",
      "10/10 [==============================] - 0s 285us/sample - loss: 2242.6067\n",
      "Epoch 50/1000\n",
      "10/10 [==============================] - 0s 377us/sample - loss: 2235.6477\n",
      "Epoch 51/1000\n",
      "10/10 [==============================] - 0s 409us/sample - loss: 2228.7598\n",
      "Epoch 52/1000\n",
      "10/10 [==============================] - 0s 361us/sample - loss: 2222.0164\n",
      "Epoch 53/1000\n",
      "10/10 [==============================] - 0s 333us/sample - loss: 2215.5305\n",
      "Epoch 54/1000\n",
      "10/10 [==============================] - 0s 434us/sample - loss: 2209.1577\n",
      "Epoch 55/1000\n",
      "10/10 [==============================] - 0s 286us/sample - loss: 2202.7261\n",
      "Epoch 56/1000\n",
      "10/10 [==============================] - 0s 341us/sample - loss: 2196.2329\n",
      "Epoch 57/1000\n",
      "10/10 [==============================] - 0s 416us/sample - loss: 2189.6763\n",
      "Epoch 58/1000\n",
      "10/10 [==============================] - 0s 402us/sample - loss: 2183.0552\n",
      "Epoch 59/1000\n",
      "10/10 [==============================] - 0s 397us/sample - loss: 2176.3694\n",
      "Epoch 60/1000\n",
      "10/10 [==============================] - 0s 302us/sample - loss: 2169.6172\n",
      "Epoch 61/1000\n",
      "10/10 [==============================] - 0s 361us/sample - loss: 2162.7981\n",
      "Epoch 62/1000\n",
      "10/10 [==============================] - 0s 297us/sample - loss: 2155.9126\n",
      "Epoch 63/1000\n",
      "10/10 [==============================] - 0s 308us/sample - loss: 2148.9600\n",
      "Epoch 64/1000\n",
      "10/10 [==============================] - 0s 424us/sample - loss: 2141.9446\n",
      "Epoch 65/1000\n",
      "10/10 [==============================] - 0s 355us/sample - loss: 2134.8745\n",
      "Epoch 66/1000\n",
      "10/10 [==============================] - 0s 356us/sample - loss: 2127.7847\n",
      "Epoch 67/1000\n",
      "10/10 [==============================] - 0s 426us/sample - loss: 2120.6880\n",
      "Epoch 68/1000\n",
      "10/10 [==============================] - 0s 303us/sample - loss: 2113.5339\n",
      "Epoch 69/1000\n",
      "10/10 [==============================] - 0s 363us/sample - loss: 2106.3164\n",
      "Epoch 70/1000\n",
      "10/10 [==============================] - 0s 338us/sample - loss: 2099.0276\n",
      "Epoch 71/1000\n",
      "10/10 [==============================] - 0s 463us/sample - loss: 2091.6738\n",
      "Epoch 72/1000\n",
      "10/10 [==============================] - 0s 446us/sample - loss: 2084.2524\n",
      "Epoch 73/1000\n",
      "10/10 [==============================] - 0s 388us/sample - loss: 2076.7678\n",
      "Epoch 74/1000\n",
      "10/10 [==============================] - 0s 346us/sample - loss: 2069.2190\n",
      "Epoch 75/1000\n",
      "10/10 [==============================] - 0s 486us/sample - loss: 2061.6099\n",
      "Epoch 76/1000\n",
      "10/10 [==============================] - 0s 243us/sample - loss: 2053.9395\n",
      "Epoch 77/1000\n",
      "10/10 [==============================] - 0s 239us/sample - loss: 2046.2057\n",
      "Epoch 78/1000\n",
      "10/10 [==============================] - 0s 291us/sample - loss: 2038.4059\n",
      "Epoch 79/1000\n",
      "10/10 [==============================] - 0s 180us/sample - loss: 2030.5397\n",
      "Epoch 80/1000\n",
      "10/10 [==============================] - 0s 185us/sample - loss: 2022.6084\n",
      "Epoch 81/1000\n",
      "10/10 [==============================] - 0s 208us/sample - loss: 2014.6130\n",
      "Epoch 82/1000\n",
      "10/10 [==============================] - 0s 191us/sample - loss: 2006.5554\n",
      "Epoch 83/1000\n",
      "10/10 [==============================] - 0s 278us/sample - loss: 1998.4379\n",
      "Epoch 84/1000\n",
      "10/10 [==============================] - 0s 268us/sample - loss: 1990.2629\n",
      "Epoch 85/1000\n",
      "10/10 [==============================] - 0s 183us/sample - loss: 1982.0258\n",
      "Epoch 86/1000\n",
      "10/10 [==============================] - 0s 220us/sample - loss: 1973.7263\n",
      "Epoch 87/1000\n",
      "10/10 [==============================] - 0s 183us/sample - loss: 1965.3668\n",
      "Epoch 88/1000\n",
      "10/10 [==============================] - 0s 181us/sample - loss: 1956.9447\n",
      "Epoch 89/1000\n",
      "10/10 [==============================] - 0s 173us/sample - loss: 1948.4623\n",
      "Epoch 90/1000\n",
      "10/10 [==============================] - 0s 178us/sample - loss: 1939.9205\n",
      "Epoch 91/1000\n",
      "10/10 [==============================] - 0s 199us/sample - loss: 1931.3193\n",
      "Epoch 92/1000\n",
      "10/10 [==============================] - 0s 233us/sample - loss: 1922.6586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93/1000\n",
      "10/10 [==============================] - 0s 210us/sample - loss: 1913.9395\n",
      "Epoch 94/1000\n",
      "10/10 [==============================] - 0s 265us/sample - loss: 1905.1631\n",
      "Epoch 95/1000\n",
      "10/10 [==============================] - 0s 252us/sample - loss: 1896.3289\n",
      "Epoch 96/1000\n",
      "10/10 [==============================] - 0s 275us/sample - loss: 1887.4375\n",
      "Epoch 97/1000\n",
      "10/10 [==============================] - 0s 274us/sample - loss: 1878.4893\n",
      "Epoch 98/1000\n",
      "10/10 [==============================] - 0s 195us/sample - loss: 1869.4854\n",
      "Epoch 99/1000\n",
      "10/10 [==============================] - 0s 298us/sample - loss: 1860.4258\n",
      "Epoch 100/1000\n",
      "10/10 [==============================] - 0s 444us/sample - loss: 1851.3113\n",
      "Epoch 101/1000\n",
      "10/10 [==============================] - 0s 193us/sample - loss: 1842.1420\n",
      "Epoch 102/1000\n",
      "10/10 [==============================] - 0s 195us/sample - loss: 1832.9183\n",
      "Epoch 103/1000\n",
      "10/10 [==============================] - 0s 284us/sample - loss: 1823.6414\n",
      "Epoch 104/1000\n",
      "10/10 [==============================] - 0s 258us/sample - loss: 1814.3109\n",
      "Epoch 105/1000\n",
      "10/10 [==============================] - 0s 205us/sample - loss: 1804.9277\n",
      "Epoch 106/1000\n",
      "10/10 [==============================] - 0s 177us/sample - loss: 1795.4924\n",
      "Epoch 107/1000\n",
      "10/10 [==============================] - 0s 210us/sample - loss: 1786.0062\n",
      "Epoch 108/1000\n",
      "10/10 [==============================] - 0s 216us/sample - loss: 1776.4678\n",
      "Epoch 109/1000\n",
      "10/10 [==============================] - 0s 451us/sample - loss: 1766.8789\n",
      "Epoch 110/1000\n",
      "10/10 [==============================] - 0s 189us/sample - loss: 1757.2400\n",
      "Epoch 111/1000\n",
      "10/10 [==============================] - 0s 198us/sample - loss: 1747.5518\n",
      "Epoch 112/1000\n",
      "10/10 [==============================] - 0s 213us/sample - loss: 1737.8141\n",
      "Epoch 113/1000\n",
      "10/10 [==============================] - 0s 406us/sample - loss: 1728.0280\n",
      "Epoch 114/1000\n",
      "10/10 [==============================] - 0s 213us/sample - loss: 1718.1934\n",
      "Epoch 115/1000\n",
      "10/10 [==============================] - 0s 206us/sample - loss: 1708.3118\n",
      "Epoch 116/1000\n",
      "10/10 [==============================] - 0s 220us/sample - loss: 1698.3834\n",
      "Epoch 117/1000\n",
      "10/10 [==============================] - 0s 332us/sample - loss: 1688.4089\n",
      "Epoch 118/1000\n",
      "10/10 [==============================] - 0s 254us/sample - loss: 1678.3889\n",
      "Epoch 119/1000\n",
      "10/10 [==============================] - 0s 204us/sample - loss: 1668.3235\n",
      "Epoch 120/1000\n",
      "10/10 [==============================] - 0s 206us/sample - loss: 1658.2133\n",
      "Epoch 121/1000\n",
      "10/10 [==============================] - 0s 331us/sample - loss: 1648.0593\n",
      "Epoch 122/1000\n",
      "10/10 [==============================] - 0s 322us/sample - loss: 1637.8621\n",
      "Epoch 123/1000\n",
      "10/10 [==============================] - 0s 258us/sample - loss: 1627.6221\n",
      "Epoch 124/1000\n",
      "10/10 [==============================] - 0s 326us/sample - loss: 1617.3403\n",
      "Epoch 125/1000\n",
      "10/10 [==============================] - 0s 269us/sample - loss: 1607.0173\n",
      "Epoch 126/1000\n",
      "10/10 [==============================] - 0s 313us/sample - loss: 1596.6536\n",
      "Epoch 127/1000\n",
      "10/10 [==============================] - 0s 320us/sample - loss: 1586.2513\n",
      "Epoch 128/1000\n",
      "10/10 [==============================] - 0s 314us/sample - loss: 1575.8093\n",
      "Epoch 129/1000\n",
      "10/10 [==============================] - 0s 295us/sample - loss: 1565.3284\n",
      "Epoch 130/1000\n",
      "10/10 [==============================] - 0s 250us/sample - loss: 1554.8112\n",
      "Epoch 131/1000\n",
      "10/10 [==============================] - 0s 311us/sample - loss: 1544.2566\n",
      "Epoch 132/1000\n",
      "10/10 [==============================] - 0s 279us/sample - loss: 1533.6650\n",
      "Epoch 133/1000\n",
      "10/10 [==============================] - 0s 310us/sample - loss: 1523.0388\n",
      "Epoch 134/1000\n",
      "10/10 [==============================] - 0s 352us/sample - loss: 1512.3778\n",
      "Epoch 135/1000\n",
      "10/10 [==============================] - 0s 305us/sample - loss: 1501.6824\n",
      "Epoch 136/1000\n",
      "10/10 [==============================] - 0s 314us/sample - loss: 1490.9534\n",
      "Epoch 137/1000\n",
      "10/10 [==============================] - 0s 371us/sample - loss: 1480.1918\n",
      "Epoch 138/1000\n",
      "10/10 [==============================] - 0s 236us/sample - loss: 1469.3984\n",
      "Epoch 139/1000\n",
      "10/10 [==============================] - 0s 241us/sample - loss: 1458.5740\n",
      "Epoch 140/1000\n",
      "10/10 [==============================] - 0s 201us/sample - loss: 1447.7197\n",
      "Epoch 141/1000\n",
      "10/10 [==============================] - 0s 313us/sample - loss: 1436.8359\n",
      "Epoch 142/1000\n",
      "10/10 [==============================] - 0s 315us/sample - loss: 1425.9241\n",
      "Epoch 143/1000\n",
      "10/10 [==============================] - 0s 229us/sample - loss: 1414.9846\n",
      "Epoch 144/1000\n",
      "10/10 [==============================] - 0s 460us/sample - loss: 1404.0189\n",
      "Epoch 145/1000\n",
      "10/10 [==============================] - 0s 265us/sample - loss: 1393.0277\n",
      "Epoch 146/1000\n",
      "10/10 [==============================] - 0s 227us/sample - loss: 1382.0120\n",
      "Epoch 147/1000\n",
      "10/10 [==============================] - 0s 459us/sample - loss: 1370.9724\n",
      "Epoch 148/1000\n",
      "10/10 [==============================] - 0s 380us/sample - loss: 1359.9105\n",
      "Epoch 149/1000\n",
      "10/10 [==============================] - 0s 270us/sample - loss: 1348.8269\n",
      "Epoch 150/1000\n",
      "10/10 [==============================] - 0s 307us/sample - loss: 1337.7227\n",
      "Epoch 151/1000\n",
      "10/10 [==============================] - 0s 286us/sample - loss: 1326.5989\n",
      "Epoch 152/1000\n",
      "10/10 [==============================] - 0s 268us/sample - loss: 1315.4565\n",
      "Epoch 153/1000\n",
      "10/10 [==============================] - 0s 266us/sample - loss: 1304.2968\n",
      "Epoch 154/1000\n",
      "10/10 [==============================] - 0s 304us/sample - loss: 1293.1202\n",
      "Epoch 155/1000\n",
      "10/10 [==============================] - 0s 348us/sample - loss: 1281.9285\n",
      "Epoch 156/1000\n",
      "10/10 [==============================] - 0s 202us/sample - loss: 1270.7225\n",
      "Epoch 157/1000\n",
      "10/10 [==============================] - 0s 316us/sample - loss: 1259.5033\n",
      "Epoch 158/1000\n",
      "10/10 [==============================] - 0s 228us/sample - loss: 1248.2721\n",
      "Epoch 159/1000\n",
      "10/10 [==============================] - 0s 305us/sample - loss: 1237.0339\n",
      "Epoch 160/1000\n",
      "10/10 [==============================] - 0s 395us/sample - loss: 1225.7821\n",
      "Epoch 161/1000\n",
      "10/10 [==============================] - 0s 332us/sample - loss: 1214.5210\n",
      "Epoch 162/1000\n",
      "10/10 [==============================] - 0s 223us/sample - loss: 1203.2523\n",
      "Epoch 163/1000\n",
      "10/10 [==============================] - 0s 227us/sample - loss: 1191.9773\n",
      "Epoch 164/1000\n",
      "10/10 [==============================] - 0s 319us/sample - loss: 1180.6971\n",
      "Epoch 165/1000\n",
      "10/10 [==============================] - 0s 202us/sample - loss: 1169.4130\n",
      "Epoch 166/1000\n",
      "10/10 [==============================] - 0s 220us/sample - loss: 1158.1262\n",
      "Epoch 167/1000\n",
      "10/10 [==============================] - 0s 294us/sample - loss: 1146.8381\n",
      "Epoch 168/1000\n",
      "10/10 [==============================] - 0s 367us/sample - loss: 1135.5496\n",
      "Epoch 169/1000\n",
      "10/10 [==============================] - 0s 192us/sample - loss: 1124.2618\n",
      "Epoch 170/1000\n",
      "10/10 [==============================] - 0s 198us/sample - loss: 1112.9767\n",
      "Epoch 171/1000\n",
      "10/10 [==============================] - 0s 234us/sample - loss: 1101.6949\n",
      "Epoch 172/1000\n",
      "10/10 [==============================] - 0s 713us/sample - loss: 1090.4180\n",
      "Epoch 173/1000\n",
      "10/10 [==============================] - 0s 330us/sample - loss: 1079.1471\n",
      "Epoch 174/1000\n",
      "10/10 [==============================] - 0s 410us/sample - loss: 1067.8838\n",
      "Epoch 175/1000\n",
      "10/10 [==============================] - 0s 214us/sample - loss: 1056.6292\n",
      "Epoch 176/1000\n",
      "10/10 [==============================] - 0s 208us/sample - loss: 1045.3848\n",
      "Epoch 177/1000\n",
      "10/10 [==============================] - 0s 219us/sample - loss: 1034.1516\n",
      "Epoch 178/1000\n",
      "10/10 [==============================] - 0s 277us/sample - loss: 1022.9315\n",
      "Epoch 179/1000\n",
      "10/10 [==============================] - 0s 220us/sample - loss: 1011.7254\n",
      "Epoch 180/1000\n",
      "10/10 [==============================] - 0s 344us/sample - loss: 1000.5350\n",
      "Epoch 181/1000\n",
      "10/10 [==============================] - 0s 185us/sample - loss: 989.3617\n",
      "Epoch 182/1000\n",
      "10/10 [==============================] - 0s 193us/sample - loss: 978.2067\n",
      "Epoch 183/1000\n",
      "10/10 [==============================] - 0s 396us/sample - loss: 967.0717\n",
      "Epoch 184/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 357us/sample - loss: 955.9576\n",
      "Epoch 185/1000\n",
      "10/10 [==============================] - 0s 325us/sample - loss: 944.8666\n",
      "Epoch 186/1000\n",
      "10/10 [==============================] - 0s 240us/sample - loss: 933.7998\n",
      "Epoch 187/1000\n",
      "10/10 [==============================] - 0s 243us/sample - loss: 922.7587\n",
      "Epoch 188/1000\n",
      "10/10 [==============================] - 0s 222us/sample - loss: 911.7446\n",
      "Epoch 189/1000\n",
      "10/10 [==============================] - 0s 315us/sample - loss: 900.7595\n",
      "Epoch 190/1000\n",
      "10/10 [==============================] - 0s 186us/sample - loss: 889.8043\n",
      "Epoch 191/1000\n",
      "10/10 [==============================] - 0s 182us/sample - loss: 878.8809\n",
      "Epoch 192/1000\n",
      "10/10 [==============================] - 0s 268us/sample - loss: 867.9908\n",
      "Epoch 193/1000\n",
      "10/10 [==============================] - 0s 202us/sample - loss: 857.1354\n",
      "Epoch 194/1000\n",
      "10/10 [==============================] - 0s 242us/sample - loss: 846.3165\n",
      "Epoch 195/1000\n",
      "10/10 [==============================] - 0s 291us/sample - loss: 835.5355\n",
      "Epoch 196/1000\n",
      "10/10 [==============================] - 0s 364us/sample - loss: 824.7938\n",
      "Epoch 197/1000\n",
      "10/10 [==============================] - 0s 216us/sample - loss: 814.0933\n",
      "Epoch 198/1000\n",
      "10/10 [==============================] - 0s 348us/sample - loss: 803.4357\n",
      "Epoch 199/1000\n",
      "10/10 [==============================] - 0s 259us/sample - loss: 792.8221\n",
      "Epoch 200/1000\n",
      "10/10 [==============================] - 0s 217us/sample - loss: 782.2546\n",
      "Epoch 201/1000\n",
      "10/10 [==============================] - 0s 249us/sample - loss: 771.7345\n",
      "Epoch 202/1000\n",
      "10/10 [==============================] - 0s 199us/sample - loss: 761.2638\n",
      "Epoch 203/1000\n",
      "10/10 [==============================] - 0s 221us/sample - loss: 750.8436\n",
      "Epoch 204/1000\n",
      "10/10 [==============================] - 0s 208us/sample - loss: 740.4762\n",
      "Epoch 205/1000\n",
      "10/10 [==============================] - 0s 210us/sample - loss: 730.1628\n",
      "Epoch 206/1000\n",
      "10/10 [==============================] - 0s 329us/sample - loss: 719.9052\n",
      "Epoch 207/1000\n",
      "10/10 [==============================] - 0s 212us/sample - loss: 709.7052\n",
      "Epoch 208/1000\n",
      "10/10 [==============================] - 0s 235us/sample - loss: 699.5642\n",
      "Epoch 209/1000\n",
      "10/10 [==============================] - 0s 247us/sample - loss: 689.4843\n",
      "Epoch 210/1000\n",
      "10/10 [==============================] - 0s 216us/sample - loss: 679.4669\n",
      "Epoch 211/1000\n",
      "10/10 [==============================] - 0s 187us/sample - loss: 669.5137\n",
      "Epoch 212/1000\n",
      "10/10 [==============================] - 0s 199us/sample - loss: 659.6266\n",
      "Epoch 213/1000\n",
      "10/10 [==============================] - 0s 228us/sample - loss: 649.8073\n",
      "Epoch 214/1000\n",
      "10/10 [==============================] - 0s 379us/sample - loss: 640.0573\n",
      "Epoch 215/1000\n",
      "10/10 [==============================] - 0s 203us/sample - loss: 630.3785\n",
      "Epoch 216/1000\n",
      "10/10 [==============================] - 0s 175us/sample - loss: 620.7726\n",
      "Epoch 217/1000\n",
      "10/10 [==============================] - 0s 207us/sample - loss: 611.2413\n",
      "Epoch 218/1000\n",
      "10/10 [==============================] - 0s 286us/sample - loss: 601.7865\n",
      "Epoch 219/1000\n",
      "10/10 [==============================] - 0s 191us/sample - loss: 592.4097\n",
      "Epoch 220/1000\n",
      "10/10 [==============================] - 0s 191us/sample - loss: 583.1128\n",
      "Epoch 221/1000\n",
      "10/10 [==============================] - 0s 191us/sample - loss: 573.8976\n",
      "Epoch 222/1000\n",
      "10/10 [==============================] - 0s 231us/sample - loss: 564.7656\n",
      "Epoch 223/1000\n",
      "10/10 [==============================] - 0s 289us/sample - loss: 555.7188\n",
      "Epoch 224/1000\n",
      "10/10 [==============================] - 0s 283us/sample - loss: 546.7588\n",
      "Epoch 225/1000\n",
      "10/10 [==============================] - 0s 302us/sample - loss: 537.8875\n",
      "Epoch 226/1000\n",
      "10/10 [==============================] - 0s 323us/sample - loss: 529.1064\n",
      "Epoch 227/1000\n",
      "10/10 [==============================] - 0s 235us/sample - loss: 520.4174\n",
      "Epoch 228/1000\n",
      "10/10 [==============================] - 0s 207us/sample - loss: 511.8222\n",
      "Epoch 229/1000\n",
      "10/10 [==============================] - 0s 171us/sample - loss: 503.3226\n",
      "Epoch 230/1000\n",
      "10/10 [==============================] - 0s 215us/sample - loss: 494.9202\n",
      "Epoch 231/1000\n",
      "10/10 [==============================] - 0s 212us/sample - loss: 486.6166\n",
      "Epoch 232/1000\n",
      "10/10 [==============================] - 0s 394us/sample - loss: 478.4140\n",
      "Epoch 233/1000\n",
      "10/10 [==============================] - 0s 321us/sample - loss: 470.3136\n",
      "Epoch 234/1000\n",
      "10/10 [==============================] - 0s 235us/sample - loss: 462.3172\n",
      "Epoch 235/1000\n",
      "10/10 [==============================] - 0s 375us/sample - loss: 454.4267\n",
      "Epoch 236/1000\n",
      "10/10 [==============================] - 0s 374us/sample - loss: 446.6433\n",
      "Epoch 237/1000\n",
      "10/10 [==============================] - 0s 262us/sample - loss: 438.9692\n",
      "Epoch 238/1000\n",
      "10/10 [==============================] - 0s 228us/sample - loss: 431.4056\n",
      "Epoch 239/1000\n",
      "10/10 [==============================] - 0s 202us/sample - loss: 423.9544\n",
      "Epoch 240/1000\n",
      "10/10 [==============================] - 0s 391us/sample - loss: 416.6171\n",
      "Epoch 241/1000\n",
      "10/10 [==============================] - 0s 233us/sample - loss: 409.3951\n",
      "Epoch 242/1000\n",
      "10/10 [==============================] - 0s 281us/sample - loss: 402.2901\n",
      "Epoch 243/1000\n",
      "10/10 [==============================] - 0s 311us/sample - loss: 395.3036\n",
      "Epoch 244/1000\n",
      "10/10 [==============================] - 0s 249us/sample - loss: 388.4371\n",
      "Epoch 245/1000\n",
      "10/10 [==============================] - 0s 292us/sample - loss: 381.6922\n",
      "Epoch 246/1000\n",
      "10/10 [==============================] - 0s 221us/sample - loss: 375.0700\n",
      "Epoch 247/1000\n",
      "10/10 [==============================] - 0s 399us/sample - loss: 368.5721\n",
      "Epoch 248/1000\n",
      "10/10 [==============================] - 0s 232us/sample - loss: 362.2000\n",
      "Epoch 249/1000\n",
      "10/10 [==============================] - 0s 234us/sample - loss: 355.9546\n",
      "Epoch 250/1000\n",
      "10/10 [==============================] - 0s 270us/sample - loss: 349.8375\n",
      "Epoch 251/1000\n",
      "10/10 [==============================] - 0s 323us/sample - loss: 343.8500\n",
      "Epoch 252/1000\n",
      "10/10 [==============================] - 0s 215us/sample - loss: 337.9930\n",
      "Epoch 253/1000\n",
      "10/10 [==============================] - 0s 284us/sample - loss: 332.2678\n",
      "Epoch 254/1000\n",
      "10/10 [==============================] - 0s 228us/sample - loss: 326.6752\n",
      "Epoch 255/1000\n",
      "10/10 [==============================] - 0s 255us/sample - loss: 321.2164\n",
      "Epoch 256/1000\n",
      "10/10 [==============================] - 0s 259us/sample - loss: 315.8924\n",
      "Epoch 257/1000\n",
      "10/10 [==============================] - 0s 389us/sample - loss: 310.7039\n",
      "Epoch 258/1000\n",
      "10/10 [==============================] - 0s 230us/sample - loss: 305.6516\n",
      "Epoch 259/1000\n",
      "10/10 [==============================] - 0s 210us/sample - loss: 300.7363\n",
      "Epoch 260/1000\n",
      "10/10 [==============================] - 0s 234us/sample - loss: 295.9587\n",
      "Epoch 261/1000\n",
      "10/10 [==============================] - 0s 209us/sample - loss: 291.3190\n",
      "Epoch 262/1000\n",
      "10/10 [==============================] - 0s 208us/sample - loss: 286.8177\n",
      "Epoch 263/1000\n",
      "10/10 [==============================] - 0s 181us/sample - loss: 282.4551\n",
      "Epoch 264/1000\n",
      "10/10 [==============================] - 0s 197us/sample - loss: 278.2312\n",
      "Epoch 265/1000\n",
      "10/10 [==============================] - 0s 231us/sample - loss: 274.1461\n",
      "Epoch 266/1000\n",
      "10/10 [==============================] - 0s 247us/sample - loss: 270.1994\n",
      "Epoch 267/1000\n",
      "10/10 [==============================] - 0s 232us/sample - loss: 266.3910\n",
      "Epoch 268/1000\n",
      "10/10 [==============================] - 0s 219us/sample - loss: 262.7202\n",
      "Epoch 269/1000\n",
      "10/10 [==============================] - 0s 218us/sample - loss: 259.1862\n",
      "Epoch 270/1000\n",
      "10/10 [==============================] - 0s 265us/sample - loss: 255.7880\n",
      "Epoch 271/1000\n",
      "10/10 [==============================] - 0s 197us/sample - loss: 252.5247\n",
      "Epoch 272/1000\n",
      "10/10 [==============================] - 0s 370us/sample - loss: 249.3944\n",
      "Epoch 273/1000\n",
      "10/10 [==============================] - 0s 194us/sample - loss: 246.3954\n",
      "Epoch 274/1000\n",
      "10/10 [==============================] - 0s 226us/sample - loss: 243.5257\n",
      "Epoch 275/1000\n",
      "10/10 [==============================] - 0s 337us/sample - loss: 240.7825\n",
      "Epoch 276/1000\n",
      "10/10 [==============================] - 0s 236us/sample - loss: 238.1632\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 277/1000\n",
      "10/10 [==============================] - 0s 221us/sample - loss: 235.6644\n",
      "Epoch 278/1000\n",
      "10/10 [==============================] - 0s 276us/sample - loss: 233.2824\n",
      "Epoch 279/1000\n",
      "10/10 [==============================] - 0s 202us/sample - loss: 231.0132\n",
      "Epoch 280/1000\n",
      "10/10 [==============================] - 0s 210us/sample - loss: 228.8526\n",
      "Epoch 281/1000\n",
      "10/10 [==============================] - 0s 329us/sample - loss: 226.7962\n",
      "Epoch 282/1000\n",
      "10/10 [==============================] - 0s 243us/sample - loss: 224.8396\n",
      "Epoch 283/1000\n",
      "10/10 [==============================] - 0s 222us/sample - loss: 222.9787\n",
      "Epoch 284/1000\n",
      "10/10 [==============================] - 0s 272us/sample - loss: 221.2095\n",
      "Epoch 285/1000\n",
      "10/10 [==============================] - 0s 177us/sample - loss: 219.5286\n",
      "Epoch 286/1000\n",
      "10/10 [==============================] - 0s 189us/sample - loss: 217.9333\n",
      "Epoch 287/1000\n",
      "10/10 [==============================] - 0s 200us/sample - loss: 216.4209\n",
      "Epoch 288/1000\n",
      "10/10 [==============================] - 0s 249us/sample - loss: 214.9894\n",
      "Epoch 289/1000\n",
      "10/10 [==============================] - 0s 218us/sample - loss: 213.6368\n",
      "Epoch 290/1000\n",
      "10/10 [==============================] - 0s 184us/sample - loss: 212.3612\n",
      "Epoch 291/1000\n",
      "10/10 [==============================] - 0s 214us/sample - loss: 211.1609\n",
      "Epoch 292/1000\n",
      "10/10 [==============================] - 0s 223us/sample - loss: 210.0336\n",
      "Epoch 293/1000\n",
      "10/10 [==============================] - 0s 220us/sample - loss: 208.9775\n",
      "Epoch 294/1000\n",
      "10/10 [==============================] - 0s 317us/sample - loss: 207.9899\n",
      "Epoch 295/1000\n",
      "10/10 [==============================] - 0s 177us/sample - loss: 207.0685\n",
      "Epoch 296/1000\n",
      "10/10 [==============================] - 0s 228us/sample - loss: 206.2104\n",
      "Epoch 297/1000\n",
      "10/10 [==============================] - 0s 228us/sample - loss: 205.4128\n",
      "Epoch 298/1000\n",
      "10/10 [==============================] - 0s 192us/sample - loss: 204.6724\n",
      "Epoch 299/1000\n",
      "10/10 [==============================] - 0s 229us/sample - loss: 203.9859\n",
      "Epoch 300/1000\n",
      "10/10 [==============================] - 0s 284us/sample - loss: 203.3501\n",
      "Epoch 301/1000\n",
      "10/10 [==============================] - 0s 286us/sample - loss: 202.7616\n",
      "Epoch 302/1000\n",
      "10/10 [==============================] - 0s 477us/sample - loss: 202.2163\n",
      "Epoch 303/1000\n",
      "10/10 [==============================] - 0s 229us/sample - loss: 201.7108\n",
      "Epoch 304/1000\n",
      "10/10 [==============================] - 0s 185us/sample - loss: 201.2414\n",
      "Epoch 305/1000\n",
      "10/10 [==============================] - 0s 227us/sample - loss: 200.8049\n",
      "Epoch 306/1000\n",
      "10/10 [==============================] - 0s 384us/sample - loss: 200.3976\n",
      "Epoch 307/1000\n",
      "10/10 [==============================] - 0s 282us/sample - loss: 200.0161\n",
      "Epoch 308/1000\n",
      "10/10 [==============================] - 0s 309us/sample - loss: 199.6573\n",
      "Epoch 309/1000\n",
      "10/10 [==============================] - 0s 289us/sample - loss: 199.3183\n",
      "Epoch 310/1000\n",
      "10/10 [==============================] - 0s 319us/sample - loss: 198.9961\n",
      "Epoch 311/1000\n",
      "10/10 [==============================] - 0s 195us/sample - loss: 198.6885\n",
      "Epoch 312/1000\n",
      "10/10 [==============================] - 0s 209us/sample - loss: 198.3931\n",
      "Epoch 313/1000\n",
      "10/10 [==============================] - 0s 302us/sample - loss: 198.1079\n",
      "Epoch 314/1000\n",
      "10/10 [==============================] - 0s 199us/sample - loss: 197.8312\n",
      "Epoch 315/1000\n",
      "10/10 [==============================] - 0s 187us/sample - loss: 197.5614\n",
      "Epoch 316/1000\n",
      "10/10 [==============================] - 0s 210us/sample - loss: 197.2975\n",
      "Epoch 317/1000\n",
      "10/10 [==============================] - 0s 213us/sample - loss: 197.0382\n",
      "Epoch 318/1000\n",
      "10/10 [==============================] - 0s 469us/sample - loss: 196.7828\n",
      "Epoch 319/1000\n",
      "10/10 [==============================] - 0s 197us/sample - loss: 196.5305\n",
      "Epoch 320/1000\n",
      "10/10 [==============================] - 0s 206us/sample - loss: 196.2808\n",
      "Epoch 321/1000\n",
      "10/10 [==============================] - 0s 204us/sample - loss: 196.0331\n",
      "Epoch 322/1000\n",
      "10/10 [==============================] - 0s 282us/sample - loss: 195.7871\n",
      "Epoch 323/1000\n",
      "10/10 [==============================] - 0s 220us/sample - loss: 195.5426\n",
      "Epoch 324/1000\n",
      "10/10 [==============================] - 0s 210us/sample - loss: 195.2991\n",
      "Epoch 325/1000\n",
      "10/10 [==============================] - 0s 286us/sample - loss: 195.0565\n",
      "Epoch 326/1000\n",
      "10/10 [==============================] - 0s 328us/sample - loss: 194.8148\n",
      "Epoch 327/1000\n",
      "10/10 [==============================] - 0s 225us/sample - loss: 194.5737\n",
      "Epoch 328/1000\n",
      "10/10 [==============================] - 0s 199us/sample - loss: 194.3330\n",
      "Epoch 329/1000\n",
      "10/10 [==============================] - 0s 230us/sample - loss: 194.0926\n",
      "Epoch 330/1000\n",
      "10/10 [==============================] - 0s 239us/sample - loss: 193.8524\n",
      "Epoch 331/1000\n",
      "10/10 [==============================] - 0s 407us/sample - loss: 193.6124\n",
      "Epoch 332/1000\n",
      "10/10 [==============================] - 0s 207us/sample - loss: 193.3723\n",
      "Epoch 333/1000\n",
      "10/10 [==============================] - 0s 183us/sample - loss: 193.1322\n",
      "Epoch 334/1000\n",
      "10/10 [==============================] - 0s 213us/sample - loss: 192.8921\n",
      "Epoch 335/1000\n",
      "10/10 [==============================] - 0s 411us/sample - loss: 192.6517\n",
      "Epoch 336/1000\n",
      "10/10 [==============================] - 0s 218us/sample - loss: 192.4112\n",
      "Epoch 337/1000\n",
      "10/10 [==============================] - 0s 177us/sample - loss: 192.1703\n",
      "Epoch 338/1000\n",
      "10/10 [==============================] - 0s 244us/sample - loss: 191.9292\n",
      "Epoch 339/1000\n",
      "10/10 [==============================] - 0s 275us/sample - loss: 191.6877\n",
      "Epoch 340/1000\n",
      "10/10 [==============================] - 0s 339us/sample - loss: 191.4458\n",
      "Epoch 341/1000\n",
      "10/10 [==============================] - 0s 181us/sample - loss: 191.2034\n",
      "Epoch 342/1000\n",
      "10/10 [==============================] - 0s 235us/sample - loss: 190.9607\n",
      "Epoch 343/1000\n",
      "10/10 [==============================] - 0s 213us/sample - loss: 190.7176\n",
      "Epoch 344/1000\n",
      "10/10 [==============================] - 0s 323us/sample - loss: 190.4739\n",
      "Epoch 345/1000\n",
      "10/10 [==============================] - 0s 218us/sample - loss: 190.2299\n",
      "Epoch 346/1000\n",
      "10/10 [==============================] - 0s 334us/sample - loss: 189.9853\n",
      "Epoch 347/1000\n",
      "10/10 [==============================] - 0s 460us/sample - loss: 189.7404\n",
      "Epoch 348/1000\n",
      "10/10 [==============================] - 0s 203us/sample - loss: 189.4949\n",
      "Epoch 349/1000\n",
      "10/10 [==============================] - 0s 258us/sample - loss: 189.2490\n",
      "Epoch 350/1000\n",
      "10/10 [==============================] - 0s 223us/sample - loss: 189.0027\n",
      "Epoch 351/1000\n",
      "10/10 [==============================] - 0s 245us/sample - loss: 188.7558\n",
      "Epoch 352/1000\n",
      "10/10 [==============================] - 0s 327us/sample - loss: 188.5088\n",
      "Epoch 353/1000\n",
      "10/10 [==============================] - 0s 353us/sample - loss: 188.2613\n",
      "Epoch 354/1000\n",
      "10/10 [==============================] - 0s 333us/sample - loss: 188.0133\n",
      "Epoch 355/1000\n",
      "10/10 [==============================] - 0s 426us/sample - loss: 187.7651\n",
      "Epoch 356/1000\n",
      "10/10 [==============================] - 0s 224us/sample - loss: 187.5166\n",
      "Epoch 357/1000\n",
      "10/10 [==============================] - 0s 327us/sample - loss: 187.2678\n",
      "Epoch 358/1000\n",
      "10/10 [==============================] - 0s 370us/sample - loss: 187.0189\n",
      "Epoch 359/1000\n",
      "10/10 [==============================] - 0s 235us/sample - loss: 186.7697\n",
      "Epoch 360/1000\n",
      "10/10 [==============================] - 0s 335us/sample - loss: 186.5203\n",
      "Epoch 361/1000\n",
      "10/10 [==============================] - 0s 446us/sample - loss: 186.2708\n",
      "Epoch 362/1000\n",
      "10/10 [==============================] - 0s 629us/sample - loss: 186.0212\n",
      "Epoch 363/1000\n",
      "10/10 [==============================] - 0s 312us/sample - loss: 185.7716\n",
      "Epoch 364/1000\n",
      "10/10 [==============================] - 0s 313us/sample - loss: 185.5219\n",
      "Epoch 365/1000\n",
      "10/10 [==============================] - 0s 388us/sample - loss: 185.2723\n",
      "Epoch 366/1000\n",
      "10/10 [==============================] - 0s 207us/sample - loss: 185.0227\n",
      "Epoch 367/1000\n",
      "10/10 [==============================] - 0s 214us/sample - loss: 184.7731\n",
      "Epoch 368/1000\n",
      "10/10 [==============================] - 0s 393us/sample - loss: 184.5237\n",
      "Epoch 369/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 252us/sample - loss: 184.2745\n",
      "Epoch 370/1000\n",
      "10/10 [==============================] - 0s 220us/sample - loss: 184.0255\n",
      "Epoch 371/1000\n",
      "10/10 [==============================] - 0s 271us/sample - loss: 183.7766\n",
      "Epoch 372/1000\n",
      "10/10 [==============================] - 0s 385us/sample - loss: 183.5281\n",
      "Epoch 373/1000\n",
      "10/10 [==============================] - 0s 299us/sample - loss: 183.2800\n",
      "Epoch 374/1000\n",
      "10/10 [==============================] - 0s 176us/sample - loss: 183.0319\n",
      "Epoch 375/1000\n",
      "10/10 [==============================] - 0s 214us/sample - loss: 182.7845\n",
      "Epoch 376/1000\n",
      "10/10 [==============================] - 0s 220us/sample - loss: 182.5373\n",
      "Epoch 377/1000\n",
      "10/10 [==============================] - 0s 216us/sample - loss: 182.2906\n",
      "Epoch 378/1000\n",
      "10/10 [==============================] - 0s 244us/sample - loss: 182.0443\n",
      "Epoch 379/1000\n",
      "10/10 [==============================] - 0s 223us/sample - loss: 181.7986\n",
      "Epoch 380/1000\n",
      "10/10 [==============================] - 0s 236us/sample - loss: 181.5540\n",
      "Epoch 381/1000\n",
      "10/10 [==============================] - 0s 190us/sample - loss: 181.3127\n",
      "Epoch 382/1000\n",
      "10/10 [==============================] - 0s 286us/sample - loss: 181.0929\n",
      "Epoch 383/1000\n",
      "10/10 [==============================] - 0s 204us/sample - loss: 180.9303\n",
      "Epoch 384/1000\n",
      "10/10 [==============================] - 0s 203us/sample - loss: 180.7543\n",
      "Epoch 385/1000\n",
      "10/10 [==============================] - 0s 366us/sample - loss: 180.4641\n",
      "Epoch 386/1000\n",
      "10/10 [==============================] - 0s 207us/sample - loss: 180.1974\n",
      "Epoch 387/1000\n",
      "10/10 [==============================] - 0s 190us/sample - loss: 179.9595\n",
      "Epoch 388/1000\n",
      "10/10 [==============================] - 0s 211us/sample - loss: 179.7322\n",
      "Epoch 389/1000\n",
      "10/10 [==============================] - 0s 357us/sample - loss: 179.5082\n",
      "Epoch 390/1000\n",
      "10/10 [==============================] - 0s 240us/sample - loss: 179.2855\n",
      "Epoch 391/1000\n",
      "10/10 [==============================] - 0s 230us/sample - loss: 179.0629\n",
      "Epoch 392/1000\n",
      "10/10 [==============================] - 0s 182us/sample - loss: 178.8405\n",
      "Epoch 393/1000\n",
      "10/10 [==============================] - 0s 225us/sample - loss: 178.6180\n",
      "Epoch 394/1000\n",
      "10/10 [==============================] - 0s 369us/sample - loss: 178.3957\n",
      "Epoch 395/1000\n",
      "10/10 [==============================] - 0s 249us/sample - loss: 178.1735\n",
      "Epoch 396/1000\n",
      "10/10 [==============================] - 0s 200us/sample - loss: 177.9521\n",
      "Epoch 397/1000\n",
      "10/10 [==============================] - 0s 250us/sample - loss: 177.7319\n",
      "Epoch 398/1000\n",
      "10/10 [==============================] - 0s 268us/sample - loss: 177.5150\n",
      "Epoch 399/1000\n",
      "10/10 [==============================] - 0s 184us/sample - loss: 177.3033\n",
      "Epoch 400/1000\n",
      "10/10 [==============================] - 0s 266us/sample - loss: 177.1009\n",
      "Epoch 401/1000\n",
      "10/10 [==============================] - 0s 179us/sample - loss: 176.9009\n",
      "Epoch 402/1000\n",
      "10/10 [==============================] - 0s 370us/sample - loss: 176.6931\n",
      "Epoch 403/1000\n",
      "10/10 [==============================] - 0s 250us/sample - loss: 176.4715\n",
      "Epoch 404/1000\n",
      "10/10 [==============================] - 0s 224us/sample - loss: 176.2460\n",
      "Epoch 405/1000\n",
      "10/10 [==============================] - 0s 257us/sample - loss: 176.0245\n",
      "Epoch 406/1000\n",
      "10/10 [==============================] - 0s 224us/sample - loss: 175.8069\n",
      "Epoch 407/1000\n",
      "10/10 [==============================] - 0s 342us/sample - loss: 175.5927\n",
      "Epoch 408/1000\n",
      "10/10 [==============================] - 0s 191us/sample - loss: 175.3803\n",
      "Epoch 409/1000\n",
      "10/10 [==============================] - 0s 193us/sample - loss: 175.1692\n",
      "Epoch 410/1000\n",
      "10/10 [==============================] - 0s 165us/sample - loss: 174.9592\n",
      "Epoch 411/1000\n",
      "10/10 [==============================] - 0s 300us/sample - loss: 174.7504\n",
      "Epoch 412/1000\n",
      "10/10 [==============================] - 0s 204us/sample - loss: 174.5427\n",
      "Epoch 413/1000\n",
      "10/10 [==============================] - 0s 237us/sample - loss: 174.3369\n",
      "Epoch 414/1000\n",
      "10/10 [==============================] - 0s 201us/sample - loss: 174.1334\n",
      "Epoch 415/1000\n",
      "10/10 [==============================] - 0s 299us/sample - loss: 173.9321\n",
      "Epoch 416/1000\n",
      "10/10 [==============================] - 0s 309us/sample - loss: 173.7318\n",
      "Epoch 417/1000\n",
      "10/10 [==============================] - 0s 207us/sample - loss: 173.5306\n",
      "Epoch 418/1000\n",
      "10/10 [==============================] - 0s 198us/sample - loss: 173.3263\n",
      "Epoch 419/1000\n",
      "10/10 [==============================] - 0s 198us/sample - loss: 173.1201\n",
      "Epoch 420/1000\n",
      "10/10 [==============================] - 0s 245us/sample - loss: 172.9124\n",
      "Epoch 421/1000\n",
      "10/10 [==============================] - 0s 228us/sample - loss: 172.7061\n",
      "Epoch 422/1000\n",
      "10/10 [==============================] - 0s 174us/sample - loss: 172.5006\n",
      "Epoch 423/1000\n",
      "10/10 [==============================] - 0s 178us/sample - loss: 172.2970\n",
      "Epoch 424/1000\n",
      "10/10 [==============================] - 0s 260us/sample - loss: 172.0945\n",
      "Epoch 425/1000\n",
      "10/10 [==============================] - 0s 245us/sample - loss: 171.8935\n",
      "Epoch 426/1000\n",
      "10/10 [==============================] - 0s 258us/sample - loss: 171.6934\n",
      "Epoch 427/1000\n",
      "10/10 [==============================] - 0s 231us/sample - loss: 171.4949\n",
      "Epoch 428/1000\n",
      "10/10 [==============================] - 0s 195us/sample - loss: 171.2973\n",
      "Epoch 429/1000\n",
      "10/10 [==============================] - 0s 230us/sample - loss: 171.1011\n",
      "Epoch 430/1000\n",
      "10/10 [==============================] - 0s 378us/sample - loss: 170.9054\n",
      "Epoch 431/1000\n",
      "10/10 [==============================] - 0s 191us/sample - loss: 170.7105\n",
      "Epoch 432/1000\n",
      "10/10 [==============================] - 0s 206us/sample - loss: 170.5146\n",
      "Epoch 433/1000\n",
      "10/10 [==============================] - 0s 192us/sample - loss: 170.3188\n",
      "Epoch 434/1000\n",
      "10/10 [==============================] - 0s 373us/sample - loss: 170.1215\n",
      "Epoch 435/1000\n",
      "10/10 [==============================] - 0s 222us/sample - loss: 169.9247\n",
      "Epoch 436/1000\n",
      "10/10 [==============================] - 0s 174us/sample - loss: 169.7274\n",
      "Epoch 437/1000\n",
      "10/10 [==============================] - 0s 188us/sample - loss: 169.5316\n",
      "Epoch 438/1000\n",
      "10/10 [==============================] - 0s 202us/sample - loss: 169.3358\n",
      "Epoch 439/1000\n",
      "10/10 [==============================] - 0s 252us/sample - loss: 169.1417\n",
      "Epoch 440/1000\n",
      "10/10 [==============================] - 0s 242us/sample - loss: 168.9480\n",
      "Epoch 441/1000\n",
      "10/10 [==============================] - 0s 201us/sample - loss: 168.7556\n",
      "Epoch 442/1000\n",
      "10/10 [==============================] - 0s 210us/sample - loss: 168.5636\n",
      "Epoch 443/1000\n",
      "10/10 [==============================] - 0s 257us/sample - loss: 168.3731\n",
      "Epoch 444/1000\n",
      "10/10 [==============================] - 0s 271us/sample - loss: 168.1824\n",
      "Epoch 445/1000\n",
      "10/10 [==============================] - 0s 209us/sample - loss: 167.9930\n",
      "Epoch 446/1000\n",
      "10/10 [==============================] - 0s 264us/sample - loss: 167.8027\n",
      "Epoch 447/1000\n",
      "10/10 [==============================] - 0s 231us/sample - loss: 167.6134\n",
      "Epoch 448/1000\n",
      "10/10 [==============================] - 0s 202us/sample - loss: 167.4229\n",
      "Epoch 449/1000\n",
      "10/10 [==============================] - 0s 200us/sample - loss: 167.2333\n",
      "Epoch 450/1000\n",
      "10/10 [==============================] - 0s 215us/sample - loss: 167.0429\n",
      "Epoch 451/1000\n",
      "10/10 [==============================] - 0s 173us/sample - loss: 166.8537\n",
      "Epoch 452/1000\n",
      "10/10 [==============================] - 0s 175us/sample - loss: 166.6640\n",
      "Epoch 453/1000\n",
      "10/10 [==============================] - 0s 212us/sample - loss: 166.4758\n",
      "Epoch 454/1000\n",
      "10/10 [==============================] - 0s 205us/sample - loss: 166.2874\n",
      "Epoch 455/1000\n",
      "10/10 [==============================] - 0s 182us/sample - loss: 166.1005\n",
      "Epoch 456/1000\n",
      "10/10 [==============================] - 0s 198us/sample - loss: 165.9133\n",
      "Epoch 457/1000\n",
      "10/10 [==============================] - 0s 183us/sample - loss: 165.7276\n",
      "Epoch 458/1000\n",
      "10/10 [==============================] - 0s 188us/sample - loss: 165.5414\n",
      "Epoch 459/1000\n",
      "10/10 [==============================] - 0s 300us/sample - loss: 165.3566\n",
      "Epoch 460/1000\n",
      "10/10 [==============================] - 0s 198us/sample - loss: 165.1708\n",
      "Epoch 461/1000\n",
      "10/10 [==============================] - 0s 198us/sample - loss: 164.9864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 462/1000\n",
      "10/10 [==============================] - 0s 182us/sample - loss: 164.8008\n",
      "Epoch 463/1000\n",
      "10/10 [==============================] - 0s 234us/sample - loss: 164.6164\n",
      "Epoch 464/1000\n",
      "10/10 [==============================] - 0s 211us/sample - loss: 164.4310\n",
      "Epoch 465/1000\n",
      "10/10 [==============================] - 0s 176us/sample - loss: 164.2469\n",
      "Epoch 466/1000\n",
      "10/10 [==============================] - 0s 213us/sample - loss: 164.0620\n",
      "Epoch 467/1000\n",
      "10/10 [==============================] - 0s 192us/sample - loss: 163.8786\n",
      "Epoch 468/1000\n",
      "10/10 [==============================] - 0s 246us/sample - loss: 163.6945\n",
      "Epoch 469/1000\n",
      "10/10 [==============================] - 0s 256us/sample - loss: 163.5119\n",
      "Epoch 470/1000\n",
      "10/10 [==============================] - 0s 185us/sample - loss: 163.3285\n",
      "Epoch 471/1000\n",
      "10/10 [==============================] - 0s 198us/sample - loss: 163.1467\n",
      "Epoch 472/1000\n",
      "10/10 [==============================] - 0s 247us/sample - loss: 162.9641\n",
      "Epoch 473/1000\n",
      "10/10 [==============================] - 0s 223us/sample - loss: 162.7829\n",
      "Epoch 474/1000\n",
      "10/10 [==============================] - 0s 203us/sample - loss: 162.6006\n",
      "Epoch 475/1000\n",
      "10/10 [==============================] - 0s 205us/sample - loss: 162.4198\n",
      "Epoch 476/1000\n",
      "10/10 [==============================] - 0s 164us/sample - loss: 162.2378\n",
      "Epoch 477/1000\n",
      "10/10 [==============================] - 0s 193us/sample - loss: 162.0572\n",
      "Epoch 478/1000\n",
      "10/10 [==============================] - 0s 265us/sample - loss: 161.8754\n",
      "Epoch 479/1000\n",
      "10/10 [==============================] - 0s 379us/sample - loss: 161.6950\n",
      "Epoch 480/1000\n",
      "10/10 [==============================] - 0s 224us/sample - loss: 161.5136\n",
      "Epoch 481/1000\n",
      "10/10 [==============================] - 0s 204us/sample - loss: 161.3336\n",
      "Epoch 482/1000\n",
      "10/10 [==============================] - 0s 200us/sample - loss: 161.1526\n",
      "Epoch 483/1000\n",
      "10/10 [==============================] - 0s 152us/sample - loss: 160.9732\n",
      "Epoch 484/1000\n",
      "10/10 [==============================] - 0s 171us/sample - loss: 160.7927\n",
      "Epoch 485/1000\n",
      "10/10 [==============================] - 0s 249us/sample - loss: 160.6137\n",
      "Epoch 486/1000\n",
      "10/10 [==============================] - 0s 219us/sample - loss: 160.4337\n",
      "Epoch 487/1000\n",
      "10/10 [==============================] - 0s 247us/sample - loss: 160.2551\n",
      "Epoch 488/1000\n",
      "10/10 [==============================] - 0s 205us/sample - loss: 160.0754\n",
      "Epoch 489/1000\n",
      "10/10 [==============================] - 0s 171us/sample - loss: 159.8971\n",
      "Epoch 490/1000\n",
      "10/10 [==============================] - 0s 200us/sample - loss: 159.7175\n",
      "Epoch 491/1000\n",
      "10/10 [==============================] - 0s 186us/sample - loss: 159.5394\n",
      "Epoch 492/1000\n",
      "10/10 [==============================] - 0s 413us/sample - loss: 159.3600\n",
      "Epoch 493/1000\n",
      "10/10 [==============================] - 0s 189us/sample - loss: 159.1820\n",
      "Epoch 494/1000\n",
      "10/10 [==============================] - 0s 231us/sample - loss: 159.0028\n",
      "Epoch 495/1000\n",
      "10/10 [==============================] - 0s 204us/sample - loss: 158.8251\n",
      "Epoch 496/1000\n",
      "10/10 [==============================] - 0s 220us/sample - loss: 158.6462\n",
      "Epoch 497/1000\n",
      "10/10 [==============================] - 0s 318us/sample - loss: 158.4687\n",
      "Epoch 498/1000\n",
      "10/10 [==============================] - 0s 246us/sample - loss: 158.2902\n",
      "Epoch 499/1000\n",
      "10/10 [==============================] - 0s 160us/sample - loss: 158.1130\n",
      "Epoch 500/1000\n",
      "10/10 [==============================] - 0s 319us/sample - loss: 157.9345\n",
      "Epoch 501/1000\n",
      "10/10 [==============================] - 0s 260us/sample - loss: 157.7575\n",
      "Epoch 502/1000\n",
      "10/10 [==============================] - 0s 216us/sample - loss: 157.5794\n",
      "Epoch 503/1000\n",
      "10/10 [==============================] - 0s 171us/sample - loss: 157.4025\n",
      "Epoch 504/1000\n",
      "10/10 [==============================] - 0s 211us/sample - loss: 157.2243\n",
      "Epoch 505/1000\n",
      "10/10 [==============================] - 0s 222us/sample - loss: 157.0475\n",
      "Epoch 506/1000\n",
      "10/10 [==============================] - 0s 255us/sample - loss: 156.8695\n",
      "Epoch 507/1000\n",
      "10/10 [==============================] - 0s 215us/sample - loss: 156.6927\n",
      "Epoch 508/1000\n",
      "10/10 [==============================] - 0s 203us/sample - loss: 156.5147\n",
      "Epoch 509/1000\n",
      "10/10 [==============================] - 0s 215us/sample - loss: 156.3381\n",
      "Epoch 510/1000\n",
      "10/10 [==============================] - 0s 178us/sample - loss: 156.1603\n",
      "Epoch 511/1000\n",
      "10/10 [==============================] - 0s 252us/sample - loss: 155.9837\n",
      "Epoch 512/1000\n",
      "10/10 [==============================] - 0s 237us/sample - loss: 155.8059\n",
      "Epoch 513/1000\n",
      "10/10 [==============================] - 0s 229us/sample - loss: 155.6294\n",
      "Epoch 514/1000\n",
      "10/10 [==============================] - 0s 278us/sample - loss: 155.4518\n",
      "Epoch 515/1000\n",
      "10/10 [==============================] - 0s 233us/sample - loss: 155.2753\n",
      "Epoch 516/1000\n",
      "10/10 [==============================] - 0s 176us/sample - loss: 155.0976\n",
      "Epoch 517/1000\n",
      "10/10 [==============================] - 0s 188us/sample - loss: 154.9211\n",
      "Epoch 518/1000\n",
      "10/10 [==============================] - 0s 181us/sample - loss: 154.7434\n",
      "Epoch 519/1000\n",
      "10/10 [==============================] - 0s 215us/sample - loss: 154.5669\n",
      "Epoch 520/1000\n",
      "10/10 [==============================] - 0s 223us/sample - loss: 154.3891\n",
      "Epoch 521/1000\n",
      "10/10 [==============================] - 0s 206us/sample - loss: 154.2126\n",
      "Epoch 522/1000\n",
      "10/10 [==============================] - 0s 184us/sample - loss: 154.0347\n",
      "Epoch 523/1000\n",
      "10/10 [==============================] - 0s 212us/sample - loss: 153.8581\n",
      "Epoch 524/1000\n",
      "10/10 [==============================] - 0s 236us/sample - loss: 153.6803\n",
      "Epoch 525/1000\n",
      "10/10 [==============================] - 0s 208us/sample - loss: 153.5036\n",
      "Epoch 526/1000\n",
      "10/10 [==============================] - 0s 210us/sample - loss: 153.3256\n",
      "Epoch 527/1000\n",
      "10/10 [==============================] - 0s 188us/sample - loss: 153.1488\n",
      "Epoch 528/1000\n",
      "10/10 [==============================] - 0s 232us/sample - loss: 152.9709\n",
      "Epoch 529/1000\n",
      "10/10 [==============================] - 0s 192us/sample - loss: 152.7939\n",
      "Epoch 530/1000\n",
      "10/10 [==============================] - 0s 281us/sample - loss: 152.6158\n",
      "Epoch 531/1000\n",
      "10/10 [==============================] - 0s 179us/sample - loss: 152.4387\n",
      "Epoch 532/1000\n",
      "10/10 [==============================] - 0s 179us/sample - loss: 152.2605\n",
      "Epoch 533/1000\n",
      "10/10 [==============================] - 0s 216us/sample - loss: 152.0833\n",
      "Epoch 534/1000\n",
      "10/10 [==============================] - 0s 239us/sample - loss: 151.9048\n",
      "Epoch 535/1000\n",
      "10/10 [==============================] - 0s 206us/sample - loss: 151.7274\n",
      "Epoch 536/1000\n",
      "10/10 [==============================] - 0s 293us/sample - loss: 151.5488\n",
      "Epoch 537/1000\n",
      "10/10 [==============================] - 0s 216us/sample - loss: 151.3712\n",
      "Epoch 538/1000\n",
      "10/10 [==============================] - 0s 314us/sample - loss: 151.1923\n",
      "Epoch 539/1000\n",
      "10/10 [==============================] - 0s 225us/sample - loss: 151.0144\n",
      "Epoch 540/1000\n",
      "10/10 [==============================] - 0s 273us/sample - loss: 150.8354\n",
      "Epoch 541/1000\n",
      "10/10 [==============================] - 0s 291us/sample - loss: 150.6574\n",
      "Epoch 542/1000\n",
      "10/10 [==============================] - 0s 247us/sample - loss: 150.4781\n",
      "Epoch 543/1000\n",
      "10/10 [==============================] - 0s 181us/sample - loss: 150.2998\n",
      "Epoch 544/1000\n",
      "10/10 [==============================] - 0s 177us/sample - loss: 150.1203\n",
      "Epoch 545/1000\n",
      "10/10 [==============================] - 0s 249us/sample - loss: 149.9416\n",
      "Epoch 546/1000\n",
      "10/10 [==============================] - 0s 272us/sample - loss: 149.7617\n",
      "Epoch 547/1000\n",
      "10/10 [==============================] - 0s 294us/sample - loss: 149.5829\n",
      "Epoch 548/1000\n",
      "10/10 [==============================] - 0s 210us/sample - loss: 149.4028\n",
      "Epoch 549/1000\n",
      "10/10 [==============================] - 0s 204us/sample - loss: 149.2235\n",
      "Epoch 550/1000\n",
      "10/10 [==============================] - 0s 301us/sample - loss: 149.0431\n",
      "Epoch 551/1000\n",
      "10/10 [==============================] - 0s 203us/sample - loss: 148.8635\n",
      "Epoch 552/1000\n",
      "10/10 [==============================] - 0s 335us/sample - loss: 148.6827\n",
      "Epoch 553/1000\n",
      "10/10 [==============================] - 0s 276us/sample - loss: 148.5028\n",
      "Epoch 554/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 300us/sample - loss: 148.3217\n",
      "Epoch 555/1000\n",
      "10/10 [==============================] - 0s 216us/sample - loss: 148.1414\n",
      "Epoch 556/1000\n",
      "10/10 [==============================] - 0s 299us/sample - loss: 147.9598\n",
      "Epoch 557/1000\n",
      "10/10 [==============================] - 0s 216us/sample - loss: 147.7791\n",
      "Epoch 558/1000\n",
      "10/10 [==============================] - 0s 310us/sample - loss: 147.5972\n",
      "Epoch 559/1000\n",
      "10/10 [==============================] - 0s 243us/sample - loss: 147.4161\n",
      "Epoch 560/1000\n",
      "10/10 [==============================] - 0s 251us/sample - loss: 147.2338\n",
      "Epoch 561/1000\n",
      "10/10 [==============================] - 0s 349us/sample - loss: 147.0522\n",
      "Epoch 562/1000\n",
      "10/10 [==============================] - 0s 386us/sample - loss: 146.8694\n",
      "Epoch 563/1000\n",
      "10/10 [==============================] - 0s 350us/sample - loss: 146.6874\n",
      "Epoch 564/1000\n",
      "10/10 [==============================] - 0s 211us/sample - loss: 146.5042\n",
      "Epoch 565/1000\n",
      "10/10 [==============================] - 0s 418us/sample - loss: 146.3217\n",
      "Epoch 566/1000\n",
      "10/10 [==============================] - 0s 250us/sample - loss: 146.1379\n",
      "Epoch 567/1000\n",
      "10/10 [==============================] - 0s 179us/sample - loss: 145.9549\n",
      "Epoch 568/1000\n",
      "10/10 [==============================] - 0s 461us/sample - loss: 145.7708\n",
      "Epoch 569/1000\n",
      "10/10 [==============================] - 0s 299us/sample - loss: 145.5873\n",
      "Epoch 570/1000\n",
      "10/10 [==============================] - 0s 306us/sample - loss: 145.4025\n",
      "Epoch 571/1000\n",
      "10/10 [==============================] - 0s 458us/sample - loss: 145.2185\n",
      "Epoch 572/1000\n",
      "10/10 [==============================] - 0s 315us/sample - loss: 145.0332\n",
      "Epoch 573/1000\n",
      "10/10 [==============================] - 0s 282us/sample - loss: 144.8486\n",
      "Epoch 574/1000\n",
      "10/10 [==============================] - 0s 230us/sample - loss: 144.6628\n",
      "Epoch 575/1000\n",
      "10/10 [==============================] - 0s 178us/sample - loss: 144.4776\n",
      "Epoch 576/1000\n",
      "10/10 [==============================] - 0s 242us/sample - loss: 144.2912\n",
      "Epoch 577/1000\n",
      "10/10 [==============================] - 0s 296us/sample - loss: 144.1054\n",
      "Epoch 578/1000\n",
      "10/10 [==============================] - 0s 233us/sample - loss: 143.9185\n",
      "Epoch 579/1000\n",
      "10/10 [==============================] - 0s 309us/sample - loss: 143.7321\n",
      "Epoch 580/1000\n",
      "10/10 [==============================] - 0s 393us/sample - loss: 143.5444\n",
      "Epoch 581/1000\n",
      "10/10 [==============================] - 0s 275us/sample - loss: 143.3574\n",
      "Epoch 582/1000\n",
      "10/10 [==============================] - 0s 259us/sample - loss: 143.1691\n",
      "Epoch 583/1000\n",
      "10/10 [==============================] - 0s 287us/sample - loss: 142.9815\n",
      "Epoch 584/1000\n",
      "10/10 [==============================] - 0s 295us/sample - loss: 142.7925\n",
      "Epoch 585/1000\n",
      "10/10 [==============================] - 0s 296us/sample - loss: 142.6042\n",
      "Epoch 586/1000\n",
      "10/10 [==============================] - 0s 385us/sample - loss: 142.4146\n",
      "Epoch 587/1000\n",
      "10/10 [==============================] - 0s 313us/sample - loss: 142.2256\n",
      "Epoch 588/1000\n",
      "10/10 [==============================] - 0s 274us/sample - loss: 142.0352\n",
      "Epoch 589/1000\n",
      "10/10 [==============================] - 0s 320us/sample - loss: 141.8455\n",
      "Epoch 590/1000\n",
      "10/10 [==============================] - 0s 330us/sample - loss: 141.6544\n",
      "Epoch 591/1000\n",
      "10/10 [==============================] - 0s 252us/sample - loss: 141.4640\n",
      "Epoch 592/1000\n",
      "10/10 [==============================] - 0s 274us/sample - loss: 141.2722\n",
      "Epoch 593/1000\n",
      "10/10 [==============================] - 0s 372us/sample - loss: 141.0810\n",
      "Epoch 594/1000\n",
      "10/10 [==============================] - 0s 475us/sample - loss: 140.8884\n",
      "Epoch 595/1000\n",
      "10/10 [==============================] - 0s 216us/sample - loss: 140.6964\n",
      "Epoch 596/1000\n",
      "10/10 [==============================] - 0s 188us/sample - loss: 140.5031\n",
      "Epoch 597/1000\n",
      "10/10 [==============================] - 0s 254us/sample - loss: 140.3103\n",
      "Epoch 598/1000\n",
      "10/10 [==============================] - 0s 209us/sample - loss: 140.1162\n",
      "Epoch 599/1000\n",
      "10/10 [==============================] - 0s 221us/sample - loss: 139.9225\n",
      "Epoch 600/1000\n",
      "10/10 [==============================] - 0s 238us/sample - loss: 139.7276\n",
      "Epoch 601/1000\n",
      "10/10 [==============================] - 0s 208us/sample - loss: 139.5332\n",
      "Epoch 602/1000\n",
      "10/10 [==============================] - 0s 292us/sample - loss: 139.3374\n",
      "Epoch 603/1000\n",
      "10/10 [==============================] - 0s 217us/sample - loss: 139.1421\n",
      "Epoch 604/1000\n",
      "10/10 [==============================] - 0s 228us/sample - loss: 138.9454\n",
      "Epoch 605/1000\n",
      "10/10 [==============================] - 0s 222us/sample - loss: 138.7492\n",
      "Epoch 606/1000\n",
      "10/10 [==============================] - 0s 239us/sample - loss: 138.5516\n",
      "Epoch 607/1000\n",
      "10/10 [==============================] - 0s 203us/sample - loss: 138.3546\n",
      "Epoch 608/1000\n",
      "10/10 [==============================] - 0s 199us/sample - loss: 138.1561\n",
      "Epoch 609/1000\n",
      "10/10 [==============================] - 0s 505us/sample - loss: 137.9582\n",
      "Epoch 610/1000\n",
      "10/10 [==============================] - 0s 228us/sample - loss: 137.7587\n",
      "Epoch 611/1000\n",
      "10/10 [==============================] - 0s 237us/sample - loss: 137.5599\n",
      "Epoch 612/1000\n",
      "10/10 [==============================] - 0s 243us/sample - loss: 137.3595\n",
      "Epoch 613/1000\n",
      "10/10 [==============================] - 0s 223us/sample - loss: 137.1597\n",
      "Epoch 614/1000\n",
      "10/10 [==============================] - 0s 238us/sample - loss: 136.9583\n",
      "Epoch 615/1000\n",
      "10/10 [==============================] - 0s 259us/sample - loss: 136.7576\n",
      "Epoch 616/1000\n",
      "10/10 [==============================] - 0s 229us/sample - loss: 136.5552\n",
      "Epoch 617/1000\n",
      "10/10 [==============================] - 0s 335us/sample - loss: 136.3534\n",
      "Epoch 618/1000\n",
      "10/10 [==============================] - 0s 289us/sample - loss: 136.1501\n",
      "Epoch 619/1000\n",
      "10/10 [==============================] - 0s 260us/sample - loss: 135.9472\n",
      "Epoch 620/1000\n",
      "10/10 [==============================] - 0s 233us/sample - loss: 135.7429\n",
      "Epoch 621/1000\n",
      "10/10 [==============================] - 0s 274us/sample - loss: 135.5390\n",
      "Epoch 622/1000\n",
      "10/10 [==============================] - 0s 260us/sample - loss: 135.3335\n",
      "Epoch 623/1000\n",
      "10/10 [==============================] - 0s 339us/sample - loss: 135.1287\n",
      "Epoch 624/1000\n",
      "10/10 [==============================] - 0s 249us/sample - loss: 134.9221\n",
      "Epoch 625/1000\n",
      "10/10 [==============================] - 0s 266us/sample - loss: 134.7162\n",
      "Epoch 626/1000\n",
      "10/10 [==============================] - 0s 265us/sample - loss: 134.5086\n",
      "Epoch 627/1000\n",
      "10/10 [==============================] - 0s 593us/sample - loss: 134.3015\n",
      "Epoch 628/1000\n",
      "10/10 [==============================] - 0s 406us/sample - loss: 134.0927\n",
      "Epoch 629/1000\n",
      "10/10 [==============================] - 0s 238us/sample - loss: 133.8846\n",
      "Epoch 630/1000\n",
      "10/10 [==============================] - 0s 234us/sample - loss: 133.6748\n",
      "Epoch 631/1000\n",
      "10/10 [==============================] - 0s 309us/sample - loss: 133.4655\n",
      "Epoch 632/1000\n",
      "10/10 [==============================] - 0s 270us/sample - loss: 133.2544\n",
      "Epoch 633/1000\n",
      "10/10 [==============================] - 0s 187us/sample - loss: 133.0439\n",
      "Epoch 634/1000\n",
      "10/10 [==============================] - 0s 296us/sample - loss: 132.8317\n",
      "Epoch 635/1000\n",
      "10/10 [==============================] - 0s 254us/sample - loss: 132.6201\n",
      "Epoch 636/1000\n",
      "10/10 [==============================] - 0s 291us/sample - loss: 132.4067\n",
      "Epoch 637/1000\n",
      "10/10 [==============================] - 0s 248us/sample - loss: 132.1939\n",
      "Epoch 638/1000\n",
      "10/10 [==============================] - 0s 201us/sample - loss: 131.9793\n",
      "Epoch 639/1000\n",
      "10/10 [==============================] - 0s 280us/sample - loss: 131.7653\n",
      "Epoch 640/1000\n",
      "10/10 [==============================] - 0s 524us/sample - loss: 131.5494\n",
      "Epoch 641/1000\n",
      "10/10 [==============================] - 0s 246us/sample - loss: 131.3342\n",
      "Epoch 642/1000\n",
      "10/10 [==============================] - 0s 309us/sample - loss: 131.1170\n",
      "Epoch 643/1000\n",
      "10/10 [==============================] - 0s 372us/sample - loss: 130.9005\n",
      "Epoch 644/1000\n",
      "10/10 [==============================] - 0s 207us/sample - loss: 130.6821\n",
      "Epoch 645/1000\n",
      "10/10 [==============================] - 0s 199us/sample - loss: 130.4643\n",
      "Epoch 646/1000\n",
      "10/10 [==============================] - 0s 267us/sample - loss: 130.2447\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 647/1000\n",
      "10/10 [==============================] - 0s 318us/sample - loss: 130.0256\n",
      "Epoch 648/1000\n",
      "10/10 [==============================] - 0s 265us/sample - loss: 129.8046\n",
      "Epoch 649/1000\n",
      "10/10 [==============================] - 0s 202us/sample - loss: 129.5841\n",
      "Epoch 650/1000\n",
      "10/10 [==============================] - 0s 288us/sample - loss: 129.3618\n",
      "Epoch 651/1000\n",
      "10/10 [==============================] - 0s 211us/sample - loss: 129.1401\n",
      "Epoch 652/1000\n",
      "10/10 [==============================] - 0s 179us/sample - loss: 128.9164\n",
      "Epoch 653/1000\n",
      "10/10 [==============================] - 0s 266us/sample - loss: 128.6934\n",
      "Epoch 654/1000\n",
      "10/10 [==============================] - 0s 497us/sample - loss: 128.4683\n",
      "Epoch 655/1000\n",
      "10/10 [==============================] - 0s 243us/sample - loss: 128.2438\n",
      "Epoch 656/1000\n",
      "10/10 [==============================] - 0s 271us/sample - loss: 128.0175\n",
      "Epoch 657/1000\n",
      "10/10 [==============================] - 0s 280us/sample - loss: 127.7916\n",
      "Epoch 658/1000\n",
      "10/10 [==============================] - 0s 342us/sample - loss: 127.5638\n",
      "Epoch 659/1000\n",
      "10/10 [==============================] - 0s 276us/sample - loss: 127.3365\n",
      "Epoch 660/1000\n",
      "10/10 [==============================] - 0s 295us/sample - loss: 127.1073\n",
      "Epoch 661/1000\n",
      "10/10 [==============================] - 0s 384us/sample - loss: 126.8786\n",
      "Epoch 662/1000\n",
      "10/10 [==============================] - 0s 277us/sample - loss: 126.6479\n",
      "Epoch 663/1000\n",
      "10/10 [==============================] - 0s 235us/sample - loss: 126.4177\n",
      "Epoch 664/1000\n",
      "10/10 [==============================] - 0s 465us/sample - loss: 126.1856\n",
      "Epoch 665/1000\n",
      "10/10 [==============================] - 0s 188us/sample - loss: 125.9540\n",
      "Epoch 666/1000\n",
      "10/10 [==============================] - 0s 231us/sample - loss: 125.7204\n",
      "Epoch 667/1000\n",
      "10/10 [==============================] - 0s 219us/sample - loss: 125.4874\n",
      "Epoch 668/1000\n",
      "10/10 [==============================] - 0s 233us/sample - loss: 125.2522\n",
      "Epoch 669/1000\n",
      "10/10 [==============================] - 0s 248us/sample - loss: 125.0177\n",
      "Epoch 670/1000\n",
      "10/10 [==============================] - 0s 173us/sample - loss: 124.7811\n",
      "Epoch 671/1000\n",
      "10/10 [==============================] - 0s 404us/sample - loss: 124.5450\n",
      "Epoch 672/1000\n",
      "10/10 [==============================] - 0s 192us/sample - loss: 124.3069\n",
      "Epoch 673/1000\n",
      "10/10 [==============================] - 0s 187us/sample - loss: 124.0693\n",
      "Epoch 674/1000\n",
      "10/10 [==============================] - 0s 202us/sample - loss: 123.8296\n",
      "Epoch 675/1000\n",
      "10/10 [==============================] - 0s 448us/sample - loss: 123.5905\n",
      "Epoch 676/1000\n",
      "10/10 [==============================] - 0s 346us/sample - loss: 123.3492\n",
      "Epoch 677/1000\n",
      "10/10 [==============================] - 0s 175us/sample - loss: 123.1086\n",
      "Epoch 678/1000\n",
      "10/10 [==============================] - 0s 191us/sample - loss: 122.8657\n",
      "Epoch 679/1000\n",
      "10/10 [==============================] - 0s 342us/sample - loss: 122.6235\n",
      "Epoch 680/1000\n",
      "10/10 [==============================] - 0s 295us/sample - loss: 122.3791\n",
      "Epoch 681/1000\n",
      "10/10 [==============================] - 0s 272us/sample - loss: 122.1353\n",
      "Epoch 682/1000\n",
      "10/10 [==============================] - 0s 219us/sample - loss: 121.8893\n",
      "Epoch 683/1000\n",
      "10/10 [==============================] - 0s 388us/sample - loss: 121.6439\n",
      "Epoch 684/1000\n",
      "10/10 [==============================] - 0s 192us/sample - loss: 121.3963\n",
      "Epoch 685/1000\n",
      "10/10 [==============================] - 0s 234us/sample - loss: 121.1492\n",
      "Epoch 686/1000\n",
      "10/10 [==============================] - 0s 262us/sample - loss: 120.9000\n",
      "Epoch 687/1000\n",
      "10/10 [==============================] - 0s 282us/sample - loss: 120.6513\n",
      "Epoch 688/1000\n",
      "10/10 [==============================] - 0s 231us/sample - loss: 120.4004\n",
      "Epoch 689/1000\n",
      "10/10 [==============================] - 0s 231us/sample - loss: 120.1501\n",
      "Epoch 690/1000\n",
      "10/10 [==============================] - 0s 365us/sample - loss: 119.8976\n",
      "Epoch 691/1000\n",
      "10/10 [==============================] - 0s 261us/sample - loss: 119.6456\n",
      "Epoch 692/1000\n",
      "10/10 [==============================] - 0s 245us/sample - loss: 119.3915\n",
      "Epoch 693/1000\n",
      "10/10 [==============================] - 0s 339us/sample - loss: 119.1378\n",
      "Epoch 694/1000\n",
      "10/10 [==============================] - 0s 316us/sample - loss: 118.8821\n",
      "Epoch 695/1000\n",
      "10/10 [==============================] - 0s 185us/sample - loss: 118.6267\n",
      "Epoch 696/1000\n",
      "10/10 [==============================] - 0s 234us/sample - loss: 118.3692\n",
      "Epoch 697/1000\n",
      "10/10 [==============================] - 0s 213us/sample - loss: 118.1121\n",
      "Epoch 698/1000\n",
      "10/10 [==============================] - 0s 265us/sample - loss: 117.8530\n",
      "Epoch 699/1000\n",
      "10/10 [==============================] - 0s 211us/sample - loss: 117.5943\n",
      "Epoch 700/1000\n",
      "10/10 [==============================] - 0s 178us/sample - loss: 117.3335\n",
      "Epoch 701/1000\n",
      "10/10 [==============================] - 0s 168us/sample - loss: 117.0730\n",
      "Epoch 702/1000\n",
      "10/10 [==============================] - 0s 350us/sample - loss: 116.8105\n",
      "Epoch 703/1000\n",
      "10/10 [==============================] - 0s 210us/sample - loss: 116.5483\n",
      "Epoch 704/1000\n",
      "10/10 [==============================] - 0s 248us/sample - loss: 116.2841\n",
      "Epoch 705/1000\n",
      "10/10 [==============================] - 0s 284us/sample - loss: 116.0202\n",
      "Epoch 706/1000\n",
      "10/10 [==============================] - 0s 330us/sample - loss: 115.7542\n",
      "Epoch 707/1000\n",
      "10/10 [==============================] - 0s 255us/sample - loss: 115.4886\n",
      "Epoch 708/1000\n",
      "10/10 [==============================] - 0s 200us/sample - loss: 115.2210\n",
      "Epoch 709/1000\n",
      "10/10 [==============================] - 0s 172us/sample - loss: 114.9536\n",
      "Epoch 710/1000\n",
      "10/10 [==============================] - 0s 261us/sample - loss: 114.6842\n",
      "Epoch 711/1000\n",
      "10/10 [==============================] - 0s 441us/sample - loss: 114.4152\n",
      "Epoch 712/1000\n",
      "10/10 [==============================] - 0s 345us/sample - loss: 114.1441\n",
      "Epoch 713/1000\n",
      "10/10 [==============================] - 0s 199us/sample - loss: 113.8732\n",
      "Epoch 714/1000\n",
      "10/10 [==============================] - 0s 295us/sample - loss: 113.6004\n",
      "Epoch 715/1000\n",
      "10/10 [==============================] - 0s 322us/sample - loss: 113.3279\n",
      "Epoch 716/1000\n",
      "10/10 [==============================] - 0s 237us/sample - loss: 113.0533\n",
      "Epoch 717/1000\n",
      "10/10 [==============================] - 0s 204us/sample - loss: 112.7789\n",
      "Epoch 718/1000\n",
      "10/10 [==============================] - 0s 291us/sample - loss: 112.5027\n",
      "Epoch 719/1000\n",
      "10/10 [==============================] - 0s 455us/sample - loss: 112.2266\n",
      "Epoch 720/1000\n",
      "10/10 [==============================] - 0s 201us/sample - loss: 111.9486\n",
      "Epoch 721/1000\n",
      "10/10 [==============================] - 0s 214us/sample - loss: 111.6708\n",
      "Epoch 722/1000\n",
      "10/10 [==============================] - 0s 317us/sample - loss: 111.3887\n",
      "Epoch 723/1000\n",
      "10/10 [==============================] - 0s 209us/sample - loss: 111.1095\n",
      "Epoch 724/1000\n",
      "10/10 [==============================] - 0s 335us/sample - loss: 110.8303\n",
      "Epoch 725/1000\n",
      "10/10 [==============================] - 0s 286us/sample - loss: 110.5514\n",
      "Epoch 726/1000\n",
      "10/10 [==============================] - 0s 359us/sample - loss: 110.2688\n",
      "Epoch 727/1000\n",
      "10/10 [==============================] - 0s 216us/sample - loss: 109.9582\n",
      "Epoch 728/1000\n",
      "10/10 [==============================] - 0s 162us/sample - loss: 109.4977\n",
      "Epoch 729/1000\n",
      "10/10 [==============================] - 0s 233us/sample - loss: 109.2036\n",
      "Epoch 730/1000\n",
      "10/10 [==============================] - 0s 247us/sample - loss: 108.9422\n",
      "Epoch 731/1000\n",
      "10/10 [==============================] - 0s 189us/sample - loss: 108.6371\n",
      "Epoch 732/1000\n",
      "10/10 [==============================] - 0s 200us/sample - loss: 108.3476\n",
      "Epoch 733/1000\n",
      "10/10 [==============================] - 0s 297us/sample - loss: 108.0587\n",
      "Epoch 734/1000\n",
      "10/10 [==============================] - 0s 375us/sample - loss: 107.7674\n",
      "Epoch 735/1000\n",
      "10/10 [==============================] - 0s 200us/sample - loss: 107.4767\n",
      "Epoch 736/1000\n",
      "10/10 [==============================] - 0s 240us/sample - loss: 107.1841\n",
      "Epoch 737/1000\n",
      "10/10 [==============================] - 0s 363us/sample - loss: 106.8914\n",
      "Epoch 738/1000\n",
      "10/10 [==============================] - 0s 216us/sample - loss: 106.5970\n",
      "Epoch 739/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 215us/sample - loss: 106.3023\n",
      "Epoch 740/1000\n",
      "10/10 [==============================] - 0s 198us/sample - loss: 106.0056\n",
      "Epoch 741/1000\n",
      "10/10 [==============================] - 0s 194us/sample - loss: 105.7086\n",
      "Epoch 742/1000\n",
      "10/10 [==============================] - 0s 255us/sample - loss: 105.4097\n",
      "Epoch 743/1000\n",
      "10/10 [==============================] - 0s 238us/sample - loss: 105.1104\n",
      "Epoch 744/1000\n",
      "10/10 [==============================] - 0s 235us/sample - loss: 104.8094\n",
      "Epoch 745/1000\n",
      "10/10 [==============================] - 0s 218us/sample - loss: 104.5081\n",
      "Epoch 746/1000\n",
      "10/10 [==============================] - 0s 186us/sample - loss: 104.2052\n",
      "Epoch 747/1000\n",
      "10/10 [==============================] - 0s 322us/sample - loss: 103.9023\n",
      "Epoch 748/1000\n",
      "10/10 [==============================] - 0s 219us/sample - loss: 103.5976\n",
      "Epoch 749/1000\n",
      "10/10 [==============================] - 0s 204us/sample - loss: 103.2931\n",
      "Epoch 750/1000\n",
      "10/10 [==============================] - 0s 198us/sample - loss: 102.9868\n",
      "Epoch 751/1000\n",
      "10/10 [==============================] - 0s 181us/sample - loss: 102.6807\n",
      "Epoch 752/1000\n",
      "10/10 [==============================] - 0s 325us/sample - loss: 102.3728\n",
      "Epoch 753/1000\n",
      "10/10 [==============================] - 0s 187us/sample - loss: 102.0650\n",
      "Epoch 754/1000\n",
      "10/10 [==============================] - 0s 178us/sample - loss: 101.7553\n",
      "Epoch 755/1000\n",
      "10/10 [==============================] - 0s 198us/sample - loss: 101.4456\n",
      "Epoch 756/1000\n",
      "10/10 [==============================] - 0s 370us/sample - loss: 101.1470\n",
      "Epoch 757/1000\n",
      "10/10 [==============================] - 0s 269us/sample - loss: 100.8407\n",
      "Epoch 758/1000\n",
      "10/10 [==============================] - 0s 201us/sample - loss: 100.5285\n",
      "Epoch 759/1000\n",
      "10/10 [==============================] - 0s 223us/sample - loss: 100.2169\n",
      "Epoch 760/1000\n",
      "10/10 [==============================] - 0s 245us/sample - loss: 99.9036\n",
      "Epoch 761/1000\n",
      "10/10 [==============================] - 0s 277us/sample - loss: 99.5903\n",
      "Epoch 762/1000\n",
      "10/10 [==============================] - 0s 240us/sample - loss: 99.2755\n",
      "Epoch 763/1000\n",
      "10/10 [==============================] - 0s 227us/sample - loss: 98.9603\n",
      "Epoch 764/1000\n",
      "10/10 [==============================] - 0s 279us/sample - loss: 98.6433\n",
      "Epoch 765/1000\n",
      "10/10 [==============================] - 0s 273us/sample - loss: 98.3257\n",
      "Epoch 766/1000\n",
      "10/10 [==============================] - 0s 290us/sample - loss: 98.0064\n",
      "Epoch 767/1000\n",
      "10/10 [==============================] - 0s 264us/sample - loss: 97.6864\n",
      "Epoch 768/1000\n",
      "10/10 [==============================] - 0s 230us/sample - loss: 97.3648\n",
      "Epoch 769/1000\n",
      "10/10 [==============================] - 0s 192us/sample - loss: 97.0426\n",
      "Epoch 770/1000\n",
      "10/10 [==============================] - 0s 185us/sample - loss: 96.7189\n",
      "Epoch 771/1000\n",
      "10/10 [==============================] - 0s 160us/sample - loss: 96.3949\n",
      "Epoch 772/1000\n",
      "10/10 [==============================] - 0s 291us/sample - loss: 96.0694\n",
      "Epoch 773/1000\n",
      "10/10 [==============================] - 0s 311us/sample - loss: 95.7440\n",
      "Epoch 774/1000\n",
      "10/10 [==============================] - 0s 287us/sample - loss: 95.4170\n",
      "Epoch 775/1000\n",
      "10/10 [==============================] - 0s 179us/sample - loss: 95.0903\n",
      "Epoch 776/1000\n",
      "10/10 [==============================] - 0s 263us/sample - loss: 94.7971\n",
      "Epoch 777/1000\n",
      "10/10 [==============================] - 0s 288us/sample - loss: 94.4547\n",
      "Epoch 778/1000\n",
      "10/10 [==============================] - 0s 181us/sample - loss: 94.1278\n",
      "Epoch 779/1000\n",
      "10/10 [==============================] - 0s 301us/sample - loss: 93.8017\n",
      "Epoch 780/1000\n",
      "10/10 [==============================] - 0s 232us/sample - loss: 93.4738\n",
      "Epoch 781/1000\n",
      "10/10 [==============================] - 0s 303us/sample - loss: 93.1463\n",
      "Epoch 782/1000\n",
      "10/10 [==============================] - 0s 246us/sample - loss: 92.8176\n",
      "Epoch 783/1000\n",
      "10/10 [==============================] - 0s 295us/sample - loss: 92.4885\n",
      "Epoch 784/1000\n",
      "10/10 [==============================] - 0s 302us/sample - loss: 92.1579\n",
      "Epoch 785/1000\n",
      "10/10 [==============================] - 0s 252us/sample - loss: 91.8267\n",
      "Epoch 786/1000\n",
      "10/10 [==============================] - 0s 250us/sample - loss: 91.4939\n",
      "Epoch 787/1000\n",
      "10/10 [==============================] - 0s 456us/sample - loss: 91.1602\n",
      "Epoch 788/1000\n",
      "10/10 [==============================] - 0s 321us/sample - loss: 90.8251\n",
      "Epoch 789/1000\n",
      "10/10 [==============================] - 0s 280us/sample - loss: 90.4890\n",
      "Epoch 790/1000\n",
      "10/10 [==============================] - 0s 464us/sample - loss: 90.1516\n",
      "Epoch 791/1000\n",
      "10/10 [==============================] - 0s 346us/sample - loss: 89.8134\n",
      "Epoch 792/1000\n",
      "10/10 [==============================] - 0s 516us/sample - loss: 89.4738\n",
      "Epoch 793/1000\n",
      "10/10 [==============================] - 0s 411us/sample - loss: 89.1331\n",
      "Epoch 794/1000\n",
      "10/10 [==============================] - 0s 460us/sample - loss: 88.7910\n",
      "Epoch 795/1000\n",
      "10/10 [==============================] - 0s 268us/sample - loss: 88.4485\n",
      "Epoch 796/1000\n",
      "10/10 [==============================] - 0s 278us/sample - loss: 88.1099\n",
      "Epoch 797/1000\n",
      "10/10 [==============================] - 0s 392us/sample - loss: 87.7785\n",
      "Epoch 798/1000\n",
      "10/10 [==============================] - 0s 209us/sample - loss: 87.4355\n",
      "Epoch 799/1000\n",
      "10/10 [==============================] - 0s 244us/sample - loss: 87.0929\n",
      "Epoch 800/1000\n",
      "10/10 [==============================] - 0s 318us/sample - loss: 86.7485\n",
      "Epoch 801/1000\n",
      "10/10 [==============================] - 0s 258us/sample - loss: 86.4044\n",
      "Epoch 802/1000\n",
      "10/10 [==============================] - 0s 200us/sample - loss: 86.0582\n",
      "Epoch 803/1000\n",
      "10/10 [==============================] - 0s 290us/sample - loss: 85.7115\n",
      "Epoch 804/1000\n",
      "10/10 [==============================] - 0s 309us/sample - loss: 85.3637\n",
      "Epoch 805/1000\n",
      "10/10 [==============================] - 0s 306us/sample - loss: 85.0154\n",
      "Epoch 806/1000\n",
      "10/10 [==============================] - 0s 360us/sample - loss: 84.6657\n",
      "Epoch 807/1000\n",
      "10/10 [==============================] - 0s 291us/sample - loss: 84.3153\n",
      "Epoch 808/1000\n",
      "10/10 [==============================] - 0s 236us/sample - loss: 83.9410\n",
      "Epoch 809/1000\n",
      "10/10 [==============================] - 0s 185us/sample - loss: 83.5715\n",
      "Epoch 810/1000\n",
      "10/10 [==============================] - 0s 417us/sample - loss: 83.2530\n",
      "Epoch 811/1000\n",
      "10/10 [==============================] - 0s 318us/sample - loss: 82.8613\n",
      "Epoch 812/1000\n",
      "10/10 [==============================] - 0s 208us/sample - loss: 82.5196\n",
      "Epoch 813/1000\n",
      "10/10 [==============================] - 0s 231us/sample - loss: 82.1529\n",
      "Epoch 814/1000\n",
      "10/10 [==============================] - 0s 321us/sample - loss: 81.7938\n",
      "Epoch 815/1000\n",
      "10/10 [==============================] - 0s 277us/sample - loss: 81.4337\n",
      "Epoch 816/1000\n",
      "10/10 [==============================] - 0s 209us/sample - loss: 81.0714\n",
      "Epoch 817/1000\n",
      "10/10 [==============================] - 0s 231us/sample - loss: 80.7083\n",
      "Epoch 818/1000\n",
      "10/10 [==============================] - 0s 347us/sample - loss: 80.3430\n",
      "Epoch 819/1000\n",
      "10/10 [==============================] - 0s 310us/sample - loss: 79.9770\n",
      "Epoch 820/1000\n",
      "10/10 [==============================] - 0s 193us/sample - loss: 79.6102\n",
      "Epoch 821/1000\n",
      "10/10 [==============================] - 0s 212us/sample - loss: 79.2423\n",
      "Epoch 822/1000\n",
      "10/10 [==============================] - 0s 304us/sample - loss: 78.8753\n",
      "Epoch 823/1000\n",
      "10/10 [==============================] - 0s 261us/sample - loss: 78.5173\n",
      "Epoch 824/1000\n",
      "10/10 [==============================] - 0s 215us/sample - loss: 78.1572\n",
      "Epoch 825/1000\n",
      "10/10 [==============================] - 0s 192us/sample - loss: 77.7944\n",
      "Epoch 826/1000\n",
      "10/10 [==============================] - 0s 799us/sample - loss: 77.4455\n",
      "Epoch 827/1000\n",
      "10/10 [==============================] - 0s 208us/sample - loss: 77.0817\n",
      "Epoch 828/1000\n",
      "10/10 [==============================] - 0s 402us/sample - loss: 76.7293\n",
      "Epoch 829/1000\n",
      "10/10 [==============================] - 0s 365us/sample - loss: 76.3668\n",
      "Epoch 830/1000\n",
      "10/10 [==============================] - 0s 243us/sample - loss: 76.0023\n",
      "Epoch 831/1000\n",
      "10/10 [==============================] - 0s 261us/sample - loss: 75.6378\n",
      "Epoch 832/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 318us/sample - loss: 75.2713\n",
      "Epoch 833/1000\n",
      "10/10 [==============================] - 0s 298us/sample - loss: 74.9021\n",
      "Epoch 834/1000\n",
      "10/10 [==============================] - 0s 232us/sample - loss: 74.5284\n",
      "Epoch 835/1000\n",
      "10/10 [==============================] - 0s 269us/sample - loss: 74.1532\n",
      "Epoch 836/1000\n",
      "10/10 [==============================] - 0s 354us/sample - loss: 73.7773\n",
      "Epoch 837/1000\n",
      "10/10 [==============================] - 0s 299us/sample - loss: 73.4009\n",
      "Epoch 838/1000\n",
      "10/10 [==============================] - 0s 192us/sample - loss: 73.0357\n",
      "Epoch 839/1000\n",
      "10/10 [==============================] - 0s 371us/sample - loss: 72.6657\n",
      "Epoch 840/1000\n",
      "10/10 [==============================] - 0s 307us/sample - loss: 72.2893\n",
      "Epoch 841/1000\n",
      "10/10 [==============================] - 0s 272us/sample - loss: 71.9125\n",
      "Epoch 842/1000\n",
      "10/10 [==============================] - 0s 208us/sample - loss: 71.5381\n",
      "Epoch 843/1000\n",
      "10/10 [==============================] - 0s 320us/sample - loss: 71.1714\n",
      "Epoch 844/1000\n",
      "10/10 [==============================] - 0s 571us/sample - loss: 70.7943\n",
      "Epoch 845/1000\n",
      "10/10 [==============================] - 0s 218us/sample - loss: 70.4167\n",
      "Epoch 846/1000\n",
      "10/10 [==============================] - 0s 261us/sample - loss: 70.0393\n",
      "Epoch 847/1000\n",
      "10/10 [==============================] - 0s 590us/sample - loss: 69.6609\n",
      "Epoch 848/1000\n",
      "10/10 [==============================] - 0s 351us/sample - loss: 69.2820\n",
      "Epoch 849/1000\n",
      "10/10 [==============================] - 0s 300us/sample - loss: 68.9133\n",
      "Epoch 850/1000\n",
      "10/10 [==============================] - 0s 372us/sample - loss: 68.5407\n",
      "Epoch 851/1000\n",
      "10/10 [==============================] - 0s 360us/sample - loss: 68.1621\n",
      "Epoch 852/1000\n",
      "10/10 [==============================] - 0s 211us/sample - loss: 67.7835\n",
      "Epoch 853/1000\n",
      "10/10 [==============================] - 0s 298us/sample - loss: 67.4240\n",
      "Epoch 854/1000\n",
      "10/10 [==============================] - 0s 344us/sample - loss: 67.0412\n",
      "Epoch 855/1000\n",
      "10/10 [==============================] - 0s 263us/sample - loss: 66.6635\n",
      "Epoch 856/1000\n",
      "10/10 [==============================] - 0s 268us/sample - loss: 66.2837\n",
      "Epoch 857/1000\n",
      "10/10 [==============================] - 0s 350us/sample - loss: 65.8990\n",
      "Epoch 858/1000\n",
      "10/10 [==============================] - 0s 294us/sample - loss: 65.4926\n",
      "Epoch 859/1000\n",
      "10/10 [==============================] - 0s 183us/sample - loss: 65.0469\n",
      "Epoch 860/1000\n",
      "10/10 [==============================] - 0s 194us/sample - loss: 64.6694\n",
      "Epoch 861/1000\n",
      "10/10 [==============================] - 0s 294us/sample - loss: 64.3025\n",
      "Epoch 862/1000\n",
      "10/10 [==============================] - 0s 215us/sample - loss: 63.9187\n",
      "Epoch 863/1000\n",
      "10/10 [==============================] - 0s 386us/sample - loss: 63.5402\n",
      "Epoch 864/1000\n",
      "10/10 [==============================] - 0s 283us/sample - loss: 63.1650\n",
      "Epoch 865/1000\n",
      "10/10 [==============================] - 0s 365us/sample - loss: 62.7962\n",
      "Epoch 866/1000\n",
      "10/10 [==============================] - 0s 200us/sample - loss: 62.4193\n",
      "Epoch 867/1000\n",
      "10/10 [==============================] - 0s 195us/sample - loss: 62.0422\n",
      "Epoch 868/1000\n",
      "10/10 [==============================] - 0s 277us/sample - loss: 61.6760\n",
      "Epoch 869/1000\n",
      "10/10 [==============================] - 0s 356us/sample - loss: 61.3093\n",
      "Epoch 870/1000\n",
      "10/10 [==============================] - 0s 246us/sample - loss: 60.9346\n",
      "Epoch 871/1000\n",
      "10/10 [==============================] - 0s 308us/sample - loss: 60.5599\n",
      "Epoch 872/1000\n",
      "10/10 [==============================] - 0s 253us/sample - loss: 60.1969\n",
      "Epoch 873/1000\n",
      "10/10 [==============================] - 0s 192us/sample - loss: 59.8432\n",
      "Epoch 874/1000\n",
      "10/10 [==============================] - 0s 273us/sample - loss: 59.4752\n",
      "Epoch 875/1000\n",
      "10/10 [==============================] - 0s 289us/sample - loss: 59.1067\n",
      "Epoch 876/1000\n",
      "10/10 [==============================] - 0s 307us/sample - loss: 58.7438\n",
      "Epoch 877/1000\n",
      "10/10 [==============================] - 0s 184us/sample - loss: 58.4013\n",
      "Epoch 878/1000\n",
      "10/10 [==============================] - 0s 219us/sample - loss: 58.0393\n",
      "Epoch 879/1000\n",
      "10/10 [==============================] - 0s 300us/sample - loss: 57.6748\n",
      "Epoch 880/1000\n",
      "10/10 [==============================] - 0s 392us/sample - loss: 57.3424\n",
      "Epoch 881/1000\n",
      "10/10 [==============================] - 0s 268us/sample - loss: 56.9979\n",
      "Epoch 882/1000\n",
      "10/10 [==============================] - 0s 179us/sample - loss: 56.6616\n",
      "Epoch 883/1000\n",
      "10/10 [==============================] - 0s 302us/sample - loss: 56.3063\n",
      "Epoch 884/1000\n",
      "10/10 [==============================] - 0s 264us/sample - loss: 55.9802\n",
      "Epoch 885/1000\n",
      "10/10 [==============================] - 0s 273us/sample - loss: 55.6438\n",
      "Epoch 886/1000\n",
      "10/10 [==============================] - 0s 218us/sample - loss: 55.2994\n",
      "Epoch 887/1000\n",
      "10/10 [==============================] - 0s 267us/sample - loss: 54.9571\n",
      "Epoch 888/1000\n",
      "10/10 [==============================] - 0s 242us/sample - loss: 54.6487\n",
      "Epoch 889/1000\n",
      "10/10 [==============================] - 0s 221us/sample - loss: 54.3124\n",
      "Epoch 890/1000\n",
      "10/10 [==============================] - 0s 328us/sample - loss: 53.9686\n",
      "Epoch 891/1000\n",
      "10/10 [==============================] - 0s 233us/sample - loss: 53.6310\n",
      "Epoch 892/1000\n",
      "10/10 [==============================] - 0s 251us/sample - loss: 53.3056\n",
      "Epoch 893/1000\n",
      "10/10 [==============================] - 0s 212us/sample - loss: 52.9671\n",
      "Epoch 894/1000\n",
      "10/10 [==============================] - 0s 257us/sample - loss: 52.6407\n",
      "Epoch 895/1000\n",
      "10/10 [==============================] - 0s 272us/sample - loss: 52.3343\n",
      "Epoch 896/1000\n",
      "10/10 [==============================] - 0s 321us/sample - loss: 51.9983\n",
      "Epoch 897/1000\n",
      "10/10 [==============================] - 0s 244us/sample - loss: 51.6624\n",
      "Epoch 898/1000\n",
      "10/10 [==============================] - 0s 374us/sample - loss: 51.3503\n",
      "Epoch 899/1000\n",
      "10/10 [==============================] - 0s 272us/sample - loss: 51.0333\n",
      "Epoch 900/1000\n",
      "10/10 [==============================] - 0s 278us/sample - loss: 50.7046\n",
      "Epoch 901/1000\n",
      "10/10 [==============================] - 0s 287us/sample - loss: 50.3766\n",
      "Epoch 902/1000\n",
      "10/10 [==============================] - 0s 189us/sample - loss: 50.0745\n",
      "Epoch 903/1000\n",
      "10/10 [==============================] - 0s 239us/sample - loss: 49.7465\n",
      "Epoch 904/1000\n",
      "10/10 [==============================] - 0s 229us/sample - loss: 49.4299\n",
      "Epoch 905/1000\n",
      "10/10 [==============================] - 0s 269us/sample - loss: 49.1427\n",
      "Epoch 906/1000\n",
      "10/10 [==============================] - 0s 234us/sample - loss: 48.8242\n",
      "Epoch 907/1000\n",
      "10/10 [==============================] - 0s 253us/sample - loss: 48.5024\n",
      "Epoch 908/1000\n",
      "10/10 [==============================] - 0s 308us/sample - loss: 48.1991\n",
      "Epoch 909/1000\n",
      "10/10 [==============================] - 0s 324us/sample - loss: 47.9029\n",
      "Epoch 910/1000\n",
      "10/10 [==============================] - 0s 228us/sample - loss: 47.5895\n",
      "Epoch 911/1000\n",
      "10/10 [==============================] - 0s 199us/sample - loss: 47.2746\n",
      "Epoch 912/1000\n",
      "10/10 [==============================] - 0s 516us/sample - loss: 46.9923\n",
      "Epoch 913/1000\n",
      "10/10 [==============================] - 0s 346us/sample - loss: 46.6748\n",
      "Epoch 914/1000\n",
      "10/10 [==============================] - 0s 200us/sample - loss: 46.3698\n",
      "Epoch 915/1000\n",
      "10/10 [==============================] - 0s 220us/sample - loss: 46.0630\n",
      "Epoch 916/1000\n",
      "10/10 [==============================] - 0s 365us/sample - loss: 45.7706\n",
      "Epoch 917/1000\n",
      "10/10 [==============================] - 0s 178us/sample - loss: 45.4701\n",
      "Epoch 918/1000\n",
      "10/10 [==============================] - 0s 271us/sample - loss: 45.1844\n",
      "Epoch 919/1000\n",
      "10/10 [==============================] - 0s 229us/sample - loss: 44.8883\n",
      "Epoch 920/1000\n",
      "10/10 [==============================] - 0s 434us/sample - loss: 44.5881\n",
      "Epoch 921/1000\n",
      "10/10 [==============================] - 0s 257us/sample - loss: 44.2878\n",
      "Epoch 922/1000\n",
      "10/10 [==============================] - 0s 196us/sample - loss: 44.0001\n",
      "Epoch 923/1000\n",
      "10/10 [==============================] - 0s 221us/sample - loss: 43.7134\n",
      "Epoch 924/1000\n",
      "10/10 [==============================] - 0s 410us/sample - loss: 43.4528\n",
      "Epoch 925/1000\n",
      "10/10 [==============================] - 0s 212us/sample - loss: 43.1698\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 926/1000\n",
      "10/10 [==============================] - 0s 215us/sample - loss: 42.8778\n",
      "Epoch 927/1000\n",
      "10/10 [==============================] - 0s 179us/sample - loss: 42.5820\n",
      "Epoch 928/1000\n",
      "10/10 [==============================] - 0s 209us/sample - loss: 42.2918\n",
      "Epoch 929/1000\n",
      "10/10 [==============================] - 0s 288us/sample - loss: 41.9990\n",
      "Epoch 930/1000\n",
      "10/10 [==============================] - 0s 231us/sample - loss: 41.7226\n",
      "Epoch 931/1000\n",
      "10/10 [==============================] - 0s 207us/sample - loss: 41.4349\n",
      "Epoch 932/1000\n",
      "10/10 [==============================] - 0s 247us/sample - loss: 41.1561\n",
      "Epoch 933/1000\n",
      "10/10 [==============================] - 0s 379us/sample - loss: 40.8639\n",
      "Epoch 934/1000\n",
      "10/10 [==============================] - 0s 258us/sample - loss: 40.5716\n",
      "Epoch 935/1000\n",
      "10/10 [==============================] - 0s 180us/sample - loss: 40.2804\n",
      "Epoch 936/1000\n",
      "10/10 [==============================] - 0s 409us/sample - loss: 39.9980\n",
      "Epoch 937/1000\n",
      "10/10 [==============================] - 0s 265us/sample - loss: 39.7178\n",
      "Epoch 938/1000\n",
      "10/10 [==============================] - 0s 218us/sample - loss: 39.4601\n",
      "Epoch 939/1000\n",
      "10/10 [==============================] - 0s 276us/sample - loss: 39.2127\n",
      "Epoch 940/1000\n",
      "10/10 [==============================] - 0s 428us/sample - loss: 38.9441\n",
      "Epoch 941/1000\n",
      "10/10 [==============================] - 0s 265us/sample - loss: 38.6538\n",
      "Epoch 942/1000\n",
      "10/10 [==============================] - 0s 204us/sample - loss: 38.3771\n",
      "Epoch 943/1000\n",
      "10/10 [==============================] - 0s 247us/sample - loss: 38.0920\n",
      "Epoch 944/1000\n",
      "10/10 [==============================] - 0s 355us/sample - loss: 37.8317\n",
      "Epoch 945/1000\n",
      "10/10 [==============================] - 0s 235us/sample - loss: 37.5526\n",
      "Epoch 946/1000\n",
      "10/10 [==============================] - 0s 270us/sample - loss: 37.2905\n",
      "Epoch 947/1000\n",
      "10/10 [==============================] - 0s 212us/sample - loss: 37.0287\n",
      "Epoch 948/1000\n",
      "10/10 [==============================] - 0s 338us/sample - loss: 36.7602\n",
      "Epoch 949/1000\n",
      "10/10 [==============================] - 0s 214us/sample - loss: 36.4913\n",
      "Epoch 950/1000\n",
      "10/10 [==============================] - 0s 217us/sample - loss: 36.2256\n",
      "Epoch 951/1000\n",
      "10/10 [==============================] - 0s 348us/sample - loss: 35.9689\n",
      "Epoch 952/1000\n",
      "10/10 [==============================] - 0s 261us/sample - loss: 35.7157\n",
      "Epoch 953/1000\n",
      "10/10 [==============================] - 0s 232us/sample - loss: 35.4688\n",
      "Epoch 954/1000\n",
      "10/10 [==============================] - 0s 215us/sample - loss: 35.2208\n",
      "Epoch 955/1000\n",
      "10/10 [==============================] - 0s 321us/sample - loss: 34.9610\n",
      "Epoch 956/1000\n",
      "10/10 [==============================] - 0s 238us/sample - loss: 34.6967\n",
      "Epoch 957/1000\n",
      "10/10 [==============================] - 0s 224us/sample - loss: 34.4293\n",
      "Epoch 958/1000\n",
      "10/10 [==============================] - 0s 211us/sample - loss: 34.1740\n",
      "Epoch 959/1000\n",
      "10/10 [==============================] - 0s 440us/sample - loss: 33.9210\n",
      "Epoch 960/1000\n",
      "10/10 [==============================] - 0s 207us/sample - loss: 33.6862\n",
      "Epoch 961/1000\n",
      "10/10 [==============================] - 0s 240us/sample - loss: 33.4305\n",
      "Epoch 962/1000\n",
      "10/10 [==============================] - 0s 280us/sample - loss: 33.1945\n",
      "Epoch 963/1000\n",
      "10/10 [==============================] - 0s 241us/sample - loss: 32.9469\n",
      "Epoch 964/1000\n",
      "10/10 [==============================] - 0s 295us/sample - loss: 32.6974\n",
      "Epoch 965/1000\n",
      "10/10 [==============================] - 0s 308us/sample - loss: 32.4465\n",
      "Epoch 966/1000\n",
      "10/10 [==============================] - 0s 247us/sample - loss: 32.2035\n",
      "Epoch 967/1000\n",
      "10/10 [==============================] - 0s 306us/sample - loss: 31.9909\n",
      "Epoch 968/1000\n",
      "10/10 [==============================] - 0s 206us/sample - loss: 31.7468\n",
      "Epoch 969/1000\n",
      "10/10 [==============================] - 0s 220us/sample - loss: 31.5023\n",
      "Epoch 970/1000\n",
      "10/10 [==============================] - 0s 298us/sample - loss: 31.2673\n",
      "Epoch 971/1000\n",
      "10/10 [==============================] - 0s 404us/sample - loss: 31.0393\n",
      "Epoch 972/1000\n",
      "10/10 [==============================] - 0s 210us/sample - loss: 30.8096\n",
      "Epoch 973/1000\n",
      "10/10 [==============================] - 0s 239us/sample - loss: 30.5711\n",
      "Epoch 974/1000\n",
      "10/10 [==============================] - 0s 343us/sample - loss: 30.3422\n",
      "Epoch 975/1000\n",
      "10/10 [==============================] - 0s 274us/sample - loss: 30.1248\n",
      "Epoch 976/1000\n",
      "10/10 [==============================] - 0s 195us/sample - loss: 29.9095\n",
      "Epoch 977/1000\n",
      "10/10 [==============================] - 0s 230us/sample - loss: 29.6738\n",
      "Epoch 978/1000\n",
      "10/10 [==============================] - 0s 412us/sample - loss: 29.4443\n",
      "Epoch 979/1000\n",
      "10/10 [==============================] - 0s 204us/sample - loss: 29.2303\n",
      "Epoch 980/1000\n",
      "10/10 [==============================] - 0s 391us/sample - loss: 29.0025\n",
      "Epoch 981/1000\n",
      "10/10 [==============================] - 0s 307us/sample - loss: 28.7740\n",
      "Epoch 982/1000\n",
      "10/10 [==============================] - 0s 182us/sample - loss: 28.5608\n",
      "Epoch 983/1000\n",
      "10/10 [==============================] - 0s 245us/sample - loss: 28.3585\n",
      "Epoch 984/1000\n",
      "10/10 [==============================] - 0s 324us/sample - loss: 28.1449\n",
      "Epoch 985/1000\n",
      "10/10 [==============================] - 0s 276us/sample - loss: 27.9278\n",
      "Epoch 986/1000\n",
      "10/10 [==============================] - 0s 196us/sample - loss: 27.7098\n",
      "Epoch 987/1000\n",
      "10/10 [==============================] - 0s 236us/sample - loss: 27.4927\n",
      "Epoch 988/1000\n",
      "10/10 [==============================] - 0s 267us/sample - loss: 27.2851\n",
      "Epoch 989/1000\n",
      "10/10 [==============================] - 0s 530us/sample - loss: 27.0783\n",
      "Epoch 990/1000\n",
      "10/10 [==============================] - 0s 184us/sample - loss: 26.8728\n",
      "Epoch 991/1000\n",
      "10/10 [==============================] - 0s 174us/sample - loss: 26.6790\n",
      "Epoch 992/1000\n",
      "10/10 [==============================] - 0s 284us/sample - loss: 26.4713\n",
      "Epoch 993/1000\n",
      "10/10 [==============================] - 0s 438us/sample - loss: 26.2691\n",
      "Epoch 994/1000\n",
      "10/10 [==============================] - 0s 173us/sample - loss: 26.0719\n",
      "Epoch 995/1000\n",
      "10/10 [==============================] - 0s 214us/sample - loss: 25.8941\n",
      "Epoch 996/1000\n",
      "10/10 [==============================] - 0s 281us/sample - loss: 25.6979\n",
      "Epoch 997/1000\n",
      "10/10 [==============================] - 0s 328us/sample - loss: 25.5020\n",
      "Epoch 998/1000\n",
      "10/10 [==============================] - 0s 222us/sample - loss: 25.3283\n",
      "Epoch 999/1000\n",
      "10/10 [==============================] - 0s 176us/sample - loss: 25.1631\n",
      "Epoch 1000/1000\n",
      "10/10 [==============================] - 0s 251us/sample - loss: 24.9739\n",
      "[[101.2809]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as kb\n",
    "import numpy as np\n",
    "\n",
    "# This is an ultra simple model to learn squares of numbers.\n",
    "# Do not take the model too seriosuly, it will overfit and is only \n",
    "# for deminstration purpose\n",
    "keras_model=tf.keras.Sequential([ \n",
    "    tf.keras.layers.Dense(32,activation=tf.nn.relu,input_shape=[1]),\n",
    "    tf.keras.layers.Dense(32,activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(1)\n",
    "]\n",
    ")\n",
    "\n",
    "# Now we define our custom loss function\n",
    "\n",
    "def custom_loss(y_actual,y_pred): \n",
    "    custom_loss=kb.square(y_actual-y_pred)\n",
    "    return custom_loss\n",
    "\n",
    "optimizer=tf.keras.optimizers.RMSprop(0.001)\n",
    "keras_model.compile(loss=custom_loss,optimizer=optimizer)\n",
    "\n",
    "#Sample data\n",
    "x=[1,2,3,4,5,6,7,8,9,10]\n",
    "x=np.asarray(x).reshape((10,1))\n",
    "y=[1,4,9,16,25,36,49,64,81,100]\n",
    "y=np.asarray(y).reshape((10,1))\n",
    "y=y.astype(np.float32)\n",
    "keras_model.fit(x,y,batch_size=10,epochs=1000)\n",
    "print(keras_model.predict([11]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.keras.layers.core.Dense object at 0x1393cffd0>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Custom training\n",
    "x=[1,2,3,4,5,6,7,8,9,10]\n",
    "x=np.asarray(x,dtype=np.float32).reshape((10,1))\n",
    "y=[1,4,9,16,25,36,49,64,81,100]\n",
    "y=np.asarray(y,dtype=np.float32).reshape((10,1))\n",
    "m=model()\n",
    "\n",
    "for i in range(10):\n",
    "    m.network_learn(x,y)\n",
    "\n",
    "    \n",
    "# Test Case\n",
    "#x=[11]\n",
    "#x=np.asarray(x,dtype=np.float32).reshape((1,1))\n",
    "#print(m.run(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     fun: 4.440892098500626e-16\n",
       " message: 'Optimization terminated successfully.'\n",
       "    nfev: 3153\n",
       "     nit: 102\n",
       " success: True\n",
       "       x: array([0., 0.])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.optimize import differential_evolution\n",
    "import numpy as np\n",
    "def ackley(x):\n",
    "    arg1 = -0.2 * np.sqrt(0.5 * (x[0] ** 2 + x[1] ** 2))\n",
    "    arg2 = 0.5 * (np.cos(2. * np.pi * x[0]) + np.cos(2. * np.pi * x[1]))\n",
    "    return -20. * np.exp(arg1) - np.exp(arg2) + 20. + np.e\n",
    "bounds = [(-5, 5), (-5, 5)]\n",
    "result = differential_evolution(ackley, bounds)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
