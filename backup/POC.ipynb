{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "import random\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from scipy import interp\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from matplotlib import pyplot\n",
    "\n",
    "from scipy.optimize import differential_evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "dataset = pd.read_csv(os.path.join(os.getcwd(), 'data/pima_indian_data.csv'))\n",
    "\n",
    "# creating input features and target variables\n",
    "X = np.asarray(dataset.iloc[:,0:8], dtype=np.float32)\n",
    "y = np.asarray(dataset.iloc[:,8], dtype=np.float32)\n",
    "\n",
    "#standardizing the input feature\n",
    "sc = StandardScaler()\n",
    "X = sc.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "num_features = X_train.shape[1]\n",
    "\n",
    "# Train with RF\n",
    "rf = RandomForestClassifier(n_estimators=25, random_state=3)\n",
    "rf.fit(X_train, y_train)\n",
    "rf_probs = rf.predict_proba(X_test)\n",
    "# Train with LR\n",
    "lr = LogisticRegression(solver='lbfgs')\n",
    "lr.fit(X_train, y_train)\n",
    "# predict probabilities\n",
    "lr_probs = lr.predict_proba(X_test)\n",
    "\n",
    "# keep probabilities for the positive outcome only\n",
    "rf_probs = rf_probs[:, 1]\n",
    "lr_probs = lr_probs[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF: ROC AUC=0.754\n",
      "LS: ROC AUC=0.794\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x11f8e1090>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# calculate scores\n",
    "rf_auc = roc_auc_score(y_test, rf_probs)\n",
    "lr_auc = roc_auc_score(y_test, lr_probs)\n",
    "\n",
    "# summarize scores\n",
    "print('RF: ROC AUC=%.3f' % (rf_auc))\n",
    "print('LS: ROC AUC=%.3f' % (lr_auc))\n",
    "\n",
    "# calculate roc curves\n",
    "rf_fpr, rf_tpr, _ = roc_curve(y_test, rf_probs)\n",
    "lr_fpr, lr_tpr, _ = roc_curve(y_test, lr_probs)\n",
    "\n",
    "# plot the roc curve for the model\n",
    "pyplot.plot(rf_fpr, rf_tpr, linestyle='--', label='RF')\n",
    "pyplot.plot(lr_fpr, lr_tpr, marker='.', label='LR')\n",
    "\n",
    "# axis labels\n",
    "pyplot.xlabel('False Positive Rate')\n",
    "pyplot.ylabel('True Positive Rate')\n",
    "\n",
    "# show the legend\n",
    "pyplot.legend()\n",
    "# show the plot\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ConeOpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ER (train) = 0.3414179104477612\n",
      "ER (test) = 0.36363636363636365\n",
      "================================================================================\n",
      "Y (curr) has prob =  [0.5696035]\n",
      "X (curr) =  [[ 0.04683068  0.78669226  1.1830373  -1.2868819  -0.6935592  -0.10026471\n",
      "   0.20322894  2.365418  ]]\n",
      "================================================================================\n",
      "Y (ref) has prob =  [0.37276665]\n",
      "X (ref) =  [[ 1.2342399  -0.27721694  0.14974046 -1.2868819  -0.6935592  -1.064249\n",
      "   1.1118226   2.1099823 ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"ER (train) = {}\".format(sum(y_train)/len(y_train)))\n",
    "print(\"ER (test) = {}\".format(sum(y_test)/len(y_test)))\n",
    "#np.select(y_test == 1, y_test)\n",
    "\n",
    "# get current point and reference point \n",
    "idx_y_test_pos = np.argwhere(y_test == 1).flatten()\n",
    "idx_y_test_neg = np.argwhere(y_test == 0).flatten()\n",
    "\n",
    "idx_curr = idx_y_test_pos[5]\n",
    "idx_ref = idx_y_test_neg[4]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "X_curr = X_test[idx_curr:idx_curr+1, :]\n",
    "print(\"Y (curr) has prob = \", lr.predict_proba(X_curr)[:, 1])\n",
    "print(\"X (curr) = \", X_curr)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "X_ref = X_test[idx_ref:idx_ref+1, :]\n",
    "print(\"Y (ref) has prob = \", lr.predict_proba(X_ref)[:, 1])\n",
    "print(\"X (ref) = \", X_ref)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.differential_evolution.html#r108fc14fa019-1\n",
    "\n",
    "def run_coneopt(X_curr, X_ref, max_step = 0.3, fixed_features = []):\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    X_cone = X_ref - X_curr\n",
    "    print(\"Cone = \",  X_cone)\n",
    "\n",
    "    bounds = list(zip(X_curr.flatten(), (X_curr + X_cone * max_step).flatten()))\n",
    "    for b in range(len(bounds)):\n",
    "        bound = bounds[b]\n",
    "        if bound[0] > bound[1]:\n",
    "            bounds[b] = bound[1], bound[0]\n",
    "\n",
    "    for idx_feat in fixed_features:\n",
    "        bounds[idx_feat] = (X_curr[0][idx_feat], X_curr[0][idx_feat])\n",
    "    print(\"Bounds = \", bounds)\n",
    "\n",
    "    #print(X_curr, X_curr + X_cone * max_step)\n",
    "\n",
    "    def my_predict_proba(x, method):\n",
    "        return method.predict_proba(x.reshape(1, len(x)))[:, 1]\n",
    "\n",
    "    result = differential_evolution(\n",
    "        func=my_predict_proba, \n",
    "        bounds=bounds, \n",
    "        args=[lr],\n",
    "        disp=True,\n",
    "        seed=0)\n",
    "    X_opt = result.x.reshape(1, len(result.x))\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(\"CURR\")\n",
    "    print(\"Y (curr) has prob = \", lr.predict_proba(X_curr)[:, 1])\n",
    "    print(\"X (curr) = \", X_curr)\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(\"OPT\")\n",
    "    print(\"Y (opt) has prob = \", lr.predict_proba(X_opt)[:, 1])\n",
    "    print(\"X (opt) = \", X_opt)\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(\"REF\")\n",
    "    print(\"Y (ref) has prob = \", lr.predict_proba(X_ref)[:, 1])\n",
    "    print(\"X (ref) = \", X_ref)\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    return X_opt\n",
    "    \n",
    "    \n",
    "#https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.differential_evolution.html#r108fc14fa019-1\n",
    "\n",
    "def run_coneopt2(X_curr, X_ref, max_step = 0.3, fixed_features = []):\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    X_cone = X_ref - X_curr\n",
    "    print(\"Cone = \",  X_cone)\n",
    "\n",
    "    bounds = list(zip(X_curr.flatten(), (X_curr + X_cone * max_step).flatten()))\n",
    "    for b in range(len(bounds)):\n",
    "        bound = bounds[b]\n",
    "        if bound[0] > bound[1]:\n",
    "            bounds[b] = bound[1], bound[0]\n",
    "\n",
    "    bounds2 = []\n",
    "    fixed_x = []\n",
    "    non_fixed_features = []\n",
    "    for b in range(len(bounds)):\n",
    "        if b not in set(fixed_features):\n",
    "            bounds2.append(bounds[b])\n",
    "            non_fixed_features.append(b)\n",
    "        else:\n",
    "            fixed_x.append(X_curr[0][b]) \n",
    "    num_features = len(bounds)\n",
    "    bounds = bounds2\n",
    "    num_features_active = len(bounds)\n",
    "    print(\"Bounds = \", bounds)\n",
    "    print(\"fixed_features = \", fixed_features)\n",
    "    print(\"fixed_x = \", fixed_x)\n",
    "    print(\"non_fixed_features = \", non_fixed_features)\n",
    "\n",
    "    #print(X_curr, X_curr + X_cone * max_step)\n",
    "\n",
    "    def get_full_x(non_fixed_x, fixed_features, non_fixed_features, fixed_x):\n",
    "        full_x = [b for b in range(len(fixed_features) + len(non_fixed_features))]\n",
    "        for b in range(len(non_fixed_features)):\n",
    "            full_x[non_fixed_features[b]] = non_fixed_x[b]\n",
    "        for b in range(len(fixed_features)):\n",
    "            full_x[fixed_features[b]] = fixed_x[b]\n",
    "        return full_x\n",
    "\n",
    "    def my_predict_proba(non_fixed_x, method, fixed_features, non_fixed_features, fixed_x):\n",
    "        if non_fixed_features == []:\n",
    "            return method.predict_proba(x.reshape(1, len(x)))[:, 1]\n",
    "        else:\n",
    "            full_x = get_full_x(non_fixed_x, fixed_features, non_fixed_features, fixed_x)\n",
    "            #print(\"non_fixed_features\", non_fixed_features)\n",
    "            #print(\"fixed_features\", fixed_features)\n",
    "            return method.predict_proba(np.array(full_x).reshape(1, len(full_x)))[:, 1]\n",
    "\n",
    "    result = differential_evolution(\n",
    "        func=my_predict_proba, \n",
    "        bounds=bounds, \n",
    "        args=[lr, fixed_features, non_fixed_features, fixed_x],\n",
    "        disp=True,\n",
    "        seed=0)\n",
    "    \n",
    "    full_x = get_full_x(result.x, fixed_features, non_fixed_features, fixed_x)\n",
    "    X_opt = np.array(full_x).reshape(1, len(full_x))\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(\"CURR\")\n",
    "    print(\"Y (curr) has prob = \", lr.predict_proba(X_curr)[:, 1])\n",
    "    print(\"X (curr) = \", X_curr)\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(\"OPT\")\n",
    "    print(\"Y (opt) has prob = \", lr.predict_proba(X_opt)[:, 1])\n",
    "    print(\"X (opt) = \", X_opt)\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(\"REF\")\n",
    "    print(\"Y (ref) has prob = \", lr.predict_proba(X_ref)[:, 1])\n",
    "    print(\"X (ref) = \", X_ref)\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    return X_opt\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_fixed_features(\n",
    "    X_curr,\n",
    "    X_opt,\n",
    "    influential_features_percentage = 0.5, \n",
    "    delta_feature_eps = 0.0001):\n",
    "\n",
    "    # Identify the most influential features -- 50% of the most important features.\n",
    "    #influential_features_percentage = 0.5 \n",
    "    #delta_feature_eps = 0.0001\n",
    "    \n",
    "    num_features = X_curr.shape[1]\n",
    "    diff = list(map(abs, X_opt.flatten() - X_curr.flatten()))\n",
    "    for i in range(len(diff)):\n",
    "        if diff[i] == 0:\n",
    "            diff[i] += random.randrange(100)*delta_feature_eps\n",
    "    num_features_changed = sum(np.array(diff) > delta_feature_eps)\n",
    "\n",
    "    num_target_features = int(max(1, influential_features_percentage * num_features_changed))\n",
    "    print(\"Will use [{}] feautres for the analysis\".format(num_target_features))\n",
    "\n",
    "    #print(\"diff\", diff)\n",
    "    #print(\"list(map(abs, X_curr))\", list(map(abs, X_curr)))\n",
    "    delta_changes = np.divide(diff, list(map(abs, X_curr)))[0]\n",
    "    print(\"delta_changes = \", delta_changes)\n",
    "    cutoff_feature_value = sorted(delta_changes, reverse = True)[num_target_features - 1]\n",
    "    print(\"Cutoff feature values (only feature with values >= cutoff will be included) = {}\".format(cutoff_feature_value))\n",
    "    flag_required_feature = delta_changes >= cutoff_feature_value\n",
    "    \n",
    "    #print(idx_required_feature)\n",
    "    assert(sum(flag_required_feature) == num_target_features)\n",
    "    \n",
    "    return [i for i in range(num_features) if flag_required_feature[i]==False]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Cone =  [[ 1.1874093  -1.0639092  -1.0332968   0.          0.         -0.9639843\n",
      "   0.90859365 -0.2554357 ]]\n",
      "Bounds =  [(0.046830676, 0.46242392), (0.41432405, 0.78669226), (0.82138336, 1.1830373), (-1.2868819, -1.2868819), (-0.6935592, -0.6935592), (-0.4376592, -0.10026471), (0.20322894, 0.5212367), (2.2760155, 2.365418)]\n",
      "differential_evolution step 1: f(x)= 0.423362\n",
      "differential_evolution step 2: f(x)= 0.415951\n",
      "differential_evolution step 3: f(x)= 0.413204\n",
      "differential_evolution step 4: f(x)= 0.412401\n",
      "differential_evolution step 5: f(x)= 0.412401\n",
      "differential_evolution step 6: f(x)= 0.411948\n",
      "differential_evolution step 7: f(x)= 0.411948\n",
      "differential_evolution step 8: f(x)= 0.410128\n",
      "differential_evolution step 9: f(x)= 0.410128\n",
      "differential_evolution step 10: f(x)= 0.410128\n",
      "differential_evolution step 11: f(x)= 0.408271\n",
      "differential_evolution step 12: f(x)= 0.408271\n",
      "differential_evolution step 13: f(x)= 0.408271\n",
      "differential_evolution step 14: f(x)= 0.407206\n",
      "differential_evolution step 15: f(x)= 0.403202\n",
      "differential_evolution step 16: f(x)= 0.403202\n",
      "differential_evolution step 17: f(x)= 0.403202\n",
      "differential_evolution step 18: f(x)= 0.403202\n",
      "differential_evolution step 19: f(x)= 0.403202\n",
      "differential_evolution step 20: f(x)= 0.403202\n",
      "differential_evolution step 21: f(x)= 0.403202\n",
      "differential_evolution step 22: f(x)= 0.402975\n",
      "differential_evolution step 23: f(x)= 0.402975\n",
      "differential_evolution step 24: f(x)= 0.402975\n",
      "================================================================================\n",
      "CURR\n",
      "Y (curr) has prob =  [0.5696035]\n",
      "X (curr) =  [[ 0.04683068  0.78669226  1.1830373  -1.2868819  -0.6935592  -0.10026471\n",
      "   0.20322894  2.365418  ]]\n",
      "================================================================================\n",
      "OPT\n",
      "Y (opt) has prob =  [0.39915256]\n",
      "X (opt) =  [[ 0.04683068  0.41432405  1.18303728 -1.28688192 -0.69355923 -0.4376592\n",
      "   0.20322894  2.27601552]]\n",
      "================================================================================\n",
      "REF\n",
      "Y (ref) has prob =  [0.37276665]\n",
      "X (ref) =  [[ 1.2342399  -0.27721694  0.14974046 -1.2868819  -0.6935592  -1.064249\n",
      "   1.1118226   2.1099823 ]]\n",
      "================================================================================\n",
      "Will use [4] feautres for the analysis\n",
      "delta_changes =  [0.20072313 0.47333403 0.00828376 0.0075376  0.00591154 3.36503721\n",
      " 0.01968224 0.03779562]\n",
      "Cutoff feature values (only feature with values >= cutoff will be included) = 0.03779561955803695\n",
      "[2, 3, 4, 6]\n",
      "================================================================================\n",
      "Cone =  [[ 1.1874093  -1.0639092  -1.0332968   0.          0.         -0.9639843\n",
      "   0.90859365 -0.2554357 ]]\n",
      "Bounds =  [(0.046830676, 0.46242392), (0.41432405, 0.78669226), (-0.4376592, -0.10026471), (2.2760155, 2.365418)]\n",
      "fixed_features =  [2, 3, 4, 6]\n",
      "fixed_x =  [1.1830373, -1.2868819, -0.6935592, 0.20322894]\n",
      "non_fixed_features =  [0, 1, 5, 7]\n",
      "differential_evolution step 1: f(x)= 0.427976\n",
      "differential_evolution step 2: f(x)= 0.416595\n",
      "differential_evolution step 3: f(x)= 0.416547\n",
      "differential_evolution step 4: f(x)= 0.411724\n",
      "differential_evolution step 5: f(x)= 0.407609\n",
      "differential_evolution step 6: f(x)= 0.407609\n",
      "differential_evolution step 7: f(x)= 0.407609\n",
      "differential_evolution step 8: f(x)= 0.406949\n",
      "differential_evolution step 9: f(x)= 0.405455\n",
      "differential_evolution step 10: f(x)= 0.405128\n",
      "differential_evolution step 11: f(x)= 0.403216\n",
      "differential_evolution step 12: f(x)= 0.403216\n",
      "differential_evolution step 13: f(x)= 0.402595\n",
      "differential_evolution step 14: f(x)= 0.402595\n",
      "differential_evolution step 15: f(x)= 0.402424\n",
      "================================================================================\n",
      "CURR\n",
      "Y (curr) has prob =  [0.5696035]\n",
      "X (curr) =  [[ 0.04683068  0.78669226  1.1830373  -1.2868819  -0.6935592  -0.10026471\n",
      "   0.20322894  2.365418  ]]\n",
      "================================================================================\n",
      "OPT\n",
      "Y (opt) has prob =  [0.39915256]\n",
      "X (opt) =  [[ 0.04683068  0.41432405  1.18303728 -1.28688192 -0.69355923 -0.4376592\n",
      "   0.20322894  2.27601552]]\n",
      "================================================================================\n",
      "REF\n",
      "Y (ref) has prob =  [0.37276665]\n",
      "X (ref) =  [[ 1.2342399  -0.27721694  0.14974046 -1.2868819  -0.6935592  -1.064249\n",
      "   1.1118226   2.1099823 ]]\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/scipy/optimize/_differentialevolution.py:1154: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return (parameters - self.__scale_arg1) / self.__scale_arg2 + 0.5\n"
     ]
    }
   ],
   "source": [
    "max_step = 0.35\n",
    "\n",
    "X_opt_init = run_coneopt(X_curr, X_ref, max_step=max_step, fixed_features=[])\n",
    "fixed_features = identify_fixed_features(X_curr, X_opt_init)\n",
    "print(fixed_features)\n",
    "\n",
    "#X_opt = run_coneopt(X_curr, X_ref, max_step=max_step, fixed_features=fixed_features)\n",
    "X_opt = run_coneopt2(X_curr, X_ref, max_step=max_step, fixed_features=fixed_features)\n",
    "#X_opt = run_coneopt(X_curr, X_ref, max_step=max_step, fixed_features=fixed_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NO NEED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y (curr) has prob =  [0.90827086]\n",
      "X (curr) =  [[2.1247969  1.0683153  0.3563998  0.4693026  0.60843456 0.16609915\n",
      "  2.6603563  1.5139652 ]]\n",
      "Y (opt) has prob =  [0.53369112]\n",
      "X (opt) =  [[ 0.49428485  0.33325735  0.32808709  0.29517624  0.54907977 -0.21322886\n",
      "   1.8112237   0.72292829]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Y (curr) has prob = \", lr.predict_proba(X_curr)[:, 1])\n",
    "print(\"X (curr) = \", X_curr)\n",
    "\n",
    "X_opt = result.x.reshape(1, len(result.x))\n",
    "print(\"Y (opt) has prob = \", lr.predict_proba(X_opt)[:, 1])\n",
    "print(\"X (opt) = \", X_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import feature_column\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import backend as k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConEOpt: Contractual Explanation with Optimization\n",
      "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
      "0   63    1   1       145   233    1        2      150      0      2.3      3   \n",
      "1   67    1   4       160   286    0        2      108      1      1.5      2   \n",
      "2   67    1   4       120   229    0        2      129      1      2.6      2   \n",
      "3   37    1   3       130   250    0        0      187      0      3.5      3   \n",
      "4   41    0   2       130   204    0        2      172      0      1.4      1   \n",
      "\n",
      "   ca        thal  target  \n",
      "0   0       fixed       0  \n",
      "1   3      normal       1  \n",
      "2   2  reversible       0  \n",
      "3   0      normal       0  \n",
      "4   0      normal       0  \n",
      "193 train examples\n",
      "49 validation examples\n",
      "61 test examples\n",
      "Every feature: ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal']\n",
      "A batch of ages: tf.Tensor([59 54 43 67 47], shape=(5,), dtype=int32)\n",
      "A batch of targets: tf.Tensor([0 0 0 0 0], shape=(5,), dtype=int32)\n",
      "NumericColumn(key='age', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None)\n",
      "WARNING:tensorflow:From /Users/AF45008/Research/ColinML/coneopt/conda_env36/lib/python3.6/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4267: IndicatorColumn._variable_shape (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "WARNING:tensorflow:From /Users/AF45008/Research/ColinML/coneopt/conda_env36/lib/python3.6/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4322: CrossedColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "WARNING:tensorflow:From /Users/AF45008/Research/ColinML/coneopt/conda_env36/lib/python3.6/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4322: VocabularyListCategoricalColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "Train for 7 steps, validate for 2 steps\n",
      "Epoch 1/5\n",
      "7/7 [==============================] - 1s 186ms/step - loss: 1.4338 - accuracy: 0.6580 - val_loss: 0.5768 - val_accuracy: 0.7347\n",
      "Epoch 2/5\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8596 - accuracy: 0.5492 - val_loss: 1.2900 - val_accuracy: 0.6735\n",
      "Epoch 3/5\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.9963 - accuracy: 0.7513 - val_loss: 0.7582 - val_accuracy: 0.6735\n",
      "Epoch 4/5\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.5179 - accuracy: 0.7409 - val_loss: 0.5028 - val_accuracy: 0.7347\n",
      "Epoch 5/5\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.4584 - accuracy: 0.7668 - val_loss: 0.6129 - val_accuracy: 0.6735\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.5560 - accuracy: 0.7377\n",
      "Accuracy 0.73770493\n"
     ]
    }
   ],
   "source": [
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.max_rows', None)\n",
    "\n",
    "    # https://www.tensorflow.org/tutorials/structured_data/feature_columns\n",
    "    print(\"ConEOpt: Contractual Explanation with Optimization\")\n",
    "\n",
    "    # Use Pandas to create a dataframe\n",
    "    filepath = '/Users/AF45008/Research/ColinML/coneopt/data/heart.csv'\n",
    "    dataframe = pd.read_csv(filepath)\n",
    "    print(dataframe.head())\n",
    "\n",
    "    # Split the dataframe into train, validation, and test\n",
    "    train, test = train_test_split(dataframe, test_size=0.2)\n",
    "    train, val = train_test_split(train, test_size=0.2)\n",
    "    print(len(train), 'train examples')\n",
    "    print(len(val), 'validation examples')\n",
    "    print(len(test), 'test examples')\n",
    "\n",
    "    # Create an input pipeline using tf.data\n",
    "    # A utility method to create a tf.data dataset from a Pandas Dataframe\n",
    "    def df_to_dataset(dataframe, shuffle=True, batch_size=32):\n",
    "        dataframe = dataframe.copy()\n",
    "        labels = dataframe.pop('target')\n",
    "        ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
    "        if shuffle:\n",
    "            ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "        ds = ds.batch(batch_size)\n",
    "        return ds\n",
    "\n",
    "    batch_size = 5  # A small batch sized is used for demonstration purposes\n",
    "    train_ds = df_to_dataset(train, batch_size=batch_size)\n",
    "    val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\n",
    "    test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)\n",
    "\n",
    "    # Understand the input pipeline\n",
    "    for feature_batch, label_batch in train_ds.take(1):\n",
    "        print('Every feature:', list(feature_batch.keys()))\n",
    "        print('A batch of ages:', feature_batch['age'])\n",
    "        print('A batch of targets:', label_batch)\n",
    "\n",
    "    # Numeric columns\n",
    "    age = feature_column.numeric_column(\"age\")\n",
    "    print(age)\n",
    "\n",
    "    feature_columns = []\n",
    "\n",
    "    # numeric cols\n",
    "    for header in ['age', 'trestbps', 'chol', 'thalach', 'oldpeak', 'slope', 'ca']:\n",
    "      feature_columns.append(feature_column.numeric_column(header))\n",
    "\n",
    "    # bucketized cols\n",
    "    age_buckets = feature_column.bucketized_column(age, boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65])\n",
    "    feature_columns.append(age_buckets)\n",
    "\n",
    "    # indicator cols\n",
    "    thal = feature_column.categorical_column_with_vocabulary_list(\n",
    "          'thal', ['fixed', 'normal', 'reversible'])\n",
    "    thal_one_hot = feature_column.indicator_column(thal)\n",
    "    feature_columns.append(thal_one_hot)\n",
    "\n",
    "    # embedding cols\n",
    "    thal_embedding = feature_column.embedding_column(thal, dimension=8)\n",
    "    feature_columns.append(thal_embedding)\n",
    "\n",
    "    # crossed cols\n",
    "    crossed_feature = feature_column.crossed_column([age_buckets, thal], hash_bucket_size=1000)\n",
    "    crossed_feature = feature_column.indicator_column(crossed_feature)\n",
    "    feature_columns.append(crossed_feature)\n",
    "\n",
    "    feature_layer = tf.keras.layers.DenseFeatures(feature_columns)\n",
    "    batch_size = 32\n",
    "    train_ds = df_to_dataset(train, batch_size=batch_size)\n",
    "    val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\n",
    "    test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)\n",
    "\n",
    "    # Create, compile, and train the model\n",
    "    model = tf.keras.Sequential([\n",
    "        feature_layer,\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model.fit(train_ds,\n",
    "              validation_data=val_ds,\n",
    "              epochs=5)\n",
    "\n",
    "    loss, accuracy = model.evaluate(test_ds)\n",
    "    print(\"Accuracy\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 3ms/step - loss: 0.5560 - accuracy: 0.7377\n",
      "     age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
      "96    41    1   3       112   250    0        0      179      0      0.0   \n",
      "283   60    0   4       150   258    0        2      157      0      2.6   \n",
      "186   42    1   2       120   295    0        0      162      0      0.0   \n",
      "31    60    1   4       117   230    1        0      160      1      1.4   \n",
      "112   58    0   4       100   248    0        2      122      0      1.0   \n",
      "266   60    1   4       130   253    0        0      144      1      1.4   \n",
      "295   39    1   4       118   219    0        0      140      0      1.2   \n",
      "134   42    1   3       120   240    1        0      194      0      0.8   \n",
      "173   47    1   4       112   204    0        0      143      0      0.1   \n",
      "248   57    0   0       140   241    0        1      123      1      0.2   \n",
      "52    53    0   4       130   264    0        2      143      0      0.4   \n",
      "297   56    1   4       125   249    1        2      144      1      1.2   \n",
      "179   49    1   3       118   149    0        2      126      0      0.8   \n",
      "79    54    1   3       120   258    0        2      147      0      0.4   \n",
      "292   59    1   4       140   177    0        0      162      1      0.0   \n",
      "55    52    1   2       120   325    0        0      172      0      0.2   \n",
      "103   64    1   4       120   246    0        2       96      1      2.2   \n",
      "66    62    0   3       130   263    0        0       97      0      1.2   \n",
      "33    59    1   4       135   234    0        0      161      0      0.5   \n",
      "122   64    1   4       145   212    0        2      132      0      2.0   \n",
      "73    65    1   1       138   282    1        2      174      0      1.4   \n",
      "2     67    1   4       120   229    0        2      129      1      2.6   \n",
      "74    45    0   2       130   234    0        2      175      0      0.6   \n",
      "77    44    1   2       120   220    0        0      170      0      0.0   \n",
      "277   60    1   4       145   282    0        2      142      1      2.8   \n",
      "90    59    1   1       170   288    0        2      159      0      0.2   \n",
      "252   57    0   1       130   236    0        0      174      0      0.0   \n",
      "299   43    0   4       132   341    1        2      136      1      3.0   \n",
      "224   57    1   2       154   232    0        2      164      0      0.0   \n",
      "276   54    1   4       120   188    0        0      113      0      1.4   \n",
      "70    63    1   4       130   330    1        2      132      1      1.8   \n",
      "290   49    1   3       120   188    0        0      139      0      2.0   \n",
      "240   41    1   2       120   157    0        0      182      0      0.0   \n",
      "126   43    1   3       130   315    0        0      162      0      1.9   \n",
      "172   34    0   2       118   210    0        0      192      0      0.7   \n",
      "265   44    1   2       130   219    0        2      188      0      0.0   \n",
      "301   48    1   4       130   256    1        2      150      1      0.0   \n",
      "159   41    1   3       130   214    0        2      168      0      2.0   \n",
      "71    51    1   3       100   222    0        0      143      1      1.2   \n",
      "208   58    0   2       136   319    1        2      152      0      0.0   \n",
      "41    65    0   3       155   269    0        0      148      0      0.8   \n",
      "220   59    1   1       134   204    0        0      162      0      0.8   \n",
      "227   47    1   3       130   253    0        0      179      0      0.0   \n",
      "148   64    0   4       180   325    0        0      154      1      0.0   \n",
      "16    48    1   2       110   229    0        0      168      0      1.0   \n",
      "223   39    0   3       138   220    0        0      152      0      0.0   \n",
      "231   58    1   4       114   318    0        1      140      0      4.4   \n",
      "136   54    1   2       192   283    0        2      195      0      0.0   \n",
      "85    70    1   4       145   174    0        0      125      1      2.6   \n",
      "206   57    1   2       124   261    0        0      141      0      0.3   \n",
      "302   63    0   4       150   407    0        2      154      0      4.0   \n",
      "256   61    0   4       130   330    0        2      169      0      0.0   \n",
      "267   54    1   4       124   266    0        2      109      1      2.2   \n",
      "100   42    0   4       102   265    0        2      122      0      0.6   \n",
      "69    35    0   4       138   183    0        0      182      0      1.4   \n",
      "157   37    0   3       120   215    0        0      170      0      0.0   \n",
      "174   67    0   3       152   277    0        0      172      0      0.0   \n",
      "50    58    1   4       150   270    0        2      111      1      0.8   \n",
      "175   54    1   4       110   206    0        2      108      1      0.0   \n",
      "181   54    0   3       160   201    0        0      163      0      0.0   \n",
      "219   71    0   4       112   149    0        0      125      0      1.6   \n",
      "\n",
      "     slope  ca        thal  target  \n",
      "96       1   0      normal       0  \n",
      "283      2   2  reversible       1  \n",
      "186      1   0      normal       0  \n",
      "31       1   2  reversible       1  \n",
      "112      2   0      normal       0  \n",
      "266      1   1  reversible       0  \n",
      "295      2   0  reversible       1  \n",
      "134      3   0  reversible       0  \n",
      "173      1   0      normal       0  \n",
      "248      1   0      normal       0  \n",
      "52       2   0      normal       0  \n",
      "297      2   1      normal       0  \n",
      "179      1   3      normal       0  \n",
      "79       2   0  reversible       0  \n",
      "292      1   1  reversible       1  \n",
      "55       1   0      normal       0  \n",
      "103      3   1      normal       1  \n",
      "66       2   1  reversible       1  \n",
      "33       2   0  reversible       0  \n",
      "122      2   2       fixed       1  \n",
      "73       2   1      normal       0  \n",
      "2        2   2  reversible       0  \n",
      "74       2   0      normal       0  \n",
      "77       1   0      normal       0  \n",
      "277      2   2  reversible       1  \n",
      "90       2   0  reversible       0  \n",
      "252      1   1           2       0  \n",
      "299      2   0  reversible       1  \n",
      "224      1   1      normal       0  \n",
      "276      2   1  reversible       1  \n",
      "70       1   3  reversible       1  \n",
      "290      2   3  reversible       1  \n",
      "240      1   0      normal       0  \n",
      "126      1   1      normal       0  \n",
      "172      1   0      normal       0  \n",
      "265      1   0      normal       0  \n",
      "301      1   2  reversible       1  \n",
      "159      2   0      normal       0  \n",
      "71       2   0      normal       0  \n",
      "208      1   2      normal       1  \n",
      "41       1   0      normal       0  \n",
      "220      1   2      normal       0  \n",
      "227      1   0      normal       0  \n",
      "148      1   0      normal       0  \n",
      "16       3   0  reversible       0  \n",
      "223      2   0      normal       0  \n",
      "231      3   3       fixed       1  \n",
      "136      1   1  reversible       0  \n",
      "85       3   0  reversible       1  \n",
      "206      1   0  reversible       0  \n",
      "302      2   3  reversible       1  \n",
      "256      1   0      normal       0  \n",
      "267      2   1  reversible       0  \n",
      "100      2   0      normal       0  \n",
      "69       1   0      normal       0  \n",
      "157      1   0      normal       0  \n",
      "174      1   1      normal       0  \n",
      "50       1   0  reversible       1  \n",
      "175      2   1      normal       1  \n",
      "181      1   1      normal       0  \n",
      "219      2   0      normal       0  \n"
     ]
    }
   ],
   "source": [
    "model.evaluate(test_ds)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BatchDataset' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-e5cb4b876385>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'BatchDataset' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "train_ds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.keras.layers.core.Dense object at 0x138fffcc0>\n"
     ]
    }
   ],
   "source": [
    "print(model.layers[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('/Users/AF45008/Research/ColinML/coneopt/data/pima_indian_data.csv')\n",
    "\n",
    "# creating input features and target variables\n",
    "X = np.asarray(dataset.iloc[:,0:8], dtype=np.float32)\n",
    "y = np.asarray(dataset.iloc[:,8], dtype=np.float32)\n",
    "\n",
    "#standardizing the input feature\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X = sc.fit_transform(X)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "#Build the model\n",
    "class model:\n",
    "    \n",
    "    def __init__(self):\n",
    "        xavier=tf.keras.initializers.GlorotUniform()\n",
    "        self.l1=tf.keras.layers.Dense(\n",
    "            4, \n",
    "            kernel_initializer=xavier, \n",
    "            #activation=tf.nn.linear, \n",
    "            input_shape=[1])\n",
    "        #self.l2=tf.keras.layers.Dense(\n",
    "        #    2, \n",
    "        #    kernel_initializer=xavier, \n",
    "        #    #activation=tf.nn.linear\n",
    "        #)\n",
    "        self.out=tf.keras.layers.Dense(\n",
    "            1, \n",
    "            kernel_initializer=xavier)\n",
    "        self.train_op = tf.keras.optimizers.Adagrad(learning_rate=0.1)\n",
    "        print(self.out)\n",
    "        \n",
    "    # Running the model\n",
    "    def run(self,X):\n",
    "        boom=self.l1(X)\n",
    "        #boom1=self.l2(boom)\n",
    "        boom1=boom\n",
    "        boom2=self.out(boom1)\n",
    "        return boom2\n",
    "    \n",
    "    #Custom loss fucntion\n",
    "    def get_loss(self,X,Y):\n",
    "        boom=self.l1(X)\n",
    "        #boom1=self.l2(boom)\n",
    "        boom1=boom\n",
    "        boom2=self.out(boom1)\n",
    "        return tf.math.square(boom2-Y)\n",
    "    \n",
    "    # get gradients\n",
    "    def get_grad(self,X,Y):\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(self.l1.variables)\n",
    "            #tape.watch(self.l2.variables)\n",
    "            tape.watch(self.out.variables)\n",
    "            L = self.get_loss(X,Y)\n",
    "            g = tape.gradient(L, [self.l1.variables[0],\n",
    "                                  self.l1.variables[1],\n",
    "                                  #self.l2.variables[0],\n",
    "                                  #self.l2.variables[1],\n",
    "                                  self.out.variables[0],\n",
    "                                  self.out.variables[1]])\n",
    "        return g\n",
    "    \n",
    "    # get gradients\n",
    "    def get_grad2(self, X):\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(self.l1.variables)\n",
    "            #tape.watch(self.l2.variables)\n",
    "            tape.watch(self.out.variables)\n",
    "            g = tape.gradient(self.run(X), \n",
    "                              [self.l1.variables[0],\n",
    "                               self.l1.variables[1],\n",
    "                                #self.l2.variables[0],\n",
    "                                #self.l2.variables[1],\n",
    "                                self.out.variables[0],\n",
    "                                self.out.variables[1]])\n",
    "        return g\n",
    "    \n",
    "    # perform gradient descent\n",
    "    def network_learn(self,X,Y):\n",
    "        g = self.get_grad(X,Y)\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"g={g}\")\n",
    "        # print(self.var)\n",
    "        self.train_op.apply_gradients(\n",
    "            zip(g, [self.l1.variables[0],\n",
    "                    self.l1.variables[1],\n",
    "                    #self.l2.variables[0],\n",
    "                    #self.l2.variables[1],\n",
    "                    self.out.variables[0],\n",
    "                    self.out.variables[1]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.keras.layers.core.Dense object at 0x13a511da0>\n",
      "================================================================================\n",
      "g=[<tf.Tensor: shape=(8, 4), dtype=float32, numpy=\n",
      "array([[ 839952.7  ,  350051.06 , -155563.7  , -748251.56 ],\n",
      "       [ 206270.97 ,   85963.61 ,  -38202.477, -183751.52 ],\n",
      "       [  51173.133,   21326.447,   -9477.538,  -45586.355],\n",
      "       [-389902.53 , -162492.23 ,   72212.02 ,  347335.25 ],\n",
      "       [-152099.31 ,  -63387.53 ,   28169.602,  135494.   ],\n",
      "       [ 315516.84 ,  131491.92 ,  -58435.4  , -281070.56 ],\n",
      "       [-176917.38 ,  -73730.47 ,   32766.035,  157602.56 ],\n",
      "       [1006093.5  ,  419290.38 , -186333.9  , -896254.1  ]],\n",
      "      dtype=float32)>, <tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 249008.38 ,  103774.484,  -46117.688, -221823.02 ], dtype=float32)>, <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\n",
      "array([[ -708786.06],\n",
      "       [-1098114.4 ],\n",
      "       [  100387.67],\n",
      "       [  816072.3 ]], dtype=float32)>, <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-228848.4], dtype=float32)>]\n",
      "================================================================================\n",
      "g=[<tf.Tensor: shape=(8, 4), dtype=float32, numpy=\n",
      "array([[ 394074.2   ,  140969.5   ,  -40488.773 , -346697.38  ],\n",
      "       [  16450.414 ,    5884.6895,   -1690.1808,  -14472.6875],\n",
      "       [-111259.414 ,  -39800.074 ,   11431.239 ,   97883.46  ],\n",
      "       [-107077.11  ,  -38303.973 ,   11001.532 ,   94203.97  ],\n",
      "       [  59705.906 ,   21358.19  ,   -6134.425 ,  -52527.875 ],\n",
      "       [ 199767.03  ,   71461.3   ,  -20524.867 , -175750.42  ],\n",
      "       [  34489.21  ,   12337.588 ,   -3543.559 ,  -30342.805 ],\n",
      "       [ 489362.22  ,  175056.22  ,  -50279.03  , -430529.5   ]],\n",
      "      dtype=float32)>, <tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 26449.084 ,   9461.441 ,  -2717.4875, -23269.275 ], dtype=float32)>, <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\n",
      "array([[-493309.28],\n",
      "       [-384075.34],\n",
      "       [ -35134.67],\n",
      "       [ 157047.64]], dtype=float32)>, <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-26767.795], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "#Custom training\n",
    "#x=[1,2,3,4,5,6,7,8,9,10]\n",
    "#x=np.asarray(x,dtype=np.float32).reshape((10,1))\n",
    "#y=[1,4,9,16,25,36,49,64,81,100]\n",
    "#y=np.asarray(y,dtype=np.float32).reshape((10,1))\n",
    "\n",
    "m=model()\n",
    "\n",
    "for i in range(2):\n",
    "    m.network_learn(X_train, y_train)\n",
    "\n",
    "    \n",
    "# Test Case\n",
    "#x=[11]\n",
    "#x=np.asarray(x,dtype=np.float32).reshape((1,1))\n",
    "#print(model.run(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(8, 4), dtype=float32, numpy=\n",
       " array([[ 273064.06  ,   93991.55  ,  -39466.55  , -249433.67  ],\n",
       "        [ -58423.203 ,  -20109.889 ,    8444.035 ,   53367.38  ],\n",
       "        [ -26712.32  ,   -9194.664 ,    3860.791 ,   24400.691 ],\n",
       "        [ -97421.58  ,  -33533.54  ,   14080.555 ,   88990.92  ],\n",
       "        [   5197.2783,    1788.9595,    -751.1743,   -4747.508 ],\n",
       "        [ 124758.64  ,   42943.246 ,  -18031.639 , -113962.266 ],\n",
       "        [ -14599.819 ,   -5025.41  ,    2110.1448,   13336.376 ],\n",
       "        [ 367553.97  ,  126515.984 ,  -53123.39  , -335746.62  ]],\n",
       "       dtype=float32)>,\n",
       " <tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 7214.6   ,  2483.3396, -1042.7422, -6590.259 ], dtype=float32)>,\n",
       " <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\n",
       " array([[-236536.97 ],\n",
       "        [-210715.67 ],\n",
       "        [   9073.932],\n",
       "        [ 198444.3  ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-7749.5654], dtype=float32)>]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = m.get_grad(X_train, y_train)\n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: shape=(8, 4), dtype=float32, numpy=\n",
      "array([[-0.04359787, -0.01500685,  0.0063013 ,  0.039825  ],\n",
      "       [ 0.84070694,  0.28938025, -0.12150922, -0.7679539 ],\n",
      "       [-0.5241901 , -0.18043183,  0.07576235,  0.4788278 ],\n",
      "       [ 1.1980461 ,  0.41238022, -0.17315625, -1.0943696 ],\n",
      "       [ 0.64568156,  0.22225046, -0.09332178, -0.5898056 ],\n",
      "       [-1.2055811 , -0.41497383,  0.17424528,  1.1012526 ],\n",
      "       [ 0.65948224,  0.2270008 , -0.09531642, -0.60241205],\n",
      "       [ 0.33443257,  0.11511525, -0.04833627, -0.30549145]],\n",
      "      dtype=float32)>, <tf.Tensor: shape=(4,), dtype=float32, numpy=array([-0.93096817, -0.32044914,  0.13455488,  0.8504041 ], dtype=float32)>, <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\n",
      "array([[-0.8798704],\n",
      "       [-1.5463147],\n",
      "       [-0.1866839],\n",
      "       [-2.1644511]], dtype=float32)>, <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>]\n",
      "================================================================================\n",
      "[<tf.Tensor: shape=(8, 4), dtype=float32, numpy=\n",
      "array([[-0.87267804, -0.30038506,  0.12613007,  0.7971583 ],\n",
      "       [-1.1110929 , -0.38244998,  0.1605887 ,  1.0149412 ],\n",
      "       [ 0.24538295,  0.08446342, -0.03546574, -0.224148  ],\n",
      "       [ 1.1980461 ,  0.41238022, -0.17315625, -1.0943696 ],\n",
      "       [ 0.64568156,  0.22225046, -0.09332178, -0.5898056 ],\n",
      "       [ 0.5420627 ,  0.18658374, -0.07834551, -0.4951537 ],\n",
      "       [ 0.4993006 ,  0.17186457, -0.07216501, -0.45609215],\n",
      "       [-0.53751045, -0.18501683,  0.07768757,  0.4909954 ]],\n",
      "      dtype=float32)>, <tf.Tensor: shape=(4,), dtype=float32, numpy=array([-0.93096817, -0.32044914,  0.13455488,  0.8504041 ], dtype=float32)>, <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\n",
      "array([[-0.7263459 ],\n",
      "       [ 1.8670055 ],\n",
      "       [ 0.3546153 ],\n",
      "       [ 0.12431362]], dtype=float32)>, <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>]\n",
      "[<tf.Tensor: shape=(8, 4), dtype=float32, numpy=\n",
      "array([[-0.9162759 , -0.3153919 ,  0.13243137,  0.8369833 ],\n",
      "       [-0.27038598, -0.09306973,  0.03907947,  0.24698734],\n",
      "       [-0.27880716, -0.09596841,  0.04029662,  0.2546798 ],\n",
      "       [ 2.3960922 ,  0.82476044, -0.3463125 , -2.1887393 ],\n",
      "       [ 1.2913631 ,  0.44450092, -0.18664356, -1.1796112 ],\n",
      "       [-0.66351837, -0.22839008,  0.09589977,  0.6060989 ],\n",
      "       [ 1.1587828 ,  0.39886537, -0.16748144, -1.0585042 ],\n",
      "       [-0.20307788, -0.06990158,  0.02935129,  0.18550396]],\n",
      "      dtype=float32)>, <tf.Tensor: shape=(4,), dtype=float32, numpy=array([-1.8619363 , -0.6408983 ,  0.26910976,  1.7008082 ], dtype=float32)>, <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\n",
      "array([[-1.6062164 ],\n",
      "       [ 0.320691  ],\n",
      "       [ 0.16793141],\n",
      "       [-2.0401378 ]], dtype=float32)>, <tf.Tensor: shape=(1,), dtype=float32, numpy=array([2.], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "print(m.get_grad2(X_train[0:1,:]))\n",
    "print(\"=\" * 80)\n",
    "print(m.get_grad2(X_train[1:2,:]))\n",
    "print(m.get_grad2(X_train[0:2,:]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(8, 4), dtype=float32, numpy=\n",
       " array([[-3.573741  ,  4.4614577 , -2.0754642 ,  1.1735111 ],\n",
       "        [-3.9608998 ,  4.9447885 , -2.3003097 ,  1.3006427 ],\n",
       "        [-0.6210106 ,  0.77526915, -0.36065477,  0.20392124],\n",
       "        [ 3.7513583 , -4.6831937 ,  2.1786158 , -1.2318348 ],\n",
       "        [-0.7751977 ,  0.9677548 , -0.4501989 ,  0.25455114],\n",
       "        [ 3.4337068 , -4.286639  ,  1.9941385 , -1.1275274 ],\n",
       "        [-2.8637142 ,  3.5750604 , -1.6631137 ,  0.9403584 ],\n",
       "        [-4.6870346 ,  5.851292  , -2.7220147 ,  1.5390848 ]],\n",
       "       dtype=float32)>,\n",
       " <tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 65.345055, -81.576866,  37.949524, -21.457415], dtype=float32)>,\n",
       " <tf.Tensor: shape=(4, 2), dtype=float32, numpy=\n",
       " array([[ 15.225409 ,   7.9595876],\n",
       "        [-18.307314 ,  -9.570757 ],\n",
       "        [ 14.491468 ,   7.5758953],\n",
       "        [-16.125036 ,  -8.429895 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(2,), dtype=float32, numpy=array([138.7106 ,  72.51534], dtype=float32)>,\n",
       " <tf.Tensor: shape=(2, 1), dtype=float32, numpy=\n",
       " array([[74.71012 ],\n",
       "        [12.841896]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1,), dtype=float32, numpy=array([231.], dtype=float32)>]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g2_test = m.get_grad2(X_test)\n",
    "g2_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.24188049]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(m.run(X_test[0:1,]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.ones((2, 2))\n",
    "\n",
    "with tf.GradientTape() as t:\n",
    "    t.watch(x)\n",
    "    y = tf.reduce_sum(x)\n",
    "    z = tf.multiply(y, y)\n",
    "\n",
    "# Derivative of z with respect to the original input tensor x\n",
    "dz_dx = t.gradient(z, x)\n",
    "for i in [0, 1]:\n",
    "    for j in [0, 1]:\n",
    "        assert dz_dx[i][j].numpy() == 8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[1., 1.],\n",
       "       [1., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10 samples\n",
      "Epoch 1/1000\n",
      "10/10 [==============================] - 0s 34ms/sample - loss: 2660.8171\n",
      "Epoch 2/1000\n",
      "10/10 [==============================] - 0s 124us/sample - loss: 2635.6772\n",
      "Epoch 3/1000\n",
      "10/10 [==============================] - 0s 198us/sample - loss: 2618.0007\n",
      "Epoch 4/1000\n",
      "10/10 [==============================] - 0s 145us/sample - loss: 2603.4373\n",
      "Epoch 5/1000\n",
      "10/10 [==============================] - 0s 184us/sample - loss: 2590.6538\n",
      "Epoch 6/1000\n",
      "10/10 [==============================] - 0s 191us/sample - loss: 2579.0430\n",
      "Epoch 7/1000\n",
      "10/10 [==============================] - 0s 206us/sample - loss: 2567.2781\n",
      "Epoch 8/1000\n",
      "10/10 [==============================] - 0s 234us/sample - loss: 2553.9441\n",
      "Epoch 9/1000\n",
      "10/10 [==============================] - 0s 222us/sample - loss: 2541.9351\n",
      "Epoch 10/1000\n",
      "10/10 [==============================] - 0s 260us/sample - loss: 2530.7222\n",
      "Epoch 11/1000\n",
      "10/10 [==============================] - 0s 259us/sample - loss: 2520.2920\n",
      "Epoch 12/1000\n",
      "10/10 [==============================] - 0s 207us/sample - loss: 2510.5376\n",
      "Epoch 13/1000\n",
      "10/10 [==============================] - 0s 254us/sample - loss: 2501.8210\n",
      "Epoch 14/1000\n",
      "10/10 [==============================] - 0s 222us/sample - loss: 2493.4617\n",
      "Epoch 15/1000\n",
      "10/10 [==============================] - 0s 292us/sample - loss: 2485.4863\n",
      "Epoch 16/1000\n",
      "10/10 [==============================] - 0s 223us/sample - loss: 2478.0571\n",
      "Epoch 17/1000\n",
      "10/10 [==============================] - 0s 420us/sample - loss: 2470.5425\n",
      "Epoch 18/1000\n",
      "10/10 [==============================] - 0s 341us/sample - loss: 2463.2515\n",
      "Epoch 19/1000\n",
      "10/10 [==============================] - 0s 409us/sample - loss: 2455.9800\n",
      "Epoch 20/1000\n",
      "10/10 [==============================] - 0s 339us/sample - loss: 2448.8992\n",
      "Epoch 21/1000\n",
      "10/10 [==============================] - 0s 241us/sample - loss: 2441.5793\n",
      "Epoch 22/1000\n",
      "10/10 [==============================] - 0s 337us/sample - loss: 2434.7478\n",
      "Epoch 23/1000\n",
      "10/10 [==============================] - 0s 291us/sample - loss: 2427.5203\n",
      "Epoch 24/1000\n",
      "10/10 [==============================] - 0s 344us/sample - loss: 2420.4634\n",
      "Epoch 25/1000\n",
      "10/10 [==============================] - 0s 371us/sample - loss: 2413.5378\n",
      "Epoch 26/1000\n",
      "10/10 [==============================] - 0s 250us/sample - loss: 2406.3921\n",
      "Epoch 27/1000\n",
      "10/10 [==============================] - 0s 265us/sample - loss: 2399.4116\n",
      "Epoch 28/1000\n",
      "10/10 [==============================] - 0s 294us/sample - loss: 2392.4961\n",
      "Epoch 29/1000\n",
      "10/10 [==============================] - 0s 425us/sample - loss: 2385.3708\n",
      "Epoch 30/1000\n",
      "10/10 [==============================] - 0s 449us/sample - loss: 2378.2812\n",
      "Epoch 31/1000\n",
      "10/10 [==============================] - 0s 335us/sample - loss: 2371.2844\n",
      "Epoch 32/1000\n",
      "10/10 [==============================] - 0s 281us/sample - loss: 2364.2437\n",
      "Epoch 33/1000\n",
      "10/10 [==============================] - 0s 309us/sample - loss: 2357.3350\n",
      "Epoch 34/1000\n",
      "10/10 [==============================] - 0s 328us/sample - loss: 2350.1899\n",
      "Epoch 35/1000\n",
      "10/10 [==============================] - 0s 427us/sample - loss: 2343.0503\n",
      "Epoch 36/1000\n",
      "10/10 [==============================] - 0s 457us/sample - loss: 2335.9272\n",
      "Epoch 37/1000\n",
      "10/10 [==============================] - 0s 397us/sample - loss: 2328.8159\n",
      "Epoch 38/1000\n",
      "10/10 [==============================] - 0s 287us/sample - loss: 2321.7014\n",
      "Epoch 39/1000\n",
      "10/10 [==============================] - 0s 323us/sample - loss: 2314.6699\n",
      "Epoch 40/1000\n",
      "10/10 [==============================] - 0s 254us/sample - loss: 2307.4292\n",
      "Epoch 41/1000\n",
      "10/10 [==============================] - 0s 555us/sample - loss: 2300.1731\n",
      "Epoch 42/1000\n",
      "10/10 [==============================] - 0s 328us/sample - loss: 2292.9043\n",
      "Epoch 43/1000\n",
      "10/10 [==============================] - 0s 266us/sample - loss: 2285.6282\n",
      "Epoch 44/1000\n",
      "10/10 [==============================] - 0s 247us/sample - loss: 2278.3411\n",
      "Epoch 45/1000\n",
      "10/10 [==============================] - 0s 373us/sample - loss: 2271.0396\n",
      "Epoch 46/1000\n",
      "10/10 [==============================] - 0s 305us/sample - loss: 2263.7690\n",
      "Epoch 47/1000\n",
      "10/10 [==============================] - 0s 382us/sample - loss: 2256.5911\n",
      "Epoch 48/1000\n",
      "10/10 [==============================] - 0s 376us/sample - loss: 2249.5881\n",
      "Epoch 49/1000\n",
      "10/10 [==============================] - 0s 285us/sample - loss: 2242.6067\n",
      "Epoch 50/1000\n",
      "10/10 [==============================] - 0s 377us/sample - loss: 2235.6477\n",
      "Epoch 51/1000\n",
      "10/10 [==============================] - 0s 409us/sample - loss: 2228.7598\n",
      "Epoch 52/1000\n",
      "10/10 [==============================] - 0s 361us/sample - loss: 2222.0164\n",
      "Epoch 53/1000\n",
      "10/10 [==============================] - 0s 333us/sample - loss: 2215.5305\n",
      "Epoch 54/1000\n",
      "10/10 [==============================] - 0s 434us/sample - loss: 2209.1577\n",
      "Epoch 55/1000\n",
      "10/10 [==============================] - 0s 286us/sample - loss: 2202.7261\n",
      "Epoch 56/1000\n",
      "10/10 [==============================] - 0s 341us/sample - loss: 2196.2329\n",
      "Epoch 57/1000\n",
      "10/10 [==============================] - 0s 416us/sample - loss: 2189.6763\n",
      "Epoch 58/1000\n",
      "10/10 [==============================] - 0s 402us/sample - loss: 2183.0552\n",
      "Epoch 59/1000\n",
      "10/10 [==============================] - 0s 397us/sample - loss: 2176.3694\n",
      "Epoch 60/1000\n",
      "10/10 [==============================] - 0s 302us/sample - loss: 2169.6172\n",
      "Epoch 61/1000\n",
      "10/10 [==============================] - 0s 361us/sample - loss: 2162.7981\n",
      "Epoch 62/1000\n",
      "10/10 [==============================] - 0s 297us/sample - loss: 2155.9126\n",
      "Epoch 63/1000\n",
      "10/10 [==============================] - 0s 308us/sample - loss: 2148.9600\n",
      "Epoch 64/1000\n",
      "10/10 [==============================] - 0s 424us/sample - loss: 2141.9446\n",
      "Epoch 65/1000\n",
      "10/10 [==============================] - 0s 355us/sample - loss: 2134.8745\n",
      "Epoch 66/1000\n",
      "10/10 [==============================] - 0s 356us/sample - loss: 2127.7847\n",
      "Epoch 67/1000\n",
      "10/10 [==============================] - 0s 426us/sample - loss: 2120.6880\n",
      "Epoch 68/1000\n",
      "10/10 [==============================] - 0s 303us/sample - loss: 2113.5339\n",
      "Epoch 69/1000\n",
      "10/10 [==============================] - 0s 363us/sample - loss: 2106.3164\n",
      "Epoch 70/1000\n",
      "10/10 [==============================] - 0s 338us/sample - loss: 2099.0276\n",
      "Epoch 71/1000\n",
      "10/10 [==============================] - 0s 463us/sample - loss: 2091.6738\n",
      "Epoch 72/1000\n",
      "10/10 [==============================] - 0s 446us/sample - loss: 2084.2524\n",
      "Epoch 73/1000\n",
      "10/10 [==============================] - 0s 388us/sample - loss: 2076.7678\n",
      "Epoch 74/1000\n",
      "10/10 [==============================] - 0s 346us/sample - loss: 2069.2190\n",
      "Epoch 75/1000\n",
      "10/10 [==============================] - 0s 486us/sample - loss: 2061.6099\n",
      "Epoch 76/1000\n",
      "10/10 [==============================] - 0s 243us/sample - loss: 2053.9395\n",
      "Epoch 77/1000\n",
      "10/10 [==============================] - 0s 239us/sample - loss: 2046.2057\n",
      "Epoch 78/1000\n",
      "10/10 [==============================] - 0s 291us/sample - loss: 2038.4059\n",
      "Epoch 79/1000\n",
      "10/10 [==============================] - 0s 180us/sample - loss: 2030.5397\n",
      "Epoch 80/1000\n",
      "10/10 [==============================] - 0s 185us/sample - loss: 2022.6084\n",
      "Epoch 81/1000\n",
      "10/10 [==============================] - 0s 208us/sample - loss: 2014.6130\n",
      "Epoch 82/1000\n",
      "10/10 [==============================] - 0s 191us/sample - loss: 2006.5554\n",
      "Epoch 83/1000\n",
      "10/10 [==============================] - 0s 278us/sample - loss: 1998.4379\n",
      "Epoch 84/1000\n",
      "10/10 [==============================] - 0s 268us/sample - loss: 1990.2629\n",
      "Epoch 85/1000\n",
      "10/10 [==============================] - 0s 183us/sample - loss: 1982.0258\n",
      "Epoch 86/1000\n",
      "10/10 [==============================] - 0s 220us/sample - loss: 1973.7263\n",
      "Epoch 87/1000\n",
      "10/10 [==============================] - 0s 183us/sample - loss: 1965.3668\n",
      "Epoch 88/1000\n",
      "10/10 [==============================] - 0s 181us/sample - loss: 1956.9447\n",
      "Epoch 89/1000\n",
      "10/10 [==============================] - 0s 173us/sample - loss: 1948.4623\n",
      "Epoch 90/1000\n",
      "10/10 [==============================] - 0s 178us/sample - loss: 1939.9205\n",
      "Epoch 91/1000\n",
      "10/10 [==============================] - 0s 199us/sample - loss: 1931.3193\n",
      "Epoch 92/1000\n",
      "10/10 [==============================] - 0s 233us/sample - loss: 1922.6586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93/1000\n",
      "10/10 [==============================] - 0s 210us/sample - loss: 1913.9395\n",
      "Epoch 94/1000\n",
      "10/10 [==============================] - 0s 265us/sample - loss: 1905.1631\n",
      "Epoch 95/1000\n",
      "10/10 [==============================] - 0s 252us/sample - loss: 1896.3289\n",
      "Epoch 96/1000\n",
      "10/10 [==============================] - 0s 275us/sample - loss: 1887.4375\n",
      "Epoch 97/1000\n",
      "10/10 [==============================] - 0s 274us/sample - loss: 1878.4893\n",
      "Epoch 98/1000\n",
      "10/10 [==============================] - 0s 195us/sample - loss: 1869.4854\n",
      "Epoch 99/1000\n",
      "10/10 [==============================] - 0s 298us/sample - loss: 1860.4258\n",
      "Epoch 100/1000\n",
      "10/10 [==============================] - 0s 444us/sample - loss: 1851.3113\n",
      "Epoch 101/1000\n",
      "10/10 [==============================] - 0s 193us/sample - loss: 1842.1420\n",
      "Epoch 102/1000\n",
      "10/10 [==============================] - 0s 195us/sample - loss: 1832.9183\n",
      "Epoch 103/1000\n",
      "10/10 [==============================] - 0s 284us/sample - loss: 1823.6414\n",
      "Epoch 104/1000\n",
      "10/10 [==============================] - 0s 258us/sample - loss: 1814.3109\n",
      "Epoch 105/1000\n",
      "10/10 [==============================] - 0s 205us/sample - loss: 1804.9277\n",
      "Epoch 106/1000\n",
      "10/10 [==============================] - 0s 177us/sample - loss: 1795.4924\n",
      "Epoch 107/1000\n",
      "10/10 [==============================] - 0s 210us/sample - loss: 1786.0062\n",
      "Epoch 108/1000\n",
      "10/10 [==============================] - 0s 216us/sample - loss: 1776.4678\n",
      "Epoch 109/1000\n",
      "10/10 [==============================] - 0s 451us/sample - loss: 1766.8789\n",
      "Epoch 110/1000\n",
      "10/10 [==============================] - 0s 189us/sample - loss: 1757.2400\n",
      "Epoch 111/1000\n",
      "10/10 [==============================] - 0s 198us/sample - loss: 1747.5518\n",
      "Epoch 112/1000\n",
      "10/10 [==============================] - 0s 213us/sample - loss: 1737.8141\n",
      "Epoch 113/1000\n",
      "10/10 [==============================] - 0s 406us/sample - loss: 1728.0280\n",
      "Epoch 114/1000\n",
      "10/10 [==============================] - 0s 213us/sample - loss: 1718.1934\n",
      "Epoch 115/1000\n",
      "10/10 [==============================] - 0s 206us/sample - loss: 1708.3118\n",
      "Epoch 116/1000\n",
      "10/10 [==============================] - 0s 220us/sample - loss: 1698.3834\n",
      "Epoch 117/1000\n",
      "10/10 [==============================] - 0s 332us/sample - loss: 1688.4089\n",
      "Epoch 118/1000\n",
      "10/10 [==============================] - 0s 254us/sample - loss: 1678.3889\n",
      "Epoch 119/1000\n",
      "10/10 [==============================] - 0s 204us/sample - loss: 1668.3235\n",
      "Epoch 120/1000\n",
      "10/10 [==============================] - 0s 206us/sample - loss: 1658.2133\n",
      "Epoch 121/1000\n",
      "10/10 [==============================] - 0s 331us/sample - loss: 1648.0593\n",
      "Epoch 122/1000\n",
      "10/10 [==============================] - 0s 322us/sample - loss: 1637.8621\n",
      "Epoch 123/1000\n",
      "10/10 [==============================] - 0s 258us/sample - loss: 1627.6221\n",
      "Epoch 124/1000\n",
      "10/10 [==============================] - 0s 326us/sample - loss: 1617.3403\n",
      "Epoch 125/1000\n",
      "10/10 [==============================] - 0s 269us/sample - loss: 1607.0173\n",
      "Epoch 126/1000\n",
      "10/10 [==============================] - 0s 313us/sample - loss: 1596.6536\n",
      "Epoch 127/1000\n",
      "10/10 [==============================] - 0s 320us/sample - loss: 1586.2513\n",
      "Epoch 128/1000\n",
      "10/10 [==============================] - 0s 314us/sample - loss: 1575.8093\n",
      "Epoch 129/1000\n",
      "10/10 [==============================] - 0s 295us/sample - loss: 1565.3284\n",
      "Epoch 130/1000\n",
      "10/10 [==============================] - 0s 250us/sample - loss: 1554.8112\n",
      "Epoch 131/1000\n",
      "10/10 [==============================] - 0s 311us/sample - loss: 1544.2566\n",
      "Epoch 132/1000\n",
      "10/10 [==============================] - 0s 279us/sample - loss: 1533.6650\n",
      "Epoch 133/1000\n",
      "10/10 [==============================] - 0s 310us/sample - loss: 1523.0388\n",
      "Epoch 134/1000\n",
      "10/10 [==============================] - 0s 352us/sample - loss: 1512.3778\n",
      "Epoch 135/1000\n",
      "10/10 [==============================] - 0s 305us/sample - loss: 1501.6824\n",
      "Epoch 136/1000\n",
      "10/10 [==============================] - 0s 314us/sample - loss: 1490.9534\n",
      "Epoch 137/1000\n",
      "10/10 [==============================] - 0s 371us/sample - loss: 1480.1918\n",
      "Epoch 138/1000\n",
      "10/10 [==============================] - 0s 236us/sample - loss: 1469.3984\n",
      "Epoch 139/1000\n",
      "10/10 [==============================] - 0s 241us/sample - loss: 1458.5740\n",
      "Epoch 140/1000\n",
      "10/10 [==============================] - 0s 201us/sample - loss: 1447.7197\n",
      "Epoch 141/1000\n",
      "10/10 [==============================] - 0s 313us/sample - loss: 1436.8359\n",
      "Epoch 142/1000\n",
      "10/10 [==============================] - 0s 315us/sample - loss: 1425.9241\n",
      "Epoch 143/1000\n",
      "10/10 [==============================] - 0s 229us/sample - loss: 1414.9846\n",
      "Epoch 144/1000\n",
      "10/10 [==============================] - 0s 460us/sample - loss: 1404.0189\n",
      "Epoch 145/1000\n",
      "10/10 [==============================] - 0s 265us/sample - loss: 1393.0277\n",
      "Epoch 146/1000\n",
      "10/10 [==============================] - 0s 227us/sample - loss: 1382.0120\n",
      "Epoch 147/1000\n",
      "10/10 [==============================] - 0s 459us/sample - loss: 1370.9724\n",
      "Epoch 148/1000\n",
      "10/10 [==============================] - 0s 380us/sample - loss: 1359.9105\n",
      "Epoch 149/1000\n",
      "10/10 [==============================] - 0s 270us/sample - loss: 1348.8269\n",
      "Epoch 150/1000\n",
      "10/10 [==============================] - 0s 307us/sample - loss: 1337.7227\n",
      "Epoch 151/1000\n",
      "10/10 [==============================] - 0s 286us/sample - loss: 1326.5989\n",
      "Epoch 152/1000\n",
      "10/10 [==============================] - 0s 268us/sample - loss: 1315.4565\n",
      "Epoch 153/1000\n",
      "10/10 [==============================] - 0s 266us/sample - loss: 1304.2968\n",
      "Epoch 154/1000\n",
      "10/10 [==============================] - 0s 304us/sample - loss: 1293.1202\n",
      "Epoch 155/1000\n",
      "10/10 [==============================] - 0s 348us/sample - loss: 1281.9285\n",
      "Epoch 156/1000\n",
      "10/10 [==============================] - 0s 202us/sample - loss: 1270.7225\n",
      "Epoch 157/1000\n",
      "10/10 [==============================] - 0s 316us/sample - loss: 1259.5033\n",
      "Epoch 158/1000\n",
      "10/10 [==============================] - 0s 228us/sample - loss: 1248.2721\n",
      "Epoch 159/1000\n",
      "10/10 [==============================] - 0s 305us/sample - loss: 1237.0339\n",
      "Epoch 160/1000\n",
      "10/10 [==============================] - 0s 395us/sample - loss: 1225.7821\n",
      "Epoch 161/1000\n",
      "10/10 [==============================] - 0s 332us/sample - loss: 1214.5210\n",
      "Epoch 162/1000\n",
      "10/10 [==============================] - 0s 223us/sample - loss: 1203.2523\n",
      "Epoch 163/1000\n",
      "10/10 [==============================] - 0s 227us/sample - loss: 1191.9773\n",
      "Epoch 164/1000\n",
      "10/10 [==============================] - 0s 319us/sample - loss: 1180.6971\n",
      "Epoch 165/1000\n",
      "10/10 [==============================] - 0s 202us/sample - loss: 1169.4130\n",
      "Epoch 166/1000\n",
      "10/10 [==============================] - 0s 220us/sample - loss: 1158.1262\n",
      "Epoch 167/1000\n",
      "10/10 [==============================] - 0s 294us/sample - loss: 1146.8381\n",
      "Epoch 168/1000\n",
      "10/10 [==============================] - 0s 367us/sample - loss: 1135.5496\n",
      "Epoch 169/1000\n",
      "10/10 [==============================] - 0s 192us/sample - loss: 1124.2618\n",
      "Epoch 170/1000\n",
      "10/10 [==============================] - 0s 198us/sample - loss: 1112.9767\n",
      "Epoch 171/1000\n",
      "10/10 [==============================] - 0s 234us/sample - loss: 1101.6949\n",
      "Epoch 172/1000\n",
      "10/10 [==============================] - 0s 713us/sample - loss: 1090.4180\n",
      "Epoch 173/1000\n",
      "10/10 [==============================] - 0s 330us/sample - loss: 1079.1471\n",
      "Epoch 174/1000\n",
      "10/10 [==============================] - 0s 410us/sample - loss: 1067.8838\n",
      "Epoch 175/1000\n",
      "10/10 [==============================] - 0s 214us/sample - loss: 1056.6292\n",
      "Epoch 176/1000\n",
      "10/10 [==============================] - 0s 208us/sample - loss: 1045.3848\n",
      "Epoch 177/1000\n",
      "10/10 [==============================] - 0s 219us/sample - loss: 1034.1516\n",
      "Epoch 178/1000\n",
      "10/10 [==============================] - 0s 277us/sample - loss: 1022.9315\n",
      "Epoch 179/1000\n",
      "10/10 [==============================] - 0s 220us/sample - loss: 1011.7254\n",
      "Epoch 180/1000\n",
      "10/10 [==============================] - 0s 344us/sample - loss: 1000.5350\n",
      "Epoch 181/1000\n",
      "10/10 [==============================] - 0s 185us/sample - loss: 989.3617\n",
      "Epoch 182/1000\n",
      "10/10 [==============================] - 0s 193us/sample - loss: 978.2067\n",
      "Epoch 183/1000\n",
      "10/10 [==============================] - 0s 396us/sample - loss: 967.0717\n",
      "Epoch 184/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 357us/sample - loss: 955.9576\n",
      "Epoch 185/1000\n",
      "10/10 [==============================] - 0s 325us/sample - loss: 944.8666\n",
      "Epoch 186/1000\n",
      "10/10 [==============================] - 0s 240us/sample - loss: 933.7998\n",
      "Epoch 187/1000\n",
      "10/10 [==============================] - 0s 243us/sample - loss: 922.7587\n",
      "Epoch 188/1000\n",
      "10/10 [==============================] - 0s 222us/sample - loss: 911.7446\n",
      "Epoch 189/1000\n",
      "10/10 [==============================] - 0s 315us/sample - loss: 900.7595\n",
      "Epoch 190/1000\n",
      "10/10 [==============================] - 0s 186us/sample - loss: 889.8043\n",
      "Epoch 191/1000\n",
      "10/10 [==============================] - 0s 182us/sample - loss: 878.8809\n",
      "Epoch 192/1000\n",
      "10/10 [==============================] - 0s 268us/sample - loss: 867.9908\n",
      "Epoch 193/1000\n",
      "10/10 [==============================] - 0s 202us/sample - loss: 857.1354\n",
      "Epoch 194/1000\n",
      "10/10 [==============================] - 0s 242us/sample - loss: 846.3165\n",
      "Epoch 195/1000\n",
      "10/10 [==============================] - 0s 291us/sample - loss: 835.5355\n",
      "Epoch 196/1000\n",
      "10/10 [==============================] - 0s 364us/sample - loss: 824.7938\n",
      "Epoch 197/1000\n",
      "10/10 [==============================] - 0s 216us/sample - loss: 814.0933\n",
      "Epoch 198/1000\n",
      "10/10 [==============================] - 0s 348us/sample - loss: 803.4357\n",
      "Epoch 199/1000\n",
      "10/10 [==============================] - 0s 259us/sample - loss: 792.8221\n",
      "Epoch 200/1000\n",
      "10/10 [==============================] - 0s 217us/sample - loss: 782.2546\n",
      "Epoch 201/1000\n",
      "10/10 [==============================] - 0s 249us/sample - loss: 771.7345\n",
      "Epoch 202/1000\n",
      "10/10 [==============================] - 0s 199us/sample - loss: 761.2638\n",
      "Epoch 203/1000\n",
      "10/10 [==============================] - 0s 221us/sample - loss: 750.8436\n",
      "Epoch 204/1000\n",
      "10/10 [==============================] - 0s 208us/sample - loss: 740.4762\n",
      "Epoch 205/1000\n",
      "10/10 [==============================] - 0s 210us/sample - loss: 730.1628\n",
      "Epoch 206/1000\n",
      "10/10 [==============================] - 0s 329us/sample - loss: 719.9052\n",
      "Epoch 207/1000\n",
      "10/10 [==============================] - 0s 212us/sample - loss: 709.7052\n",
      "Epoch 208/1000\n",
      "10/10 [==============================] - 0s 235us/sample - loss: 699.5642\n",
      "Epoch 209/1000\n",
      "10/10 [==============================] - 0s 247us/sample - loss: 689.4843\n",
      "Epoch 210/1000\n",
      "10/10 [==============================] - 0s 216us/sample - loss: 679.4669\n",
      "Epoch 211/1000\n",
      "10/10 [==============================] - 0s 187us/sample - loss: 669.5137\n",
      "Epoch 212/1000\n",
      "10/10 [==============================] - 0s 199us/sample - loss: 659.6266\n",
      "Epoch 213/1000\n",
      "10/10 [==============================] - 0s 228us/sample - loss: 649.8073\n",
      "Epoch 214/1000\n",
      "10/10 [==============================] - 0s 379us/sample - loss: 640.0573\n",
      "Epoch 215/1000\n",
      "10/10 [==============================] - 0s 203us/sample - loss: 630.3785\n",
      "Epoch 216/1000\n",
      "10/10 [==============================] - 0s 175us/sample - loss: 620.7726\n",
      "Epoch 217/1000\n",
      "10/10 [==============================] - 0s 207us/sample - loss: 611.2413\n",
      "Epoch 218/1000\n",
      "10/10 [==============================] - 0s 286us/sample - loss: 601.7865\n",
      "Epoch 219/1000\n",
      "10/10 [==============================] - 0s 191us/sample - loss: 592.4097\n",
      "Epoch 220/1000\n",
      "10/10 [==============================] - 0s 191us/sample - loss: 583.1128\n",
      "Epoch 221/1000\n",
      "10/10 [==============================] - 0s 191us/sample - loss: 573.8976\n",
      "Epoch 222/1000\n",
      "10/10 [==============================] - 0s 231us/sample - loss: 564.7656\n",
      "Epoch 223/1000\n",
      "10/10 [==============================] - 0s 289us/sample - loss: 555.7188\n",
      "Epoch 224/1000\n",
      "10/10 [==============================] - 0s 283us/sample - loss: 546.7588\n",
      "Epoch 225/1000\n",
      "10/10 [==============================] - 0s 302us/sample - loss: 537.8875\n",
      "Epoch 226/1000\n",
      "10/10 [==============================] - 0s 323us/sample - loss: 529.1064\n",
      "Epoch 227/1000\n",
      "10/10 [==============================] - 0s 235us/sample - loss: 520.4174\n",
      "Epoch 228/1000\n",
      "10/10 [==============================] - 0s 207us/sample - loss: 511.8222\n",
      "Epoch 229/1000\n",
      "10/10 [==============================] - 0s 171us/sample - loss: 503.3226\n",
      "Epoch 230/1000\n",
      "10/10 [==============================] - 0s 215us/sample - loss: 494.9202\n",
      "Epoch 231/1000\n",
      "10/10 [==============================] - 0s 212us/sample - loss: 486.6166\n",
      "Epoch 232/1000\n",
      "10/10 [==============================] - 0s 394us/sample - loss: 478.4140\n",
      "Epoch 233/1000\n",
      "10/10 [==============================] - 0s 321us/sample - loss: 470.3136\n",
      "Epoch 234/1000\n",
      "10/10 [==============================] - 0s 235us/sample - loss: 462.3172\n",
      "Epoch 235/1000\n",
      "10/10 [==============================] - 0s 375us/sample - loss: 454.4267\n",
      "Epoch 236/1000\n",
      "10/10 [==============================] - 0s 374us/sample - loss: 446.6433\n",
      "Epoch 237/1000\n",
      "10/10 [==============================] - 0s 262us/sample - loss: 438.9692\n",
      "Epoch 238/1000\n",
      "10/10 [==============================] - 0s 228us/sample - loss: 431.4056\n",
      "Epoch 239/1000\n",
      "10/10 [==============================] - 0s 202us/sample - loss: 423.9544\n",
      "Epoch 240/1000\n",
      "10/10 [==============================] - 0s 391us/sample - loss: 416.6171\n",
      "Epoch 241/1000\n",
      "10/10 [==============================] - 0s 233us/sample - loss: 409.3951\n",
      "Epoch 242/1000\n",
      "10/10 [==============================] - 0s 281us/sample - loss: 402.2901\n",
      "Epoch 243/1000\n",
      "10/10 [==============================] - 0s 311us/sample - loss: 395.3036\n",
      "Epoch 244/1000\n",
      "10/10 [==============================] - 0s 249us/sample - loss: 388.4371\n",
      "Epoch 245/1000\n",
      "10/10 [==============================] - 0s 292us/sample - loss: 381.6922\n",
      "Epoch 246/1000\n",
      "10/10 [==============================] - 0s 221us/sample - loss: 375.0700\n",
      "Epoch 247/1000\n",
      "10/10 [==============================] - 0s 399us/sample - loss: 368.5721\n",
      "Epoch 248/1000\n",
      "10/10 [==============================] - 0s 232us/sample - loss: 362.2000\n",
      "Epoch 249/1000\n",
      "10/10 [==============================] - 0s 234us/sample - loss: 355.9546\n",
      "Epoch 250/1000\n",
      "10/10 [==============================] - 0s 270us/sample - loss: 349.8375\n",
      "Epoch 251/1000\n",
      "10/10 [==============================] - 0s 323us/sample - loss: 343.8500\n",
      "Epoch 252/1000\n",
      "10/10 [==============================] - 0s 215us/sample - loss: 337.9930\n",
      "Epoch 253/1000\n",
      "10/10 [==============================] - 0s 284us/sample - loss: 332.2678\n",
      "Epoch 254/1000\n",
      "10/10 [==============================] - 0s 228us/sample - loss: 326.6752\n",
      "Epoch 255/1000\n",
      "10/10 [==============================] - 0s 255us/sample - loss: 321.2164\n",
      "Epoch 256/1000\n",
      "10/10 [==============================] - 0s 259us/sample - loss: 315.8924\n",
      "Epoch 257/1000\n",
      "10/10 [==============================] - 0s 389us/sample - loss: 310.7039\n",
      "Epoch 258/1000\n",
      "10/10 [==============================] - 0s 230us/sample - loss: 305.6516\n",
      "Epoch 259/1000\n",
      "10/10 [==============================] - 0s 210us/sample - loss: 300.7363\n",
      "Epoch 260/1000\n",
      "10/10 [==============================] - 0s 234us/sample - loss: 295.9587\n",
      "Epoch 261/1000\n",
      "10/10 [==============================] - 0s 209us/sample - loss: 291.3190\n",
      "Epoch 262/1000\n",
      "10/10 [==============================] - 0s 208us/sample - loss: 286.8177\n",
      "Epoch 263/1000\n",
      "10/10 [==============================] - 0s 181us/sample - loss: 282.4551\n",
      "Epoch 264/1000\n",
      "10/10 [==============================] - 0s 197us/sample - loss: 278.2312\n",
      "Epoch 265/1000\n",
      "10/10 [==============================] - 0s 231us/sample - loss: 274.1461\n",
      "Epoch 266/1000\n",
      "10/10 [==============================] - 0s 247us/sample - loss: 270.1994\n",
      "Epoch 267/1000\n",
      "10/10 [==============================] - 0s 232us/sample - loss: 266.3910\n",
      "Epoch 268/1000\n",
      "10/10 [==============================] - 0s 219us/sample - loss: 262.7202\n",
      "Epoch 269/1000\n",
      "10/10 [==============================] - 0s 218us/sample - loss: 259.1862\n",
      "Epoch 270/1000\n",
      "10/10 [==============================] - 0s 265us/sample - loss: 255.7880\n",
      "Epoch 271/1000\n",
      "10/10 [==============================] - 0s 197us/sample - loss: 252.5247\n",
      "Epoch 272/1000\n",
      "10/10 [==============================] - 0s 370us/sample - loss: 249.3944\n",
      "Epoch 273/1000\n",
      "10/10 [==============================] - 0s 194us/sample - loss: 246.3954\n",
      "Epoch 274/1000\n",
      "10/10 [==============================] - 0s 226us/sample - loss: 243.5257\n",
      "Epoch 275/1000\n",
      "10/10 [==============================] - 0s 337us/sample - loss: 240.7825\n",
      "Epoch 276/1000\n",
      "10/10 [==============================] - 0s 236us/sample - loss: 238.1632\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 277/1000\n",
      "10/10 [==============================] - 0s 221us/sample - loss: 235.6644\n",
      "Epoch 278/1000\n",
      "10/10 [==============================] - 0s 276us/sample - loss: 233.2824\n",
      "Epoch 279/1000\n",
      "10/10 [==============================] - 0s 202us/sample - loss: 231.0132\n",
      "Epoch 280/1000\n",
      "10/10 [==============================] - 0s 210us/sample - loss: 228.8526\n",
      "Epoch 281/1000\n",
      "10/10 [==============================] - 0s 329us/sample - loss: 226.7962\n",
      "Epoch 282/1000\n",
      "10/10 [==============================] - 0s 243us/sample - loss: 224.8396\n",
      "Epoch 283/1000\n",
      "10/10 [==============================] - 0s 222us/sample - loss: 222.9787\n",
      "Epoch 284/1000\n",
      "10/10 [==============================] - 0s 272us/sample - loss: 221.2095\n",
      "Epoch 285/1000\n",
      "10/10 [==============================] - 0s 177us/sample - loss: 219.5286\n",
      "Epoch 286/1000\n",
      "10/10 [==============================] - 0s 189us/sample - loss: 217.9333\n",
      "Epoch 287/1000\n",
      "10/10 [==============================] - 0s 200us/sample - loss: 216.4209\n",
      "Epoch 288/1000\n",
      "10/10 [==============================] - 0s 249us/sample - loss: 214.9894\n",
      "Epoch 289/1000\n",
      "10/10 [==============================] - 0s 218us/sample - loss: 213.6368\n",
      "Epoch 290/1000\n",
      "10/10 [==============================] - 0s 184us/sample - loss: 212.3612\n",
      "Epoch 291/1000\n",
      "10/10 [==============================] - 0s 214us/sample - loss: 211.1609\n",
      "Epoch 292/1000\n",
      "10/10 [==============================] - 0s 223us/sample - loss: 210.0336\n",
      "Epoch 293/1000\n",
      "10/10 [==============================] - 0s 220us/sample - loss: 208.9775\n",
      "Epoch 294/1000\n",
      "10/10 [==============================] - 0s 317us/sample - loss: 207.9899\n",
      "Epoch 295/1000\n",
      "10/10 [==============================] - 0s 177us/sample - loss: 207.0685\n",
      "Epoch 296/1000\n",
      "10/10 [==============================] - 0s 228us/sample - loss: 206.2104\n",
      "Epoch 297/1000\n",
      "10/10 [==============================] - 0s 228us/sample - loss: 205.4128\n",
      "Epoch 298/1000\n",
      "10/10 [==============================] - 0s 192us/sample - loss: 204.6724\n",
      "Epoch 299/1000\n",
      "10/10 [==============================] - 0s 229us/sample - loss: 203.9859\n",
      "Epoch 300/1000\n",
      "10/10 [==============================] - 0s 284us/sample - loss: 203.3501\n",
      "Epoch 301/1000\n",
      "10/10 [==============================] - 0s 286us/sample - loss: 202.7616\n",
      "Epoch 302/1000\n",
      "10/10 [==============================] - 0s 477us/sample - loss: 202.2163\n",
      "Epoch 303/1000\n",
      "10/10 [==============================] - 0s 229us/sample - loss: 201.7108\n",
      "Epoch 304/1000\n",
      "10/10 [==============================] - 0s 185us/sample - loss: 201.2414\n",
      "Epoch 305/1000\n",
      "10/10 [==============================] - 0s 227us/sample - loss: 200.8049\n",
      "Epoch 306/1000\n",
      "10/10 [==============================] - 0s 384us/sample - loss: 200.3976\n",
      "Epoch 307/1000\n",
      "10/10 [==============================] - 0s 282us/sample - loss: 200.0161\n",
      "Epoch 308/1000\n",
      "10/10 [==============================] - 0s 309us/sample - loss: 199.6573\n",
      "Epoch 309/1000\n",
      "10/10 [==============================] - 0s 289us/sample - loss: 199.3183\n",
      "Epoch 310/1000\n",
      "10/10 [==============================] - 0s 319us/sample - loss: 198.9961\n",
      "Epoch 311/1000\n",
      "10/10 [==============================] - 0s 195us/sample - loss: 198.6885\n",
      "Epoch 312/1000\n",
      "10/10 [==============================] - 0s 209us/sample - loss: 198.3931\n",
      "Epoch 313/1000\n",
      "10/10 [==============================] - 0s 302us/sample - loss: 198.1079\n",
      "Epoch 314/1000\n",
      "10/10 [==============================] - 0s 199us/sample - loss: 197.8312\n",
      "Epoch 315/1000\n",
      "10/10 [==============================] - 0s 187us/sample - loss: 197.5614\n",
      "Epoch 316/1000\n",
      "10/10 [==============================] - 0s 210us/sample - loss: 197.2975\n",
      "Epoch 317/1000\n",
      "10/10 [==============================] - 0s 213us/sample - loss: 197.0382\n",
      "Epoch 318/1000\n",
      "10/10 [==============================] - 0s 469us/sample - loss: 196.7828\n",
      "Epoch 319/1000\n",
      "10/10 [==============================] - 0s 197us/sample - loss: 196.5305\n",
      "Epoch 320/1000\n",
      "10/10 [==============================] - 0s 206us/sample - loss: 196.2808\n",
      "Epoch 321/1000\n",
      "10/10 [==============================] - 0s 204us/sample - loss: 196.0331\n",
      "Epoch 322/1000\n",
      "10/10 [==============================] - 0s 282us/sample - loss: 195.7871\n",
      "Epoch 323/1000\n",
      "10/10 [==============================] - 0s 220us/sample - loss: 195.5426\n",
      "Epoch 324/1000\n",
      "10/10 [==============================] - 0s 210us/sample - loss: 195.2991\n",
      "Epoch 325/1000\n",
      "10/10 [==============================] - 0s 286us/sample - loss: 195.0565\n",
      "Epoch 326/1000\n",
      "10/10 [==============================] - 0s 328us/sample - loss: 194.8148\n",
      "Epoch 327/1000\n",
      "10/10 [==============================] - 0s 225us/sample - loss: 194.5737\n",
      "Epoch 328/1000\n",
      "10/10 [==============================] - 0s 199us/sample - loss: 194.3330\n",
      "Epoch 329/1000\n",
      "10/10 [==============================] - 0s 230us/sample - loss: 194.0926\n",
      "Epoch 330/1000\n",
      "10/10 [==============================] - 0s 239us/sample - loss: 193.8524\n",
      "Epoch 331/1000\n",
      "10/10 [==============================] - 0s 407us/sample - loss: 193.6124\n",
      "Epoch 332/1000\n",
      "10/10 [==============================] - 0s 207us/sample - loss: 193.3723\n",
      "Epoch 333/1000\n",
      "10/10 [==============================] - 0s 183us/sample - loss: 193.1322\n",
      "Epoch 334/1000\n",
      "10/10 [==============================] - 0s 213us/sample - loss: 192.8921\n",
      "Epoch 335/1000\n",
      "10/10 [==============================] - 0s 411us/sample - loss: 192.6517\n",
      "Epoch 336/1000\n",
      "10/10 [==============================] - 0s 218us/sample - loss: 192.4112\n",
      "Epoch 337/1000\n",
      "10/10 [==============================] - 0s 177us/sample - loss: 192.1703\n",
      "Epoch 338/1000\n",
      "10/10 [==============================] - 0s 244us/sample - loss: 191.9292\n",
      "Epoch 339/1000\n",
      "10/10 [==============================] - 0s 275us/sample - loss: 191.6877\n",
      "Epoch 340/1000\n",
      "10/10 [==============================] - 0s 339us/sample - loss: 191.4458\n",
      "Epoch 341/1000\n",
      "10/10 [==============================] - 0s 181us/sample - loss: 191.2034\n",
      "Epoch 342/1000\n",
      "10/10 [==============================] - 0s 235us/sample - loss: 190.9607\n",
      "Epoch 343/1000\n",
      "10/10 [==============================] - 0s 213us/sample - loss: 190.7176\n",
      "Epoch 344/1000\n",
      "10/10 [==============================] - 0s 323us/sample - loss: 190.4739\n",
      "Epoch 345/1000\n",
      "10/10 [==============================] - 0s 218us/sample - loss: 190.2299\n",
      "Epoch 346/1000\n",
      "10/10 [==============================] - 0s 334us/sample - loss: 189.9853\n",
      "Epoch 347/1000\n",
      "10/10 [==============================] - 0s 460us/sample - loss: 189.7404\n",
      "Epoch 348/1000\n",
      "10/10 [==============================] - 0s 203us/sample - loss: 189.4949\n",
      "Epoch 349/1000\n",
      "10/10 [==============================] - 0s 258us/sample - loss: 189.2490\n",
      "Epoch 350/1000\n",
      "10/10 [==============================] - 0s 223us/sample - loss: 189.0027\n",
      "Epoch 351/1000\n",
      "10/10 [==============================] - 0s 245us/sample - loss: 188.7558\n",
      "Epoch 352/1000\n",
      "10/10 [==============================] - 0s 327us/sample - loss: 188.5088\n",
      "Epoch 353/1000\n",
      "10/10 [==============================] - 0s 353us/sample - loss: 188.2613\n",
      "Epoch 354/1000\n",
      "10/10 [==============================] - 0s 333us/sample - loss: 188.0133\n",
      "Epoch 355/1000\n",
      "10/10 [==============================] - 0s 426us/sample - loss: 187.7651\n",
      "Epoch 356/1000\n",
      "10/10 [==============================] - 0s 224us/sample - loss: 187.5166\n",
      "Epoch 357/1000\n",
      "10/10 [==============================] - 0s 327us/sample - loss: 187.2678\n",
      "Epoch 358/1000\n",
      "10/10 [==============================] - 0s 370us/sample - loss: 187.0189\n",
      "Epoch 359/1000\n",
      "10/10 [==============================] - 0s 235us/sample - loss: 186.7697\n",
      "Epoch 360/1000\n",
      "10/10 [==============================] - 0s 335us/sample - loss: 186.5203\n",
      "Epoch 361/1000\n",
      "10/10 [==============================] - 0s 446us/sample - loss: 186.2708\n",
      "Epoch 362/1000\n",
      "10/10 [==============================] - 0s 629us/sample - loss: 186.0212\n",
      "Epoch 363/1000\n",
      "10/10 [==============================] - 0s 312us/sample - loss: 185.7716\n",
      "Epoch 364/1000\n",
      "10/10 [==============================] - 0s 313us/sample - loss: 185.5219\n",
      "Epoch 365/1000\n",
      "10/10 [==============================] - 0s 388us/sample - loss: 185.2723\n",
      "Epoch 366/1000\n",
      "10/10 [==============================] - 0s 207us/sample - loss: 185.0227\n",
      "Epoch 367/1000\n",
      "10/10 [==============================] - 0s 214us/sample - loss: 184.7731\n",
      "Epoch 368/1000\n",
      "10/10 [==============================] - 0s 393us/sample - loss: 184.5237\n",
      "Epoch 369/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 252us/sample - loss: 184.2745\n",
      "Epoch 370/1000\n",
      "10/10 [==============================] - 0s 220us/sample - loss: 184.0255\n",
      "Epoch 371/1000\n",
      "10/10 [==============================] - 0s 271us/sample - loss: 183.7766\n",
      "Epoch 372/1000\n",
      "10/10 [==============================] - 0s 385us/sample - loss: 183.5281\n",
      "Epoch 373/1000\n",
      "10/10 [==============================] - 0s 299us/sample - loss: 183.2800\n",
      "Epoch 374/1000\n",
      "10/10 [==============================] - 0s 176us/sample - loss: 183.0319\n",
      "Epoch 375/1000\n",
      "10/10 [==============================] - 0s 214us/sample - loss: 182.7845\n",
      "Epoch 376/1000\n",
      "10/10 [==============================] - 0s 220us/sample - loss: 182.5373\n",
      "Epoch 377/1000\n",
      "10/10 [==============================] - 0s 216us/sample - loss: 182.2906\n",
      "Epoch 378/1000\n",
      "10/10 [==============================] - 0s 244us/sample - loss: 182.0443\n",
      "Epoch 379/1000\n",
      "10/10 [==============================] - 0s 223us/sample - loss: 181.7986\n",
      "Epoch 380/1000\n",
      "10/10 [==============================] - 0s 236us/sample - loss: 181.5540\n",
      "Epoch 381/1000\n",
      "10/10 [==============================] - 0s 190us/sample - loss: 181.3127\n",
      "Epoch 382/1000\n",
      "10/10 [==============================] - 0s 286us/sample - loss: 181.0929\n",
      "Epoch 383/1000\n",
      "10/10 [==============================] - 0s 204us/sample - loss: 180.9303\n",
      "Epoch 384/1000\n",
      "10/10 [==============================] - 0s 203us/sample - loss: 180.7543\n",
      "Epoch 385/1000\n",
      "10/10 [==============================] - 0s 366us/sample - loss: 180.4641\n",
      "Epoch 386/1000\n",
      "10/10 [==============================] - 0s 207us/sample - loss: 180.1974\n",
      "Epoch 387/1000\n",
      "10/10 [==============================] - 0s 190us/sample - loss: 179.9595\n",
      "Epoch 388/1000\n",
      "10/10 [==============================] - 0s 211us/sample - loss: 179.7322\n",
      "Epoch 389/1000\n",
      "10/10 [==============================] - 0s 357us/sample - loss: 179.5082\n",
      "Epoch 390/1000\n",
      "10/10 [==============================] - 0s 240us/sample - loss: 179.2855\n",
      "Epoch 391/1000\n",
      "10/10 [==============================] - 0s 230us/sample - loss: 179.0629\n",
      "Epoch 392/1000\n",
      "10/10 [==============================] - 0s 182us/sample - loss: 178.8405\n",
      "Epoch 393/1000\n",
      "10/10 [==============================] - 0s 225us/sample - loss: 178.6180\n",
      "Epoch 394/1000\n",
      "10/10 [==============================] - 0s 369us/sample - loss: 178.3957\n",
      "Epoch 395/1000\n",
      "10/10 [==============================] - 0s 249us/sample - loss: 178.1735\n",
      "Epoch 396/1000\n",
      "10/10 [==============================] - 0s 200us/sample - loss: 177.9521\n",
      "Epoch 397/1000\n",
      "10/10 [==============================] - 0s 250us/sample - loss: 177.7319\n",
      "Epoch 398/1000\n",
      "10/10 [==============================] - 0s 268us/sample - loss: 177.5150\n",
      "Epoch 399/1000\n",
      "10/10 [==============================] - 0s 184us/sample - loss: 177.3033\n",
      "Epoch 400/1000\n",
      "10/10 [==============================] - 0s 266us/sample - loss: 177.1009\n",
      "Epoch 401/1000\n",
      "10/10 [==============================] - 0s 179us/sample - loss: 176.9009\n",
      "Epoch 402/1000\n",
      "10/10 [==============================] - 0s 370us/sample - loss: 176.6931\n",
      "Epoch 403/1000\n",
      "10/10 [==============================] - 0s 250us/sample - loss: 176.4715\n",
      "Epoch 404/1000\n",
      "10/10 [==============================] - 0s 224us/sample - loss: 176.2460\n",
      "Epoch 405/1000\n",
      "10/10 [==============================] - 0s 257us/sample - loss: 176.0245\n",
      "Epoch 406/1000\n",
      "10/10 [==============================] - 0s 224us/sample - loss: 175.8069\n",
      "Epoch 407/1000\n",
      "10/10 [==============================] - 0s 342us/sample - loss: 175.5927\n",
      "Epoch 408/1000\n",
      "10/10 [==============================] - 0s 191us/sample - loss: 175.3803\n",
      "Epoch 409/1000\n",
      "10/10 [==============================] - 0s 193us/sample - loss: 175.1692\n",
      "Epoch 410/1000\n",
      "10/10 [==============================] - 0s 165us/sample - loss: 174.9592\n",
      "Epoch 411/1000\n",
      "10/10 [==============================] - 0s 300us/sample - loss: 174.7504\n",
      "Epoch 412/1000\n",
      "10/10 [==============================] - 0s 204us/sample - loss: 174.5427\n",
      "Epoch 413/1000\n",
      "10/10 [==============================] - 0s 237us/sample - loss: 174.3369\n",
      "Epoch 414/1000\n",
      "10/10 [==============================] - 0s 201us/sample - loss: 174.1334\n",
      "Epoch 415/1000\n",
      "10/10 [==============================] - 0s 299us/sample - loss: 173.9321\n",
      "Epoch 416/1000\n",
      "10/10 [==============================] - 0s 309us/sample - loss: 173.7318\n",
      "Epoch 417/1000\n",
      "10/10 [==============================] - 0s 207us/sample - loss: 173.5306\n",
      "Epoch 418/1000\n",
      "10/10 [==============================] - 0s 198us/sample - loss: 173.3263\n",
      "Epoch 419/1000\n",
      "10/10 [==============================] - 0s 198us/sample - loss: 173.1201\n",
      "Epoch 420/1000\n",
      "10/10 [==============================] - 0s 245us/sample - loss: 172.9124\n",
      "Epoch 421/1000\n",
      "10/10 [==============================] - 0s 228us/sample - loss: 172.7061\n",
      "Epoch 422/1000\n",
      "10/10 [==============================] - 0s 174us/sample - loss: 172.5006\n",
      "Epoch 423/1000\n",
      "10/10 [==============================] - 0s 178us/sample - loss: 172.2970\n",
      "Epoch 424/1000\n",
      "10/10 [==============================] - 0s 260us/sample - loss: 172.0945\n",
      "Epoch 425/1000\n",
      "10/10 [==============================] - 0s 245us/sample - loss: 171.8935\n",
      "Epoch 426/1000\n",
      "10/10 [==============================] - 0s 258us/sample - loss: 171.6934\n",
      "Epoch 427/1000\n",
      "10/10 [==============================] - 0s 231us/sample - loss: 171.4949\n",
      "Epoch 428/1000\n",
      "10/10 [==============================] - 0s 195us/sample - loss: 171.2973\n",
      "Epoch 429/1000\n",
      "10/10 [==============================] - 0s 230us/sample - loss: 171.1011\n",
      "Epoch 430/1000\n",
      "10/10 [==============================] - 0s 378us/sample - loss: 170.9054\n",
      "Epoch 431/1000\n",
      "10/10 [==============================] - 0s 191us/sample - loss: 170.7105\n",
      "Epoch 432/1000\n",
      "10/10 [==============================] - 0s 206us/sample - loss: 170.5146\n",
      "Epoch 433/1000\n",
      "10/10 [==============================] - 0s 192us/sample - loss: 170.3188\n",
      "Epoch 434/1000\n",
      "10/10 [==============================] - 0s 373us/sample - loss: 170.1215\n",
      "Epoch 435/1000\n",
      "10/10 [==============================] - 0s 222us/sample - loss: 169.9247\n",
      "Epoch 436/1000\n",
      "10/10 [==============================] - 0s 174us/sample - loss: 169.7274\n",
      "Epoch 437/1000\n",
      "10/10 [==============================] - 0s 188us/sample - loss: 169.5316\n",
      "Epoch 438/1000\n",
      "10/10 [==============================] - 0s 202us/sample - loss: 169.3358\n",
      "Epoch 439/1000\n",
      "10/10 [==============================] - 0s 252us/sample - loss: 169.1417\n",
      "Epoch 440/1000\n",
      "10/10 [==============================] - 0s 242us/sample - loss: 168.9480\n",
      "Epoch 441/1000\n",
      "10/10 [==============================] - 0s 201us/sample - loss: 168.7556\n",
      "Epoch 442/1000\n",
      "10/10 [==============================] - 0s 210us/sample - loss: 168.5636\n",
      "Epoch 443/1000\n",
      "10/10 [==============================] - 0s 257us/sample - loss: 168.3731\n",
      "Epoch 444/1000\n",
      "10/10 [==============================] - 0s 271us/sample - loss: 168.1824\n",
      "Epoch 445/1000\n",
      "10/10 [==============================] - 0s 209us/sample - loss: 167.9930\n",
      "Epoch 446/1000\n",
      "10/10 [==============================] - 0s 264us/sample - loss: 167.8027\n",
      "Epoch 447/1000\n",
      "10/10 [==============================] - 0s 231us/sample - loss: 167.6134\n",
      "Epoch 448/1000\n",
      "10/10 [==============================] - 0s 202us/sample - loss: 167.4229\n",
      "Epoch 449/1000\n",
      "10/10 [==============================] - 0s 200us/sample - loss: 167.2333\n",
      "Epoch 450/1000\n",
      "10/10 [==============================] - 0s 215us/sample - loss: 167.0429\n",
      "Epoch 451/1000\n",
      "10/10 [==============================] - 0s 173us/sample - loss: 166.8537\n",
      "Epoch 452/1000\n",
      "10/10 [==============================] - 0s 175us/sample - loss: 166.6640\n",
      "Epoch 453/1000\n",
      "10/10 [==============================] - 0s 212us/sample - loss: 166.4758\n",
      "Epoch 454/1000\n",
      "10/10 [==============================] - 0s 205us/sample - loss: 166.2874\n",
      "Epoch 455/1000\n",
      "10/10 [==============================] - 0s 182us/sample - loss: 166.1005\n",
      "Epoch 456/1000\n",
      "10/10 [==============================] - 0s 198us/sample - loss: 165.9133\n",
      "Epoch 457/1000\n",
      "10/10 [==============================] - 0s 183us/sample - loss: 165.7276\n",
      "Epoch 458/1000\n",
      "10/10 [==============================] - 0s 188us/sample - loss: 165.5414\n",
      "Epoch 459/1000\n",
      "10/10 [==============================] - 0s 300us/sample - loss: 165.3566\n",
      "Epoch 460/1000\n",
      "10/10 [==============================] - 0s 198us/sample - loss: 165.1708\n",
      "Epoch 461/1000\n",
      "10/10 [==============================] - 0s 198us/sample - loss: 164.9864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 462/1000\n",
      "10/10 [==============================] - 0s 182us/sample - loss: 164.8008\n",
      "Epoch 463/1000\n",
      "10/10 [==============================] - 0s 234us/sample - loss: 164.6164\n",
      "Epoch 464/1000\n",
      "10/10 [==============================] - 0s 211us/sample - loss: 164.4310\n",
      "Epoch 465/1000\n",
      "10/10 [==============================] - 0s 176us/sample - loss: 164.2469\n",
      "Epoch 466/1000\n",
      "10/10 [==============================] - 0s 213us/sample - loss: 164.0620\n",
      "Epoch 467/1000\n",
      "10/10 [==============================] - 0s 192us/sample - loss: 163.8786\n",
      "Epoch 468/1000\n",
      "10/10 [==============================] - 0s 246us/sample - loss: 163.6945\n",
      "Epoch 469/1000\n",
      "10/10 [==============================] - 0s 256us/sample - loss: 163.5119\n",
      "Epoch 470/1000\n",
      "10/10 [==============================] - 0s 185us/sample - loss: 163.3285\n",
      "Epoch 471/1000\n",
      "10/10 [==============================] - 0s 198us/sample - loss: 163.1467\n",
      "Epoch 472/1000\n",
      "10/10 [==============================] - 0s 247us/sample - loss: 162.9641\n",
      "Epoch 473/1000\n",
      "10/10 [==============================] - 0s 223us/sample - loss: 162.7829\n",
      "Epoch 474/1000\n",
      "10/10 [==============================] - 0s 203us/sample - loss: 162.6006\n",
      "Epoch 475/1000\n",
      "10/10 [==============================] - 0s 205us/sample - loss: 162.4198\n",
      "Epoch 476/1000\n",
      "10/10 [==============================] - 0s 164us/sample - loss: 162.2378\n",
      "Epoch 477/1000\n",
      "10/10 [==============================] - 0s 193us/sample - loss: 162.0572\n",
      "Epoch 478/1000\n",
      "10/10 [==============================] - 0s 265us/sample - loss: 161.8754\n",
      "Epoch 479/1000\n",
      "10/10 [==============================] - 0s 379us/sample - loss: 161.6950\n",
      "Epoch 480/1000\n",
      "10/10 [==============================] - 0s 224us/sample - loss: 161.5136\n",
      "Epoch 481/1000\n",
      "10/10 [==============================] - 0s 204us/sample - loss: 161.3336\n",
      "Epoch 482/1000\n",
      "10/10 [==============================] - 0s 200us/sample - loss: 161.1526\n",
      "Epoch 483/1000\n",
      "10/10 [==============================] - 0s 152us/sample - loss: 160.9732\n",
      "Epoch 484/1000\n",
      "10/10 [==============================] - 0s 171us/sample - loss: 160.7927\n",
      "Epoch 485/1000\n",
      "10/10 [==============================] - 0s 249us/sample - loss: 160.6137\n",
      "Epoch 486/1000\n",
      "10/10 [==============================] - 0s 219us/sample - loss: 160.4337\n",
      "Epoch 487/1000\n",
      "10/10 [==============================] - 0s 247us/sample - loss: 160.2551\n",
      "Epoch 488/1000\n",
      "10/10 [==============================] - 0s 205us/sample - loss: 160.0754\n",
      "Epoch 489/1000\n",
      "10/10 [==============================] - 0s 171us/sample - loss: 159.8971\n",
      "Epoch 490/1000\n",
      "10/10 [==============================] - 0s 200us/sample - loss: 159.7175\n",
      "Epoch 491/1000\n",
      "10/10 [==============================] - 0s 186us/sample - loss: 159.5394\n",
      "Epoch 492/1000\n",
      "10/10 [==============================] - 0s 413us/sample - loss: 159.3600\n",
      "Epoch 493/1000\n",
      "10/10 [==============================] - 0s 189us/sample - loss: 159.1820\n",
      "Epoch 494/1000\n",
      "10/10 [==============================] - 0s 231us/sample - loss: 159.0028\n",
      "Epoch 495/1000\n",
      "10/10 [==============================] - 0s 204us/sample - loss: 158.8251\n",
      "Epoch 496/1000\n",
      "10/10 [==============================] - 0s 220us/sample - loss: 158.6462\n",
      "Epoch 497/1000\n",
      "10/10 [==============================] - 0s 318us/sample - loss: 158.4687\n",
      "Epoch 498/1000\n",
      "10/10 [==============================] - 0s 246us/sample - loss: 158.2902\n",
      "Epoch 499/1000\n",
      "10/10 [==============================] - 0s 160us/sample - loss: 158.1130\n",
      "Epoch 500/1000\n",
      "10/10 [==============================] - 0s 319us/sample - loss: 157.9345\n",
      "Epoch 501/1000\n",
      "10/10 [==============================] - 0s 260us/sample - loss: 157.7575\n",
      "Epoch 502/1000\n",
      "10/10 [==============================] - 0s 216us/sample - loss: 157.5794\n",
      "Epoch 503/1000\n",
      "10/10 [==============================] - 0s 171us/sample - loss: 157.4025\n",
      "Epoch 504/1000\n",
      "10/10 [==============================] - 0s 211us/sample - loss: 157.2243\n",
      "Epoch 505/1000\n",
      "10/10 [==============================] - 0s 222us/sample - loss: 157.0475\n",
      "Epoch 506/1000\n",
      "10/10 [==============================] - 0s 255us/sample - loss: 156.8695\n",
      "Epoch 507/1000\n",
      "10/10 [==============================] - 0s 215us/sample - loss: 156.6927\n",
      "Epoch 508/1000\n",
      "10/10 [==============================] - 0s 203us/sample - loss: 156.5147\n",
      "Epoch 509/1000\n",
      "10/10 [==============================] - 0s 215us/sample - loss: 156.3381\n",
      "Epoch 510/1000\n",
      "10/10 [==============================] - 0s 178us/sample - loss: 156.1603\n",
      "Epoch 511/1000\n",
      "10/10 [==============================] - 0s 252us/sample - loss: 155.9837\n",
      "Epoch 512/1000\n",
      "10/10 [==============================] - 0s 237us/sample - loss: 155.8059\n",
      "Epoch 513/1000\n",
      "10/10 [==============================] - 0s 229us/sample - loss: 155.6294\n",
      "Epoch 514/1000\n",
      "10/10 [==============================] - 0s 278us/sample - loss: 155.4518\n",
      "Epoch 515/1000\n",
      "10/10 [==============================] - 0s 233us/sample - loss: 155.2753\n",
      "Epoch 516/1000\n",
      "10/10 [==============================] - 0s 176us/sample - loss: 155.0976\n",
      "Epoch 517/1000\n",
      "10/10 [==============================] - 0s 188us/sample - loss: 154.9211\n",
      "Epoch 518/1000\n",
      "10/10 [==============================] - 0s 181us/sample - loss: 154.7434\n",
      "Epoch 519/1000\n",
      "10/10 [==============================] - 0s 215us/sample - loss: 154.5669\n",
      "Epoch 520/1000\n",
      "10/10 [==============================] - 0s 223us/sample - loss: 154.3891\n",
      "Epoch 521/1000\n",
      "10/10 [==============================] - 0s 206us/sample - loss: 154.2126\n",
      "Epoch 522/1000\n",
      "10/10 [==============================] - 0s 184us/sample - loss: 154.0347\n",
      "Epoch 523/1000\n",
      "10/10 [==============================] - 0s 212us/sample - loss: 153.8581\n",
      "Epoch 524/1000\n",
      "10/10 [==============================] - 0s 236us/sample - loss: 153.6803\n",
      "Epoch 525/1000\n",
      "10/10 [==============================] - 0s 208us/sample - loss: 153.5036\n",
      "Epoch 526/1000\n",
      "10/10 [==============================] - 0s 210us/sample - loss: 153.3256\n",
      "Epoch 527/1000\n",
      "10/10 [==============================] - 0s 188us/sample - loss: 153.1488\n",
      "Epoch 528/1000\n",
      "10/10 [==============================] - 0s 232us/sample - loss: 152.9709\n",
      "Epoch 529/1000\n",
      "10/10 [==============================] - 0s 192us/sample - loss: 152.7939\n",
      "Epoch 530/1000\n",
      "10/10 [==============================] - 0s 281us/sample - loss: 152.6158\n",
      "Epoch 531/1000\n",
      "10/10 [==============================] - 0s 179us/sample - loss: 152.4387\n",
      "Epoch 532/1000\n",
      "10/10 [==============================] - 0s 179us/sample - loss: 152.2605\n",
      "Epoch 533/1000\n",
      "10/10 [==============================] - 0s 216us/sample - loss: 152.0833\n",
      "Epoch 534/1000\n",
      "10/10 [==============================] - 0s 239us/sample - loss: 151.9048\n",
      "Epoch 535/1000\n",
      "10/10 [==============================] - 0s 206us/sample - loss: 151.7274\n",
      "Epoch 536/1000\n",
      "10/10 [==============================] - 0s 293us/sample - loss: 151.5488\n",
      "Epoch 537/1000\n",
      "10/10 [==============================] - 0s 216us/sample - loss: 151.3712\n",
      "Epoch 538/1000\n",
      "10/10 [==============================] - 0s 314us/sample - loss: 151.1923\n",
      "Epoch 539/1000\n",
      "10/10 [==============================] - 0s 225us/sample - loss: 151.0144\n",
      "Epoch 540/1000\n",
      "10/10 [==============================] - 0s 273us/sample - loss: 150.8354\n",
      "Epoch 541/1000\n",
      "10/10 [==============================] - 0s 291us/sample - loss: 150.6574\n",
      "Epoch 542/1000\n",
      "10/10 [==============================] - 0s 247us/sample - loss: 150.4781\n",
      "Epoch 543/1000\n",
      "10/10 [==============================] - 0s 181us/sample - loss: 150.2998\n",
      "Epoch 544/1000\n",
      "10/10 [==============================] - 0s 177us/sample - loss: 150.1203\n",
      "Epoch 545/1000\n",
      "10/10 [==============================] - 0s 249us/sample - loss: 149.9416\n",
      "Epoch 546/1000\n",
      "10/10 [==============================] - 0s 272us/sample - loss: 149.7617\n",
      "Epoch 547/1000\n",
      "10/10 [==============================] - 0s 294us/sample - loss: 149.5829\n",
      "Epoch 548/1000\n",
      "10/10 [==============================] - 0s 210us/sample - loss: 149.4028\n",
      "Epoch 549/1000\n",
      "10/10 [==============================] - 0s 204us/sample - loss: 149.2235\n",
      "Epoch 550/1000\n",
      "10/10 [==============================] - 0s 301us/sample - loss: 149.0431\n",
      "Epoch 551/1000\n",
      "10/10 [==============================] - 0s 203us/sample - loss: 148.8635\n",
      "Epoch 552/1000\n",
      "10/10 [==============================] - 0s 335us/sample - loss: 148.6827\n",
      "Epoch 553/1000\n",
      "10/10 [==============================] - 0s 276us/sample - loss: 148.5028\n",
      "Epoch 554/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 300us/sample - loss: 148.3217\n",
      "Epoch 555/1000\n",
      "10/10 [==============================] - 0s 216us/sample - loss: 148.1414\n",
      "Epoch 556/1000\n",
      "10/10 [==============================] - 0s 299us/sample - loss: 147.9598\n",
      "Epoch 557/1000\n",
      "10/10 [==============================] - 0s 216us/sample - loss: 147.7791\n",
      "Epoch 558/1000\n",
      "10/10 [==============================] - 0s 310us/sample - loss: 147.5972\n",
      "Epoch 559/1000\n",
      "10/10 [==============================] - 0s 243us/sample - loss: 147.4161\n",
      "Epoch 560/1000\n",
      "10/10 [==============================] - 0s 251us/sample - loss: 147.2338\n",
      "Epoch 561/1000\n",
      "10/10 [==============================] - 0s 349us/sample - loss: 147.0522\n",
      "Epoch 562/1000\n",
      "10/10 [==============================] - 0s 386us/sample - loss: 146.8694\n",
      "Epoch 563/1000\n",
      "10/10 [==============================] - 0s 350us/sample - loss: 146.6874\n",
      "Epoch 564/1000\n",
      "10/10 [==============================] - 0s 211us/sample - loss: 146.5042\n",
      "Epoch 565/1000\n",
      "10/10 [==============================] - 0s 418us/sample - loss: 146.3217\n",
      "Epoch 566/1000\n",
      "10/10 [==============================] - 0s 250us/sample - loss: 146.1379\n",
      "Epoch 567/1000\n",
      "10/10 [==============================] - 0s 179us/sample - loss: 145.9549\n",
      "Epoch 568/1000\n",
      "10/10 [==============================] - 0s 461us/sample - loss: 145.7708\n",
      "Epoch 569/1000\n",
      "10/10 [==============================] - 0s 299us/sample - loss: 145.5873\n",
      "Epoch 570/1000\n",
      "10/10 [==============================] - 0s 306us/sample - loss: 145.4025\n",
      "Epoch 571/1000\n",
      "10/10 [==============================] - 0s 458us/sample - loss: 145.2185\n",
      "Epoch 572/1000\n",
      "10/10 [==============================] - 0s 315us/sample - loss: 145.0332\n",
      "Epoch 573/1000\n",
      "10/10 [==============================] - 0s 282us/sample - loss: 144.8486\n",
      "Epoch 574/1000\n",
      "10/10 [==============================] - 0s 230us/sample - loss: 144.6628\n",
      "Epoch 575/1000\n",
      "10/10 [==============================] - 0s 178us/sample - loss: 144.4776\n",
      "Epoch 576/1000\n",
      "10/10 [==============================] - 0s 242us/sample - loss: 144.2912\n",
      "Epoch 577/1000\n",
      "10/10 [==============================] - 0s 296us/sample - loss: 144.1054\n",
      "Epoch 578/1000\n",
      "10/10 [==============================] - 0s 233us/sample - loss: 143.9185\n",
      "Epoch 579/1000\n",
      "10/10 [==============================] - 0s 309us/sample - loss: 143.7321\n",
      "Epoch 580/1000\n",
      "10/10 [==============================] - 0s 393us/sample - loss: 143.5444\n",
      "Epoch 581/1000\n",
      "10/10 [==============================] - 0s 275us/sample - loss: 143.3574\n",
      "Epoch 582/1000\n",
      "10/10 [==============================] - 0s 259us/sample - loss: 143.1691\n",
      "Epoch 583/1000\n",
      "10/10 [==============================] - 0s 287us/sample - loss: 142.9815\n",
      "Epoch 584/1000\n",
      "10/10 [==============================] - 0s 295us/sample - loss: 142.7925\n",
      "Epoch 585/1000\n",
      "10/10 [==============================] - 0s 296us/sample - loss: 142.6042\n",
      "Epoch 586/1000\n",
      "10/10 [==============================] - 0s 385us/sample - loss: 142.4146\n",
      "Epoch 587/1000\n",
      "10/10 [==============================] - 0s 313us/sample - loss: 142.2256\n",
      "Epoch 588/1000\n",
      "10/10 [==============================] - 0s 274us/sample - loss: 142.0352\n",
      "Epoch 589/1000\n",
      "10/10 [==============================] - 0s 320us/sample - loss: 141.8455\n",
      "Epoch 590/1000\n",
      "10/10 [==============================] - 0s 330us/sample - loss: 141.6544\n",
      "Epoch 591/1000\n",
      "10/10 [==============================] - 0s 252us/sample - loss: 141.4640\n",
      "Epoch 592/1000\n",
      "10/10 [==============================] - 0s 274us/sample - loss: 141.2722\n",
      "Epoch 593/1000\n",
      "10/10 [==============================] - 0s 372us/sample - loss: 141.0810\n",
      "Epoch 594/1000\n",
      "10/10 [==============================] - 0s 475us/sample - loss: 140.8884\n",
      "Epoch 595/1000\n",
      "10/10 [==============================] - 0s 216us/sample - loss: 140.6964\n",
      "Epoch 596/1000\n",
      "10/10 [==============================] - 0s 188us/sample - loss: 140.5031\n",
      "Epoch 597/1000\n",
      "10/10 [==============================] - 0s 254us/sample - loss: 140.3103\n",
      "Epoch 598/1000\n",
      "10/10 [==============================] - 0s 209us/sample - loss: 140.1162\n",
      "Epoch 599/1000\n",
      "10/10 [==============================] - 0s 221us/sample - loss: 139.9225\n",
      "Epoch 600/1000\n",
      "10/10 [==============================] - 0s 238us/sample - loss: 139.7276\n",
      "Epoch 601/1000\n",
      "10/10 [==============================] - 0s 208us/sample - loss: 139.5332\n",
      "Epoch 602/1000\n",
      "10/10 [==============================] - 0s 292us/sample - loss: 139.3374\n",
      "Epoch 603/1000\n",
      "10/10 [==============================] - 0s 217us/sample - loss: 139.1421\n",
      "Epoch 604/1000\n",
      "10/10 [==============================] - 0s 228us/sample - loss: 138.9454\n",
      "Epoch 605/1000\n",
      "10/10 [==============================] - 0s 222us/sample - loss: 138.7492\n",
      "Epoch 606/1000\n",
      "10/10 [==============================] - 0s 239us/sample - loss: 138.5516\n",
      "Epoch 607/1000\n",
      "10/10 [==============================] - 0s 203us/sample - loss: 138.3546\n",
      "Epoch 608/1000\n",
      "10/10 [==============================] - 0s 199us/sample - loss: 138.1561\n",
      "Epoch 609/1000\n",
      "10/10 [==============================] - 0s 505us/sample - loss: 137.9582\n",
      "Epoch 610/1000\n",
      "10/10 [==============================] - 0s 228us/sample - loss: 137.7587\n",
      "Epoch 611/1000\n",
      "10/10 [==============================] - 0s 237us/sample - loss: 137.5599\n",
      "Epoch 612/1000\n",
      "10/10 [==============================] - 0s 243us/sample - loss: 137.3595\n",
      "Epoch 613/1000\n",
      "10/10 [==============================] - 0s 223us/sample - loss: 137.1597\n",
      "Epoch 614/1000\n",
      "10/10 [==============================] - 0s 238us/sample - loss: 136.9583\n",
      "Epoch 615/1000\n",
      "10/10 [==============================] - 0s 259us/sample - loss: 136.7576\n",
      "Epoch 616/1000\n",
      "10/10 [==============================] - 0s 229us/sample - loss: 136.5552\n",
      "Epoch 617/1000\n",
      "10/10 [==============================] - 0s 335us/sample - loss: 136.3534\n",
      "Epoch 618/1000\n",
      "10/10 [==============================] - 0s 289us/sample - loss: 136.1501\n",
      "Epoch 619/1000\n",
      "10/10 [==============================] - 0s 260us/sample - loss: 135.9472\n",
      "Epoch 620/1000\n",
      "10/10 [==============================] - 0s 233us/sample - loss: 135.7429\n",
      "Epoch 621/1000\n",
      "10/10 [==============================] - 0s 274us/sample - loss: 135.5390\n",
      "Epoch 622/1000\n",
      "10/10 [==============================] - 0s 260us/sample - loss: 135.3335\n",
      "Epoch 623/1000\n",
      "10/10 [==============================] - 0s 339us/sample - loss: 135.1287\n",
      "Epoch 624/1000\n",
      "10/10 [==============================] - 0s 249us/sample - loss: 134.9221\n",
      "Epoch 625/1000\n",
      "10/10 [==============================] - 0s 266us/sample - loss: 134.7162\n",
      "Epoch 626/1000\n",
      "10/10 [==============================] - 0s 265us/sample - loss: 134.5086\n",
      "Epoch 627/1000\n",
      "10/10 [==============================] - 0s 593us/sample - loss: 134.3015\n",
      "Epoch 628/1000\n",
      "10/10 [==============================] - 0s 406us/sample - loss: 134.0927\n",
      "Epoch 629/1000\n",
      "10/10 [==============================] - 0s 238us/sample - loss: 133.8846\n",
      "Epoch 630/1000\n",
      "10/10 [==============================] - 0s 234us/sample - loss: 133.6748\n",
      "Epoch 631/1000\n",
      "10/10 [==============================] - 0s 309us/sample - loss: 133.4655\n",
      "Epoch 632/1000\n",
      "10/10 [==============================] - 0s 270us/sample - loss: 133.2544\n",
      "Epoch 633/1000\n",
      "10/10 [==============================] - 0s 187us/sample - loss: 133.0439\n",
      "Epoch 634/1000\n",
      "10/10 [==============================] - 0s 296us/sample - loss: 132.8317\n",
      "Epoch 635/1000\n",
      "10/10 [==============================] - 0s 254us/sample - loss: 132.6201\n",
      "Epoch 636/1000\n",
      "10/10 [==============================] - 0s 291us/sample - loss: 132.4067\n",
      "Epoch 637/1000\n",
      "10/10 [==============================] - 0s 248us/sample - loss: 132.1939\n",
      "Epoch 638/1000\n",
      "10/10 [==============================] - 0s 201us/sample - loss: 131.9793\n",
      "Epoch 639/1000\n",
      "10/10 [==============================] - 0s 280us/sample - loss: 131.7653\n",
      "Epoch 640/1000\n",
      "10/10 [==============================] - 0s 524us/sample - loss: 131.5494\n",
      "Epoch 641/1000\n",
      "10/10 [==============================] - 0s 246us/sample - loss: 131.3342\n",
      "Epoch 642/1000\n",
      "10/10 [==============================] - 0s 309us/sample - loss: 131.1170\n",
      "Epoch 643/1000\n",
      "10/10 [==============================] - 0s 372us/sample - loss: 130.9005\n",
      "Epoch 644/1000\n",
      "10/10 [==============================] - 0s 207us/sample - loss: 130.6821\n",
      "Epoch 645/1000\n",
      "10/10 [==============================] - 0s 199us/sample - loss: 130.4643\n",
      "Epoch 646/1000\n",
      "10/10 [==============================] - 0s 267us/sample - loss: 130.2447\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 647/1000\n",
      "10/10 [==============================] - 0s 318us/sample - loss: 130.0256\n",
      "Epoch 648/1000\n",
      "10/10 [==============================] - 0s 265us/sample - loss: 129.8046\n",
      "Epoch 649/1000\n",
      "10/10 [==============================] - 0s 202us/sample - loss: 129.5841\n",
      "Epoch 650/1000\n",
      "10/10 [==============================] - 0s 288us/sample - loss: 129.3618\n",
      "Epoch 651/1000\n",
      "10/10 [==============================] - 0s 211us/sample - loss: 129.1401\n",
      "Epoch 652/1000\n",
      "10/10 [==============================] - 0s 179us/sample - loss: 128.9164\n",
      "Epoch 653/1000\n",
      "10/10 [==============================] - 0s 266us/sample - loss: 128.6934\n",
      "Epoch 654/1000\n",
      "10/10 [==============================] - 0s 497us/sample - loss: 128.4683\n",
      "Epoch 655/1000\n",
      "10/10 [==============================] - 0s 243us/sample - loss: 128.2438\n",
      "Epoch 656/1000\n",
      "10/10 [==============================] - 0s 271us/sample - loss: 128.0175\n",
      "Epoch 657/1000\n",
      "10/10 [==============================] - 0s 280us/sample - loss: 127.7916\n",
      "Epoch 658/1000\n",
      "10/10 [==============================] - 0s 342us/sample - loss: 127.5638\n",
      "Epoch 659/1000\n",
      "10/10 [==============================] - 0s 276us/sample - loss: 127.3365\n",
      "Epoch 660/1000\n",
      "10/10 [==============================] - 0s 295us/sample - loss: 127.1073\n",
      "Epoch 661/1000\n",
      "10/10 [==============================] - 0s 384us/sample - loss: 126.8786\n",
      "Epoch 662/1000\n",
      "10/10 [==============================] - 0s 277us/sample - loss: 126.6479\n",
      "Epoch 663/1000\n",
      "10/10 [==============================] - 0s 235us/sample - loss: 126.4177\n",
      "Epoch 664/1000\n",
      "10/10 [==============================] - 0s 465us/sample - loss: 126.1856\n",
      "Epoch 665/1000\n",
      "10/10 [==============================] - 0s 188us/sample - loss: 125.9540\n",
      "Epoch 666/1000\n",
      "10/10 [==============================] - 0s 231us/sample - loss: 125.7204\n",
      "Epoch 667/1000\n",
      "10/10 [==============================] - 0s 219us/sample - loss: 125.4874\n",
      "Epoch 668/1000\n",
      "10/10 [==============================] - 0s 233us/sample - loss: 125.2522\n",
      "Epoch 669/1000\n",
      "10/10 [==============================] - 0s 248us/sample - loss: 125.0177\n",
      "Epoch 670/1000\n",
      "10/10 [==============================] - 0s 173us/sample - loss: 124.7811\n",
      "Epoch 671/1000\n",
      "10/10 [==============================] - 0s 404us/sample - loss: 124.5450\n",
      "Epoch 672/1000\n",
      "10/10 [==============================] - 0s 192us/sample - loss: 124.3069\n",
      "Epoch 673/1000\n",
      "10/10 [==============================] - 0s 187us/sample - loss: 124.0693\n",
      "Epoch 674/1000\n",
      "10/10 [==============================] - 0s 202us/sample - loss: 123.8296\n",
      "Epoch 675/1000\n",
      "10/10 [==============================] - 0s 448us/sample - loss: 123.5905\n",
      "Epoch 676/1000\n",
      "10/10 [==============================] - 0s 346us/sample - loss: 123.3492\n",
      "Epoch 677/1000\n",
      "10/10 [==============================] - 0s 175us/sample - loss: 123.1086\n",
      "Epoch 678/1000\n",
      "10/10 [==============================] - 0s 191us/sample - loss: 122.8657\n",
      "Epoch 679/1000\n",
      "10/10 [==============================] - 0s 342us/sample - loss: 122.6235\n",
      "Epoch 680/1000\n",
      "10/10 [==============================] - 0s 295us/sample - loss: 122.3791\n",
      "Epoch 681/1000\n",
      "10/10 [==============================] - 0s 272us/sample - loss: 122.1353\n",
      "Epoch 682/1000\n",
      "10/10 [==============================] - 0s 219us/sample - loss: 121.8893\n",
      "Epoch 683/1000\n",
      "10/10 [==============================] - 0s 388us/sample - loss: 121.6439\n",
      "Epoch 684/1000\n",
      "10/10 [==============================] - 0s 192us/sample - loss: 121.3963\n",
      "Epoch 685/1000\n",
      "10/10 [==============================] - 0s 234us/sample - loss: 121.1492\n",
      "Epoch 686/1000\n",
      "10/10 [==============================] - 0s 262us/sample - loss: 120.9000\n",
      "Epoch 687/1000\n",
      "10/10 [==============================] - 0s 282us/sample - loss: 120.6513\n",
      "Epoch 688/1000\n",
      "10/10 [==============================] - 0s 231us/sample - loss: 120.4004\n",
      "Epoch 689/1000\n",
      "10/10 [==============================] - 0s 231us/sample - loss: 120.1501\n",
      "Epoch 690/1000\n",
      "10/10 [==============================] - 0s 365us/sample - loss: 119.8976\n",
      "Epoch 691/1000\n",
      "10/10 [==============================] - 0s 261us/sample - loss: 119.6456\n",
      "Epoch 692/1000\n",
      "10/10 [==============================] - 0s 245us/sample - loss: 119.3915\n",
      "Epoch 693/1000\n",
      "10/10 [==============================] - 0s 339us/sample - loss: 119.1378\n",
      "Epoch 694/1000\n",
      "10/10 [==============================] - 0s 316us/sample - loss: 118.8821\n",
      "Epoch 695/1000\n",
      "10/10 [==============================] - 0s 185us/sample - loss: 118.6267\n",
      "Epoch 696/1000\n",
      "10/10 [==============================] - 0s 234us/sample - loss: 118.3692\n",
      "Epoch 697/1000\n",
      "10/10 [==============================] - 0s 213us/sample - loss: 118.1121\n",
      "Epoch 698/1000\n",
      "10/10 [==============================] - 0s 265us/sample - loss: 117.8530\n",
      "Epoch 699/1000\n",
      "10/10 [==============================] - 0s 211us/sample - loss: 117.5943\n",
      "Epoch 700/1000\n",
      "10/10 [==============================] - 0s 178us/sample - loss: 117.3335\n",
      "Epoch 701/1000\n",
      "10/10 [==============================] - 0s 168us/sample - loss: 117.0730\n",
      "Epoch 702/1000\n",
      "10/10 [==============================] - 0s 350us/sample - loss: 116.8105\n",
      "Epoch 703/1000\n",
      "10/10 [==============================] - 0s 210us/sample - loss: 116.5483\n",
      "Epoch 704/1000\n",
      "10/10 [==============================] - 0s 248us/sample - loss: 116.2841\n",
      "Epoch 705/1000\n",
      "10/10 [==============================] - 0s 284us/sample - loss: 116.0202\n",
      "Epoch 706/1000\n",
      "10/10 [==============================] - 0s 330us/sample - loss: 115.7542\n",
      "Epoch 707/1000\n",
      "10/10 [==============================] - 0s 255us/sample - loss: 115.4886\n",
      "Epoch 708/1000\n",
      "10/10 [==============================] - 0s 200us/sample - loss: 115.2210\n",
      "Epoch 709/1000\n",
      "10/10 [==============================] - 0s 172us/sample - loss: 114.9536\n",
      "Epoch 710/1000\n",
      "10/10 [==============================] - 0s 261us/sample - loss: 114.6842\n",
      "Epoch 711/1000\n",
      "10/10 [==============================] - 0s 441us/sample - loss: 114.4152\n",
      "Epoch 712/1000\n",
      "10/10 [==============================] - 0s 345us/sample - loss: 114.1441\n",
      "Epoch 713/1000\n",
      "10/10 [==============================] - 0s 199us/sample - loss: 113.8732\n",
      "Epoch 714/1000\n",
      "10/10 [==============================] - 0s 295us/sample - loss: 113.6004\n",
      "Epoch 715/1000\n",
      "10/10 [==============================] - 0s 322us/sample - loss: 113.3279\n",
      "Epoch 716/1000\n",
      "10/10 [==============================] - 0s 237us/sample - loss: 113.0533\n",
      "Epoch 717/1000\n",
      "10/10 [==============================] - 0s 204us/sample - loss: 112.7789\n",
      "Epoch 718/1000\n",
      "10/10 [==============================] - 0s 291us/sample - loss: 112.5027\n",
      "Epoch 719/1000\n",
      "10/10 [==============================] - 0s 455us/sample - loss: 112.2266\n",
      "Epoch 720/1000\n",
      "10/10 [==============================] - 0s 201us/sample - loss: 111.9486\n",
      "Epoch 721/1000\n",
      "10/10 [==============================] - 0s 214us/sample - loss: 111.6708\n",
      "Epoch 722/1000\n",
      "10/10 [==============================] - 0s 317us/sample - loss: 111.3887\n",
      "Epoch 723/1000\n",
      "10/10 [==============================] - 0s 209us/sample - loss: 111.1095\n",
      "Epoch 724/1000\n",
      "10/10 [==============================] - 0s 335us/sample - loss: 110.8303\n",
      "Epoch 725/1000\n",
      "10/10 [==============================] - 0s 286us/sample - loss: 110.5514\n",
      "Epoch 726/1000\n",
      "10/10 [==============================] - 0s 359us/sample - loss: 110.2688\n",
      "Epoch 727/1000\n",
      "10/10 [==============================] - 0s 216us/sample - loss: 109.9582\n",
      "Epoch 728/1000\n",
      "10/10 [==============================] - 0s 162us/sample - loss: 109.4977\n",
      "Epoch 729/1000\n",
      "10/10 [==============================] - 0s 233us/sample - loss: 109.2036\n",
      "Epoch 730/1000\n",
      "10/10 [==============================] - 0s 247us/sample - loss: 108.9422\n",
      "Epoch 731/1000\n",
      "10/10 [==============================] - 0s 189us/sample - loss: 108.6371\n",
      "Epoch 732/1000\n",
      "10/10 [==============================] - 0s 200us/sample - loss: 108.3476\n",
      "Epoch 733/1000\n",
      "10/10 [==============================] - 0s 297us/sample - loss: 108.0587\n",
      "Epoch 734/1000\n",
      "10/10 [==============================] - 0s 375us/sample - loss: 107.7674\n",
      "Epoch 735/1000\n",
      "10/10 [==============================] - 0s 200us/sample - loss: 107.4767\n",
      "Epoch 736/1000\n",
      "10/10 [==============================] - 0s 240us/sample - loss: 107.1841\n",
      "Epoch 737/1000\n",
      "10/10 [==============================] - 0s 363us/sample - loss: 106.8914\n",
      "Epoch 738/1000\n",
      "10/10 [==============================] - 0s 216us/sample - loss: 106.5970\n",
      "Epoch 739/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 215us/sample - loss: 106.3023\n",
      "Epoch 740/1000\n",
      "10/10 [==============================] - 0s 198us/sample - loss: 106.0056\n",
      "Epoch 741/1000\n",
      "10/10 [==============================] - 0s 194us/sample - loss: 105.7086\n",
      "Epoch 742/1000\n",
      "10/10 [==============================] - 0s 255us/sample - loss: 105.4097\n",
      "Epoch 743/1000\n",
      "10/10 [==============================] - 0s 238us/sample - loss: 105.1104\n",
      "Epoch 744/1000\n",
      "10/10 [==============================] - 0s 235us/sample - loss: 104.8094\n",
      "Epoch 745/1000\n",
      "10/10 [==============================] - 0s 218us/sample - loss: 104.5081\n",
      "Epoch 746/1000\n",
      "10/10 [==============================] - 0s 186us/sample - loss: 104.2052\n",
      "Epoch 747/1000\n",
      "10/10 [==============================] - 0s 322us/sample - loss: 103.9023\n",
      "Epoch 748/1000\n",
      "10/10 [==============================] - 0s 219us/sample - loss: 103.5976\n",
      "Epoch 749/1000\n",
      "10/10 [==============================] - 0s 204us/sample - loss: 103.2931\n",
      "Epoch 750/1000\n",
      "10/10 [==============================] - 0s 198us/sample - loss: 102.9868\n",
      "Epoch 751/1000\n",
      "10/10 [==============================] - 0s 181us/sample - loss: 102.6807\n",
      "Epoch 752/1000\n",
      "10/10 [==============================] - 0s 325us/sample - loss: 102.3728\n",
      "Epoch 753/1000\n",
      "10/10 [==============================] - 0s 187us/sample - loss: 102.0650\n",
      "Epoch 754/1000\n",
      "10/10 [==============================] - 0s 178us/sample - loss: 101.7553\n",
      "Epoch 755/1000\n",
      "10/10 [==============================] - 0s 198us/sample - loss: 101.4456\n",
      "Epoch 756/1000\n",
      "10/10 [==============================] - 0s 370us/sample - loss: 101.1470\n",
      "Epoch 757/1000\n",
      "10/10 [==============================] - 0s 269us/sample - loss: 100.8407\n",
      "Epoch 758/1000\n",
      "10/10 [==============================] - 0s 201us/sample - loss: 100.5285\n",
      "Epoch 759/1000\n",
      "10/10 [==============================] - 0s 223us/sample - loss: 100.2169\n",
      "Epoch 760/1000\n",
      "10/10 [==============================] - 0s 245us/sample - loss: 99.9036\n",
      "Epoch 761/1000\n",
      "10/10 [==============================] - 0s 277us/sample - loss: 99.5903\n",
      "Epoch 762/1000\n",
      "10/10 [==============================] - 0s 240us/sample - loss: 99.2755\n",
      "Epoch 763/1000\n",
      "10/10 [==============================] - 0s 227us/sample - loss: 98.9603\n",
      "Epoch 764/1000\n",
      "10/10 [==============================] - 0s 279us/sample - loss: 98.6433\n",
      "Epoch 765/1000\n",
      "10/10 [==============================] - 0s 273us/sample - loss: 98.3257\n",
      "Epoch 766/1000\n",
      "10/10 [==============================] - 0s 290us/sample - loss: 98.0064\n",
      "Epoch 767/1000\n",
      "10/10 [==============================] - 0s 264us/sample - loss: 97.6864\n",
      "Epoch 768/1000\n",
      "10/10 [==============================] - 0s 230us/sample - loss: 97.3648\n",
      "Epoch 769/1000\n",
      "10/10 [==============================] - 0s 192us/sample - loss: 97.0426\n",
      "Epoch 770/1000\n",
      "10/10 [==============================] - 0s 185us/sample - loss: 96.7189\n",
      "Epoch 771/1000\n",
      "10/10 [==============================] - 0s 160us/sample - loss: 96.3949\n",
      "Epoch 772/1000\n",
      "10/10 [==============================] - 0s 291us/sample - loss: 96.0694\n",
      "Epoch 773/1000\n",
      "10/10 [==============================] - 0s 311us/sample - loss: 95.7440\n",
      "Epoch 774/1000\n",
      "10/10 [==============================] - 0s 287us/sample - loss: 95.4170\n",
      "Epoch 775/1000\n",
      "10/10 [==============================] - 0s 179us/sample - loss: 95.0903\n",
      "Epoch 776/1000\n",
      "10/10 [==============================] - 0s 263us/sample - loss: 94.7971\n",
      "Epoch 777/1000\n",
      "10/10 [==============================] - 0s 288us/sample - loss: 94.4547\n",
      "Epoch 778/1000\n",
      "10/10 [==============================] - 0s 181us/sample - loss: 94.1278\n",
      "Epoch 779/1000\n",
      "10/10 [==============================] - 0s 301us/sample - loss: 93.8017\n",
      "Epoch 780/1000\n",
      "10/10 [==============================] - 0s 232us/sample - loss: 93.4738\n",
      "Epoch 781/1000\n",
      "10/10 [==============================] - 0s 303us/sample - loss: 93.1463\n",
      "Epoch 782/1000\n",
      "10/10 [==============================] - 0s 246us/sample - loss: 92.8176\n",
      "Epoch 783/1000\n",
      "10/10 [==============================] - 0s 295us/sample - loss: 92.4885\n",
      "Epoch 784/1000\n",
      "10/10 [==============================] - 0s 302us/sample - loss: 92.1579\n",
      "Epoch 785/1000\n",
      "10/10 [==============================] - 0s 252us/sample - loss: 91.8267\n",
      "Epoch 786/1000\n",
      "10/10 [==============================] - 0s 250us/sample - loss: 91.4939\n",
      "Epoch 787/1000\n",
      "10/10 [==============================] - 0s 456us/sample - loss: 91.1602\n",
      "Epoch 788/1000\n",
      "10/10 [==============================] - 0s 321us/sample - loss: 90.8251\n",
      "Epoch 789/1000\n",
      "10/10 [==============================] - 0s 280us/sample - loss: 90.4890\n",
      "Epoch 790/1000\n",
      "10/10 [==============================] - 0s 464us/sample - loss: 90.1516\n",
      "Epoch 791/1000\n",
      "10/10 [==============================] - 0s 346us/sample - loss: 89.8134\n",
      "Epoch 792/1000\n",
      "10/10 [==============================] - 0s 516us/sample - loss: 89.4738\n",
      "Epoch 793/1000\n",
      "10/10 [==============================] - 0s 411us/sample - loss: 89.1331\n",
      "Epoch 794/1000\n",
      "10/10 [==============================] - 0s 460us/sample - loss: 88.7910\n",
      "Epoch 795/1000\n",
      "10/10 [==============================] - 0s 268us/sample - loss: 88.4485\n",
      "Epoch 796/1000\n",
      "10/10 [==============================] - 0s 278us/sample - loss: 88.1099\n",
      "Epoch 797/1000\n",
      "10/10 [==============================] - 0s 392us/sample - loss: 87.7785\n",
      "Epoch 798/1000\n",
      "10/10 [==============================] - 0s 209us/sample - loss: 87.4355\n",
      "Epoch 799/1000\n",
      "10/10 [==============================] - 0s 244us/sample - loss: 87.0929\n",
      "Epoch 800/1000\n",
      "10/10 [==============================] - 0s 318us/sample - loss: 86.7485\n",
      "Epoch 801/1000\n",
      "10/10 [==============================] - 0s 258us/sample - loss: 86.4044\n",
      "Epoch 802/1000\n",
      "10/10 [==============================] - 0s 200us/sample - loss: 86.0582\n",
      "Epoch 803/1000\n",
      "10/10 [==============================] - 0s 290us/sample - loss: 85.7115\n",
      "Epoch 804/1000\n",
      "10/10 [==============================] - 0s 309us/sample - loss: 85.3637\n",
      "Epoch 805/1000\n",
      "10/10 [==============================] - 0s 306us/sample - loss: 85.0154\n",
      "Epoch 806/1000\n",
      "10/10 [==============================] - 0s 360us/sample - loss: 84.6657\n",
      "Epoch 807/1000\n",
      "10/10 [==============================] - 0s 291us/sample - loss: 84.3153\n",
      "Epoch 808/1000\n",
      "10/10 [==============================] - 0s 236us/sample - loss: 83.9410\n",
      "Epoch 809/1000\n",
      "10/10 [==============================] - 0s 185us/sample - loss: 83.5715\n",
      "Epoch 810/1000\n",
      "10/10 [==============================] - 0s 417us/sample - loss: 83.2530\n",
      "Epoch 811/1000\n",
      "10/10 [==============================] - 0s 318us/sample - loss: 82.8613\n",
      "Epoch 812/1000\n",
      "10/10 [==============================] - 0s 208us/sample - loss: 82.5196\n",
      "Epoch 813/1000\n",
      "10/10 [==============================] - 0s 231us/sample - loss: 82.1529\n",
      "Epoch 814/1000\n",
      "10/10 [==============================] - 0s 321us/sample - loss: 81.7938\n",
      "Epoch 815/1000\n",
      "10/10 [==============================] - 0s 277us/sample - loss: 81.4337\n",
      "Epoch 816/1000\n",
      "10/10 [==============================] - 0s 209us/sample - loss: 81.0714\n",
      "Epoch 817/1000\n",
      "10/10 [==============================] - 0s 231us/sample - loss: 80.7083\n",
      "Epoch 818/1000\n",
      "10/10 [==============================] - 0s 347us/sample - loss: 80.3430\n",
      "Epoch 819/1000\n",
      "10/10 [==============================] - 0s 310us/sample - loss: 79.9770\n",
      "Epoch 820/1000\n",
      "10/10 [==============================] - 0s 193us/sample - loss: 79.6102\n",
      "Epoch 821/1000\n",
      "10/10 [==============================] - 0s 212us/sample - loss: 79.2423\n",
      "Epoch 822/1000\n",
      "10/10 [==============================] - 0s 304us/sample - loss: 78.8753\n",
      "Epoch 823/1000\n",
      "10/10 [==============================] - 0s 261us/sample - loss: 78.5173\n",
      "Epoch 824/1000\n",
      "10/10 [==============================] - 0s 215us/sample - loss: 78.1572\n",
      "Epoch 825/1000\n",
      "10/10 [==============================] - 0s 192us/sample - loss: 77.7944\n",
      "Epoch 826/1000\n",
      "10/10 [==============================] - 0s 799us/sample - loss: 77.4455\n",
      "Epoch 827/1000\n",
      "10/10 [==============================] - 0s 208us/sample - loss: 77.0817\n",
      "Epoch 828/1000\n",
      "10/10 [==============================] - 0s 402us/sample - loss: 76.7293\n",
      "Epoch 829/1000\n",
      "10/10 [==============================] - 0s 365us/sample - loss: 76.3668\n",
      "Epoch 830/1000\n",
      "10/10 [==============================] - 0s 243us/sample - loss: 76.0023\n",
      "Epoch 831/1000\n",
      "10/10 [==============================] - 0s 261us/sample - loss: 75.6378\n",
      "Epoch 832/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 318us/sample - loss: 75.2713\n",
      "Epoch 833/1000\n",
      "10/10 [==============================] - 0s 298us/sample - loss: 74.9021\n",
      "Epoch 834/1000\n",
      "10/10 [==============================] - 0s 232us/sample - loss: 74.5284\n",
      "Epoch 835/1000\n",
      "10/10 [==============================] - 0s 269us/sample - loss: 74.1532\n",
      "Epoch 836/1000\n",
      "10/10 [==============================] - 0s 354us/sample - loss: 73.7773\n",
      "Epoch 837/1000\n",
      "10/10 [==============================] - 0s 299us/sample - loss: 73.4009\n",
      "Epoch 838/1000\n",
      "10/10 [==============================] - 0s 192us/sample - loss: 73.0357\n",
      "Epoch 839/1000\n",
      "10/10 [==============================] - 0s 371us/sample - loss: 72.6657\n",
      "Epoch 840/1000\n",
      "10/10 [==============================] - 0s 307us/sample - loss: 72.2893\n",
      "Epoch 841/1000\n",
      "10/10 [==============================] - 0s 272us/sample - loss: 71.9125\n",
      "Epoch 842/1000\n",
      "10/10 [==============================] - 0s 208us/sample - loss: 71.5381\n",
      "Epoch 843/1000\n",
      "10/10 [==============================] - 0s 320us/sample - loss: 71.1714\n",
      "Epoch 844/1000\n",
      "10/10 [==============================] - 0s 571us/sample - loss: 70.7943\n",
      "Epoch 845/1000\n",
      "10/10 [==============================] - 0s 218us/sample - loss: 70.4167\n",
      "Epoch 846/1000\n",
      "10/10 [==============================] - 0s 261us/sample - loss: 70.0393\n",
      "Epoch 847/1000\n",
      "10/10 [==============================] - 0s 590us/sample - loss: 69.6609\n",
      "Epoch 848/1000\n",
      "10/10 [==============================] - 0s 351us/sample - loss: 69.2820\n",
      "Epoch 849/1000\n",
      "10/10 [==============================] - 0s 300us/sample - loss: 68.9133\n",
      "Epoch 850/1000\n",
      "10/10 [==============================] - 0s 372us/sample - loss: 68.5407\n",
      "Epoch 851/1000\n",
      "10/10 [==============================] - 0s 360us/sample - loss: 68.1621\n",
      "Epoch 852/1000\n",
      "10/10 [==============================] - 0s 211us/sample - loss: 67.7835\n",
      "Epoch 853/1000\n",
      "10/10 [==============================] - 0s 298us/sample - loss: 67.4240\n",
      "Epoch 854/1000\n",
      "10/10 [==============================] - 0s 344us/sample - loss: 67.0412\n",
      "Epoch 855/1000\n",
      "10/10 [==============================] - 0s 263us/sample - loss: 66.6635\n",
      "Epoch 856/1000\n",
      "10/10 [==============================] - 0s 268us/sample - loss: 66.2837\n",
      "Epoch 857/1000\n",
      "10/10 [==============================] - 0s 350us/sample - loss: 65.8990\n",
      "Epoch 858/1000\n",
      "10/10 [==============================] - 0s 294us/sample - loss: 65.4926\n",
      "Epoch 859/1000\n",
      "10/10 [==============================] - 0s 183us/sample - loss: 65.0469\n",
      "Epoch 860/1000\n",
      "10/10 [==============================] - 0s 194us/sample - loss: 64.6694\n",
      "Epoch 861/1000\n",
      "10/10 [==============================] - 0s 294us/sample - loss: 64.3025\n",
      "Epoch 862/1000\n",
      "10/10 [==============================] - 0s 215us/sample - loss: 63.9187\n",
      "Epoch 863/1000\n",
      "10/10 [==============================] - 0s 386us/sample - loss: 63.5402\n",
      "Epoch 864/1000\n",
      "10/10 [==============================] - 0s 283us/sample - loss: 63.1650\n",
      "Epoch 865/1000\n",
      "10/10 [==============================] - 0s 365us/sample - loss: 62.7962\n",
      "Epoch 866/1000\n",
      "10/10 [==============================] - 0s 200us/sample - loss: 62.4193\n",
      "Epoch 867/1000\n",
      "10/10 [==============================] - 0s 195us/sample - loss: 62.0422\n",
      "Epoch 868/1000\n",
      "10/10 [==============================] - 0s 277us/sample - loss: 61.6760\n",
      "Epoch 869/1000\n",
      "10/10 [==============================] - 0s 356us/sample - loss: 61.3093\n",
      "Epoch 870/1000\n",
      "10/10 [==============================] - 0s 246us/sample - loss: 60.9346\n",
      "Epoch 871/1000\n",
      "10/10 [==============================] - 0s 308us/sample - loss: 60.5599\n",
      "Epoch 872/1000\n",
      "10/10 [==============================] - 0s 253us/sample - loss: 60.1969\n",
      "Epoch 873/1000\n",
      "10/10 [==============================] - 0s 192us/sample - loss: 59.8432\n",
      "Epoch 874/1000\n",
      "10/10 [==============================] - 0s 273us/sample - loss: 59.4752\n",
      "Epoch 875/1000\n",
      "10/10 [==============================] - 0s 289us/sample - loss: 59.1067\n",
      "Epoch 876/1000\n",
      "10/10 [==============================] - 0s 307us/sample - loss: 58.7438\n",
      "Epoch 877/1000\n",
      "10/10 [==============================] - 0s 184us/sample - loss: 58.4013\n",
      "Epoch 878/1000\n",
      "10/10 [==============================] - 0s 219us/sample - loss: 58.0393\n",
      "Epoch 879/1000\n",
      "10/10 [==============================] - 0s 300us/sample - loss: 57.6748\n",
      "Epoch 880/1000\n",
      "10/10 [==============================] - 0s 392us/sample - loss: 57.3424\n",
      "Epoch 881/1000\n",
      "10/10 [==============================] - 0s 268us/sample - loss: 56.9979\n",
      "Epoch 882/1000\n",
      "10/10 [==============================] - 0s 179us/sample - loss: 56.6616\n",
      "Epoch 883/1000\n",
      "10/10 [==============================] - 0s 302us/sample - loss: 56.3063\n",
      "Epoch 884/1000\n",
      "10/10 [==============================] - 0s 264us/sample - loss: 55.9802\n",
      "Epoch 885/1000\n",
      "10/10 [==============================] - 0s 273us/sample - loss: 55.6438\n",
      "Epoch 886/1000\n",
      "10/10 [==============================] - 0s 218us/sample - loss: 55.2994\n",
      "Epoch 887/1000\n",
      "10/10 [==============================] - 0s 267us/sample - loss: 54.9571\n",
      "Epoch 888/1000\n",
      "10/10 [==============================] - 0s 242us/sample - loss: 54.6487\n",
      "Epoch 889/1000\n",
      "10/10 [==============================] - 0s 221us/sample - loss: 54.3124\n",
      "Epoch 890/1000\n",
      "10/10 [==============================] - 0s 328us/sample - loss: 53.9686\n",
      "Epoch 891/1000\n",
      "10/10 [==============================] - 0s 233us/sample - loss: 53.6310\n",
      "Epoch 892/1000\n",
      "10/10 [==============================] - 0s 251us/sample - loss: 53.3056\n",
      "Epoch 893/1000\n",
      "10/10 [==============================] - 0s 212us/sample - loss: 52.9671\n",
      "Epoch 894/1000\n",
      "10/10 [==============================] - 0s 257us/sample - loss: 52.6407\n",
      "Epoch 895/1000\n",
      "10/10 [==============================] - 0s 272us/sample - loss: 52.3343\n",
      "Epoch 896/1000\n",
      "10/10 [==============================] - 0s 321us/sample - loss: 51.9983\n",
      "Epoch 897/1000\n",
      "10/10 [==============================] - 0s 244us/sample - loss: 51.6624\n",
      "Epoch 898/1000\n",
      "10/10 [==============================] - 0s 374us/sample - loss: 51.3503\n",
      "Epoch 899/1000\n",
      "10/10 [==============================] - 0s 272us/sample - loss: 51.0333\n",
      "Epoch 900/1000\n",
      "10/10 [==============================] - 0s 278us/sample - loss: 50.7046\n",
      "Epoch 901/1000\n",
      "10/10 [==============================] - 0s 287us/sample - loss: 50.3766\n",
      "Epoch 902/1000\n",
      "10/10 [==============================] - 0s 189us/sample - loss: 50.0745\n",
      "Epoch 903/1000\n",
      "10/10 [==============================] - 0s 239us/sample - loss: 49.7465\n",
      "Epoch 904/1000\n",
      "10/10 [==============================] - 0s 229us/sample - loss: 49.4299\n",
      "Epoch 905/1000\n",
      "10/10 [==============================] - 0s 269us/sample - loss: 49.1427\n",
      "Epoch 906/1000\n",
      "10/10 [==============================] - 0s 234us/sample - loss: 48.8242\n",
      "Epoch 907/1000\n",
      "10/10 [==============================] - 0s 253us/sample - loss: 48.5024\n",
      "Epoch 908/1000\n",
      "10/10 [==============================] - 0s 308us/sample - loss: 48.1991\n",
      "Epoch 909/1000\n",
      "10/10 [==============================] - 0s 324us/sample - loss: 47.9029\n",
      "Epoch 910/1000\n",
      "10/10 [==============================] - 0s 228us/sample - loss: 47.5895\n",
      "Epoch 911/1000\n",
      "10/10 [==============================] - 0s 199us/sample - loss: 47.2746\n",
      "Epoch 912/1000\n",
      "10/10 [==============================] - 0s 516us/sample - loss: 46.9923\n",
      "Epoch 913/1000\n",
      "10/10 [==============================] - 0s 346us/sample - loss: 46.6748\n",
      "Epoch 914/1000\n",
      "10/10 [==============================] - 0s 200us/sample - loss: 46.3698\n",
      "Epoch 915/1000\n",
      "10/10 [==============================] - 0s 220us/sample - loss: 46.0630\n",
      "Epoch 916/1000\n",
      "10/10 [==============================] - 0s 365us/sample - loss: 45.7706\n",
      "Epoch 917/1000\n",
      "10/10 [==============================] - 0s 178us/sample - loss: 45.4701\n",
      "Epoch 918/1000\n",
      "10/10 [==============================] - 0s 271us/sample - loss: 45.1844\n",
      "Epoch 919/1000\n",
      "10/10 [==============================] - 0s 229us/sample - loss: 44.8883\n",
      "Epoch 920/1000\n",
      "10/10 [==============================] - 0s 434us/sample - loss: 44.5881\n",
      "Epoch 921/1000\n",
      "10/10 [==============================] - 0s 257us/sample - loss: 44.2878\n",
      "Epoch 922/1000\n",
      "10/10 [==============================] - 0s 196us/sample - loss: 44.0001\n",
      "Epoch 923/1000\n",
      "10/10 [==============================] - 0s 221us/sample - loss: 43.7134\n",
      "Epoch 924/1000\n",
      "10/10 [==============================] - 0s 410us/sample - loss: 43.4528\n",
      "Epoch 925/1000\n",
      "10/10 [==============================] - 0s 212us/sample - loss: 43.1698\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 926/1000\n",
      "10/10 [==============================] - 0s 215us/sample - loss: 42.8778\n",
      "Epoch 927/1000\n",
      "10/10 [==============================] - 0s 179us/sample - loss: 42.5820\n",
      "Epoch 928/1000\n",
      "10/10 [==============================] - 0s 209us/sample - loss: 42.2918\n",
      "Epoch 929/1000\n",
      "10/10 [==============================] - 0s 288us/sample - loss: 41.9990\n",
      "Epoch 930/1000\n",
      "10/10 [==============================] - 0s 231us/sample - loss: 41.7226\n",
      "Epoch 931/1000\n",
      "10/10 [==============================] - 0s 207us/sample - loss: 41.4349\n",
      "Epoch 932/1000\n",
      "10/10 [==============================] - 0s 247us/sample - loss: 41.1561\n",
      "Epoch 933/1000\n",
      "10/10 [==============================] - 0s 379us/sample - loss: 40.8639\n",
      "Epoch 934/1000\n",
      "10/10 [==============================] - 0s 258us/sample - loss: 40.5716\n",
      "Epoch 935/1000\n",
      "10/10 [==============================] - 0s 180us/sample - loss: 40.2804\n",
      "Epoch 936/1000\n",
      "10/10 [==============================] - 0s 409us/sample - loss: 39.9980\n",
      "Epoch 937/1000\n",
      "10/10 [==============================] - 0s 265us/sample - loss: 39.7178\n",
      "Epoch 938/1000\n",
      "10/10 [==============================] - 0s 218us/sample - loss: 39.4601\n",
      "Epoch 939/1000\n",
      "10/10 [==============================] - 0s 276us/sample - loss: 39.2127\n",
      "Epoch 940/1000\n",
      "10/10 [==============================] - 0s 428us/sample - loss: 38.9441\n",
      "Epoch 941/1000\n",
      "10/10 [==============================] - 0s 265us/sample - loss: 38.6538\n",
      "Epoch 942/1000\n",
      "10/10 [==============================] - 0s 204us/sample - loss: 38.3771\n",
      "Epoch 943/1000\n",
      "10/10 [==============================] - 0s 247us/sample - loss: 38.0920\n",
      "Epoch 944/1000\n",
      "10/10 [==============================] - 0s 355us/sample - loss: 37.8317\n",
      "Epoch 945/1000\n",
      "10/10 [==============================] - 0s 235us/sample - loss: 37.5526\n",
      "Epoch 946/1000\n",
      "10/10 [==============================] - 0s 270us/sample - loss: 37.2905\n",
      "Epoch 947/1000\n",
      "10/10 [==============================] - 0s 212us/sample - loss: 37.0287\n",
      "Epoch 948/1000\n",
      "10/10 [==============================] - 0s 338us/sample - loss: 36.7602\n",
      "Epoch 949/1000\n",
      "10/10 [==============================] - 0s 214us/sample - loss: 36.4913\n",
      "Epoch 950/1000\n",
      "10/10 [==============================] - 0s 217us/sample - loss: 36.2256\n",
      "Epoch 951/1000\n",
      "10/10 [==============================] - 0s 348us/sample - loss: 35.9689\n",
      "Epoch 952/1000\n",
      "10/10 [==============================] - 0s 261us/sample - loss: 35.7157\n",
      "Epoch 953/1000\n",
      "10/10 [==============================] - 0s 232us/sample - loss: 35.4688\n",
      "Epoch 954/1000\n",
      "10/10 [==============================] - 0s 215us/sample - loss: 35.2208\n",
      "Epoch 955/1000\n",
      "10/10 [==============================] - 0s 321us/sample - loss: 34.9610\n",
      "Epoch 956/1000\n",
      "10/10 [==============================] - 0s 238us/sample - loss: 34.6967\n",
      "Epoch 957/1000\n",
      "10/10 [==============================] - 0s 224us/sample - loss: 34.4293\n",
      "Epoch 958/1000\n",
      "10/10 [==============================] - 0s 211us/sample - loss: 34.1740\n",
      "Epoch 959/1000\n",
      "10/10 [==============================] - 0s 440us/sample - loss: 33.9210\n",
      "Epoch 960/1000\n",
      "10/10 [==============================] - 0s 207us/sample - loss: 33.6862\n",
      "Epoch 961/1000\n",
      "10/10 [==============================] - 0s 240us/sample - loss: 33.4305\n",
      "Epoch 962/1000\n",
      "10/10 [==============================] - 0s 280us/sample - loss: 33.1945\n",
      "Epoch 963/1000\n",
      "10/10 [==============================] - 0s 241us/sample - loss: 32.9469\n",
      "Epoch 964/1000\n",
      "10/10 [==============================] - 0s 295us/sample - loss: 32.6974\n",
      "Epoch 965/1000\n",
      "10/10 [==============================] - 0s 308us/sample - loss: 32.4465\n",
      "Epoch 966/1000\n",
      "10/10 [==============================] - 0s 247us/sample - loss: 32.2035\n",
      "Epoch 967/1000\n",
      "10/10 [==============================] - 0s 306us/sample - loss: 31.9909\n",
      "Epoch 968/1000\n",
      "10/10 [==============================] - 0s 206us/sample - loss: 31.7468\n",
      "Epoch 969/1000\n",
      "10/10 [==============================] - 0s 220us/sample - loss: 31.5023\n",
      "Epoch 970/1000\n",
      "10/10 [==============================] - 0s 298us/sample - loss: 31.2673\n",
      "Epoch 971/1000\n",
      "10/10 [==============================] - 0s 404us/sample - loss: 31.0393\n",
      "Epoch 972/1000\n",
      "10/10 [==============================] - 0s 210us/sample - loss: 30.8096\n",
      "Epoch 973/1000\n",
      "10/10 [==============================] - 0s 239us/sample - loss: 30.5711\n",
      "Epoch 974/1000\n",
      "10/10 [==============================] - 0s 343us/sample - loss: 30.3422\n",
      "Epoch 975/1000\n",
      "10/10 [==============================] - 0s 274us/sample - loss: 30.1248\n",
      "Epoch 976/1000\n",
      "10/10 [==============================] - 0s 195us/sample - loss: 29.9095\n",
      "Epoch 977/1000\n",
      "10/10 [==============================] - 0s 230us/sample - loss: 29.6738\n",
      "Epoch 978/1000\n",
      "10/10 [==============================] - 0s 412us/sample - loss: 29.4443\n",
      "Epoch 979/1000\n",
      "10/10 [==============================] - 0s 204us/sample - loss: 29.2303\n",
      "Epoch 980/1000\n",
      "10/10 [==============================] - 0s 391us/sample - loss: 29.0025\n",
      "Epoch 981/1000\n",
      "10/10 [==============================] - 0s 307us/sample - loss: 28.7740\n",
      "Epoch 982/1000\n",
      "10/10 [==============================] - 0s 182us/sample - loss: 28.5608\n",
      "Epoch 983/1000\n",
      "10/10 [==============================] - 0s 245us/sample - loss: 28.3585\n",
      "Epoch 984/1000\n",
      "10/10 [==============================] - 0s 324us/sample - loss: 28.1449\n",
      "Epoch 985/1000\n",
      "10/10 [==============================] - 0s 276us/sample - loss: 27.9278\n",
      "Epoch 986/1000\n",
      "10/10 [==============================] - 0s 196us/sample - loss: 27.7098\n",
      "Epoch 987/1000\n",
      "10/10 [==============================] - 0s 236us/sample - loss: 27.4927\n",
      "Epoch 988/1000\n",
      "10/10 [==============================] - 0s 267us/sample - loss: 27.2851\n",
      "Epoch 989/1000\n",
      "10/10 [==============================] - 0s 530us/sample - loss: 27.0783\n",
      "Epoch 990/1000\n",
      "10/10 [==============================] - 0s 184us/sample - loss: 26.8728\n",
      "Epoch 991/1000\n",
      "10/10 [==============================] - 0s 174us/sample - loss: 26.6790\n",
      "Epoch 992/1000\n",
      "10/10 [==============================] - 0s 284us/sample - loss: 26.4713\n",
      "Epoch 993/1000\n",
      "10/10 [==============================] - 0s 438us/sample - loss: 26.2691\n",
      "Epoch 994/1000\n",
      "10/10 [==============================] - 0s 173us/sample - loss: 26.0719\n",
      "Epoch 995/1000\n",
      "10/10 [==============================] - 0s 214us/sample - loss: 25.8941\n",
      "Epoch 996/1000\n",
      "10/10 [==============================] - 0s 281us/sample - loss: 25.6979\n",
      "Epoch 997/1000\n",
      "10/10 [==============================] - 0s 328us/sample - loss: 25.5020\n",
      "Epoch 998/1000\n",
      "10/10 [==============================] - 0s 222us/sample - loss: 25.3283\n",
      "Epoch 999/1000\n",
      "10/10 [==============================] - 0s 176us/sample - loss: 25.1631\n",
      "Epoch 1000/1000\n",
      "10/10 [==============================] - 0s 251us/sample - loss: 24.9739\n",
      "[[101.2809]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as kb\n",
    "import numpy as np\n",
    "\n",
    "# This is an ultra simple model to learn squares of numbers.\n",
    "# Do not take the model too seriosuly, it will overfit and is only \n",
    "# for deminstration purpose\n",
    "keras_model=tf.keras.Sequential([ \n",
    "    tf.keras.layers.Dense(32,activation=tf.nn.relu,input_shape=[1]),\n",
    "    tf.keras.layers.Dense(32,activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(1)\n",
    "]\n",
    ")\n",
    "\n",
    "# Now we define our custom loss function\n",
    "\n",
    "def custom_loss(y_actual,y_pred): \n",
    "    custom_loss=kb.square(y_actual-y_pred)\n",
    "    return custom_loss\n",
    "\n",
    "optimizer=tf.keras.optimizers.RMSprop(0.001)\n",
    "keras_model.compile(loss=custom_loss,optimizer=optimizer)\n",
    "\n",
    "#Sample data\n",
    "x=[1,2,3,4,5,6,7,8,9,10]\n",
    "x=np.asarray(x).reshape((10,1))\n",
    "y=[1,4,9,16,25,36,49,64,81,100]\n",
    "y=np.asarray(y).reshape((10,1))\n",
    "y=y.astype(np.float32)\n",
    "keras_model.fit(x,y,batch_size=10,epochs=1000)\n",
    "print(keras_model.predict([11]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.keras.layers.core.Dense object at 0x1393cffd0>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Custom training\n",
    "x=[1,2,3,4,5,6,7,8,9,10]\n",
    "x=np.asarray(x,dtype=np.float32).reshape((10,1))\n",
    "y=[1,4,9,16,25,36,49,64,81,100]\n",
    "y=np.asarray(y,dtype=np.float32).reshape((10,1))\n",
    "m=model()\n",
    "\n",
    "for i in range(10):\n",
    "    m.network_learn(x,y)\n",
    "\n",
    "    \n",
    "# Test Case\n",
    "#x=[11]\n",
    "#x=np.asarray(x,dtype=np.float32).reshape((1,1))\n",
    "#print(m.run(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     fun: 4.440892098500626e-16\n",
       " message: 'Optimization terminated successfully.'\n",
       "    nfev: 3153\n",
       "     nit: 102\n",
       " success: True\n",
       "       x: array([0., 0.])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.optimize import differential_evolution\n",
    "import numpy as np\n",
    "def ackley(x):\n",
    "    arg1 = -0.2 * np.sqrt(0.5 * (x[0] ** 2 + x[1] ** 2))\n",
    "    arg2 = 0.5 * (np.cos(2. * np.pi * x[0]) + np.cos(2. * np.pi * x[1]))\n",
    "    return -20. * np.exp(arg1) - np.exp(arg2) + 20. + np.e\n",
    "bounds = [(-5, 5), (-5, 5)]\n",
    "result = differential_evolution(ackley, bounds)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
